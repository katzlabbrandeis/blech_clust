[
    {
        "filename": "blech_clean_slate.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/blech_clean_slate.py",
        "docstring": "This script resets a data folder to an almost raw form by deleting most processing files while retaining specific file types such as info files and sorting tables.\n\n- Imports necessary modules and a utility function `imp_metadata` to handle metadata.\n- Retrieves directory name and file list from metadata.\n- Defines a list of file patterns to keep, including `.dat`, `.info`, `.rhd`, `.csv`, `*_info`, `.txt`, and `.xml`.\n- Identifies files to be removed by excluding the files matching the keep patterns.\n- Attempts to remove each file or directory not matching the keep patterns using `shutil.rmtree` or `os.remove`."
    },
    {
        "filename": "blech_clust.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/blech_clust.py",
        "docstring": "blech_clust.py - Main script for processing and clustering neural recording data\n\nThis script handles the initial processing of neural recording data from Intan files,\ncreating an HDF5 file structure to store the processed data. It performs several key functions:\n\n1. Data Import and Organization:\n   - Reads raw data files (.rhd or .dat format)\n   - Creates HDF5 file structure for organized data storage\n   - Handles different file formats (one file per channel, one file per signal type, traditional)\n\n2. Directory Setup:\n   - Creates necessary directories for storing:\n     * Spike waveforms\n     * Spike times\n     * Clustering results\n     * Analysis plots\n     * Memory monitoring data\n\n3. Quality Assurance:\n   - Performs channel correlation analysis\n   - Generates QA plots and reports\n   - Validates digital input signals\n\n4. Processing Pipeline:\n   - Sets up parallel processing scripts for spike sorting\n   - Handles parameter file creation and management\n   - Integrates with the broader blech_clust pipeline\n\nUsage:\n    python blech_clust.py <dir_name> [--force_run]\n\nArguments:\n    dir_name    : Directory containing the raw data files\n    --force_run : Optional flag to bypass user confirmations\n\nDependencies:\n    - numpy, tables, pandas, matplotlib\n    - Custom utility modules from blech_clust package"
    },
    {
        "filename": "blech_common_avg_reference.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/blech_common_avg_reference.py",
        "docstring": "blech_common_avg_reference.py - Common Average Reference (CAR) processing for neural recordings\n\nThis script performs common average referencing on raw electrode data to reduce noise and artifacts.\nIt processes electrode recordings by:\n\n1. Data Organization:\n   - Groups electrodes based on anatomical regions and recording ports\n   - Excludes EMG channels and electrodes marked as 'none' from CAR processing\n   - Handles multiple CAR groups independently\n\n2. Reference Calculation:\n   - Calculates common average reference for each electrode group\n   - Averages voltage values across all electrodes in a group\n   - Processes groups sequentially to optimize memory usage\n\n3. Signal Processing:\n   - Subtracts appropriate common average reference from each electrode\n   - Updates electrode data in-place in the HDF5 file\n   - Maintains data integrity through careful memory management\n\nUsage:\n    python blech_common_avg_reference.py <dir_name>\n\nArguments:\n    dir_name : Directory containing the HDF5 file with raw electrode data\n\nDependencies:\n    - numpy, tables, tqdm\n    - Custom utility modules from blech_clust package\n\nNotes:\n    - Requires properly formatted electrode layout information in the experiment info file\n    - CAR groups are defined by the 'CAR_group' column in the electrode layout\n    - EMG channels and electrodes marked as 'none' are automatically excluded\n\nAuthor: Abuzar Mahmood"
    },
    {
        "filename": "blech_exp_info.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/blech_exp_info.py",
        "docstring": "This module generates a file containing relevant experimental information for a given dataset. It processes data files to extract and organize details such as animal name, experiment type, date, timestamp, regions recorded from, electrode layout, taste concentrations, palatability ranks, laser parameters, and miscellaneous notes.\n\n- Parses command-line arguments to specify the directory containing data files and optional parameters like template files, mode, and various experimental details.\n- `parse_csv(s, convert=str)`: Helper function to parse comma-separated values from a string and convert them to a specified type.\n- Extracts metadata from the directory name and checks the pipeline status.\n- Processes digital input (dig-in) data to determine taste dig-ins, concentrations, palatability rankings, and laser parameters.\n- Handles different file types for electrode data and generates or uses an existing electrode layout file.\n- Organizes and writes out the final experimental information into a JSON file.\n- Logs the completion status of the pipeline process."
    },
    {
        "filename": "blech_make_arrays.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/blech_make_arrays.py",
        "docstring": "This module processes neural and EMG data from an HDF5 file, extracting and organizing spike trains and EMG trials based on digital input events. It also handles metadata and logs the processing steps.\n\n- `create_spike_trains_for_digin(this_starts, this_dig_name, durations, sampling_rate_ms, units, hf5)`: Generates spike trains for specified digital input events and stores them in the HDF5 file.\n- `create_emg_trials_for_digin(this_starts, dig_in_basename, durations, sampling_rate_ms, emg_nodes, hf5)`: Extracts EMG trial data for specified digital input events and stores it in the HDF5 file.\n- The main script:\n  - Loads metadata and performs a pipeline graph check.\n  - Extracts digital input data and organizes it into a trial information frame.\n  - Calculates laser timing corrections and saves trial information to the HDF5 file and a CSV.\n  - Determines experiment end time based on EMG or spike data.\n  - Creates spike trains if sorted units are present and saves them to the HDF5 file.\n  - Creates EMG trial arrays if EMG data is available and saves them to the HDF5 file.\n  - Logs the successful completion of the processing steps."
    },
    {
        "filename": "blech_post_process.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/blech_post_process.py",
        "docstring": "blech_post_process.py - Post-processing and unit sorting for neural recordings\n\nThis script handles the final stage of spike sorting, allowing both manual and automatic \nprocessing of clustered neural data. Key functionalities include:\n\n1. Unit Processing:\n   - Manual cluster selection and refinement\n   - Splitting of clusters into subclusters\n   - Merging of similar clusters\n   - Automatic unit classification based on quality metrics\n\n2. Quality Metrics:\n   - Inter-spike interval (ISI) violation analysis\n   - Cluster isolation metrics\n   - Waveform consistency checks\n   - Mahalanobis distance calculations between clusters\n\n3. Data Management:\n   - Handles sorted unit data in HDF5 file structure\n   - Maintains unit descriptors and metadata\n   - Supports both interactive and batch processing modes\n   - Optional integration with external sorting files\n\n4. Visualization:\n   - Generates waveform plots for manual inspection\n   - Creates quality metric visualizations\n   - Produces summary plots for merged/split units\n   - Automated report generation for batch processing\n\nUsage:\n    python blech_post_process.py <dir_name> [--show-plot True/False] [--sort-file path/to/file]\n\nArguments:\n    dir_name    : Directory containing the HDF5 file with clustered data\n    --show-plot : Toggle visualization during processing (default: True)\n    --sort-file : Optional CSV file with pre-defined sorting decisions\n\nDependencies:\n    - numpy, pandas, tables, sklearn, matplotlib\n    - Custom utility modules from blech_clust package\n    - datashader for large dataset visualization\n\nNotes:\n    - Requires completed execution of blech_clust.py and clustering scripts\n    - Supports both manual and automated quality control\n    - Integrates with the broader blech_clust processing pipeline\n\nAuthor: Abuzar Mahmood"
    },
    {
        "filename": "blech_process.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/blech_process.py",
        "docstring": "This module processes single electrode waveforms for spike detection and clustering. It includes data loading, preprocessing, spike extraction, feature extraction, and clustering. The module also supports classification and logging of the processing steps.\n\n- **Argument Parsing**: Parses command-line arguments for the data directory and electrode number.\n- **Data Loading**: Loads data and metadata for the specified electrode, and initializes a processing log.\n- **Preprocessing**: \n  - Filters electrode data.\n  - Calculates voltage parameters and cuts the recording accordingly.\n  - Extracts spike times and waveforms.\n  - Generates plots of filtered data with threshold overlays.\n- **Spike Processing**: \n  - Dejitters spike waveforms and extracts their maximum amplitudes.\n- **Classification**: \n  - Loads and applies a classifier if specified, using neuRecommend features.\n  - Optionally throws out noise waveforms based on classification results.\n- **Feature Extraction**: \n  - Extracts features using either neuRecommend or blech_spike_features pipelines.\n- **Clustering**: \n  - Performs manual or automatic clustering using Gaussian Mixture Models (GMM).\n  - Removes outliers and calculates Mahalanobis distance matrices.\n  - Saves cluster labels and generates output plots.\n- **Logging**: \n  - Updates the processing log with start and end times, and status of processing.\n  - Writes successful execution to a pipeline log."
    },
    {
        "filename": "blech_units_characteristics.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/blech_units_characteristics.py",
        "docstring": "This module performs various analyses on neural data, focusing on firing rates, responsiveness, discriminability, palatability, and dynamicity of neurons. It generates plots and saves results to disk and an HDF5 file.\n\n- **Firing Rate Calculation**: Computes and plots firing rates for neurons under different conditions, including peristimulus time histograms (PSTH) and raster plots.\n- **Responsiveness Analysis**: Determines if neurons are responsive to stimuli by comparing pre- and post-stimulus firing rates using t-tests.\n- **Discriminability Analysis**: Uses two-way ANOVA to assess if neurons can discriminate between different tastes over time.\n- **Palatability Analysis**: Evaluates the correlation between firing rates and taste palatability using Spearman's rank correlation.\n- **Dynamicity Analysis**: Assesses changes in neural activity over time using ANOVA.\n- **Aggregate Plot Generation**: Creates summary plots showing the significance of responsiveness, discriminability, palatability, and dynamicity for each condition.\n- **Data Export**: Merges results into a single DataFrame and exports it to CSV and HDF5 formats."
    },
    {
        "filename": "blech_units_plot.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/blech_units_plot.py",
        "docstring": "This module processes neural data stored in an HDF5 file, generating and saving plots of unit waveforms, inter-spike interval (ISI) histograms, and spike count histograms. It also logs the execution of the processing pipeline.\n\n- Imports necessary libraries and utility functions for data handling, plotting, and logging.\n- Retrieves metadata and directory information using `imp_metadata`.\n- Performs a pipeline graph check with `pipeline_graph_check` to ensure the correct sequence of operations.\n- Opens an HDF5 file containing sorted neural units and retrieves unit data.\n- Calculates the minimum and maximum times for plotting purposes.\n- Creates a directory for storing waveform plots, deleting any existing directory with the same name.\n- Iterates over each unit to:\n  - Plot waveforms using `blech_waveforms_datashader`.\n  - Plot mean and standard deviation of waveforms.\n  - Generate ISI histograms using `gen_isi_hist`.\n  - Plot spike count histograms over time.\n  - Save the generated plots in the specified directory.\n- Separately saves datashader and average unit plots in a subdirectory.\n- Closes the HDF5 file after processing.\n- Logs the successful completion of the pipeline."
    },
    {
        "filename": "emg_filter.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/emg/emg_filter.py",
        "docstring": "This module processes EMG (electromyography) data by subtracting signals, filtering them, and saving the results. It handles data loading, filtering, differencing, and identifying significant trials based on activity changes.\n\n- Imports necessary libraries and modules, including numpy, scipy, pandas, and custom utilities.\n- Loads EMG data from an HDF5 file and retrieves metadata and parameters.\n- Configures Butterworth highpass and lowpass filters for signal processing.\n- Reads electrode layout and groups EMG channels by CAR (Common Average Reference) groups.\n- Differentiates EMG signals within CAR groups, handling cases with one or two channels.\n- Applies bandpass and lowpass filters to the differentiated signals.\n- Identifies significant trials by comparing pre- and post-stimulus activity levels.\n- Saves filtered signals, envelopes, and significant trial indicators back to the HDF5 file.\n- Logs the execution status of the processing pipeline."
    },
    {
        "filename": "emg_freq_plot.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/emg/emg_freq_plot.py",
        "docstring": "This module processes and visualizes EMG data related to gapes and LTPS (licking tongue protrusions) in response to different taste and laser conditions. It reads data from CSV and NPY files, processes it, and generates plots for analysis.\n\n- Imports necessary libraries and modules, including custom utilities for metadata handling.\n- Reads metadata and changes the working directory to the location of the data files.\n- Loads EMG data from CSV and NPY files, filling missing laser data with `False`.\n- Extracts and processes time indices for plotting based on metadata parameters.\n- Constructs a long-format DataFrame by manually adding time, gapes, and LTPS data.\n- Melts the DataFrame to long format for easier plotting, grouping by relevant categories.\n- Creates a directory for saving plots if it doesn't exist.\n- Generates and saves plots for gapes and LTPS data, showing single trials and averages for each combination of CAR, taste, and laser condition.\n- Produces overlay plots for taste and laser conditions using seaborn, saving them to the specified directory."
    },
    {
        "filename": "emg_freq_post_process.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/emg/emg_freq_post_process.py",
        "docstring": "This module performs post-processing cleanup of files created by `emg_local_BSA_execute.py`, saving output files to an HDF5 file under the node `emg_BSA_results`. It processes EMG data, removes unnecessary nodes, and calculates specific frequency arrays.\n\n- Imports necessary libraries and utility functions.\n- Retrieves the directory name and metadata using `imp_metadata`.\n- Performs a pipeline graph check using `pipeline_graph_check`.\n- Opens the HDF5 file and removes the `/raw_emg` node if it exists to reduce file size.\n- Extracts experimental information and taste names from metadata.\n- Loads trial data from a CSV file and frequency analysis results from NPY files.\n- Processes and saves the first non-NaN omega data to disk.\n- Calculates `gape_array` and `ltp_array` based on specific frequency ranges and saves them to disk.\n- Logs the successful execution of the script."
    },
    {
        "filename": "emg_freq_setup.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/emg/emg_freq_setup.py",
        "docstring": "This module sets up EMG data for running the envelope of EMG recordings through a local Bayesian Spectrum Analysis (BSA). It requires an installation of R and the R library BaSAR. The script is a preparatory step for `emg_local_BSA_execute.py`.\n\n- Imports necessary libraries and modules, including custom utilities for metadata and path handling.\n- Retrieves the directory name containing data files using metadata.\n- Performs a pipeline graph check to ensure the script's execution order.\n- Sets up parameters for processing, including durations and plotting parameters.\n- Retrieves paths for data directories and checks for the existence of necessary parameter files.\n- Loads EMG envelope and filtered data from an HDF5 file and organizes it into lists.\n- Converts the EMG data into a pandas DataFrame for further processing.\n- Creates output directories and saves the EMG envelope data to a CSV file and a NumPy file.\n- Deletes previous results and logs, and sets up shell scripts for running parallel jobs using GNU parallel.\n- Merges the EMG data with trial information from a CSV file and saves the merged data.\n- Generates plots of the EMG envelope and filtered data, saving them to the output directory.\n- Logs the successful execution of the script."
    },
    {
        "filename": "emg_local_BSA_execute.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/emg/emg_local_BSA_execute.py",
        "docstring": "This module performs a local Bayesian Spectral Analysis (BSA) on a single trial of Electromyography (EMG) data using the High-Performance Computing (HPC) environment. It integrates Python with R to execute the BSA using the BaSAR package.\n\n- `Logger` class: A custom logger that writes messages to both the terminal and a log file, appending timestamps to each message.\n- Reads the directory name from 'BSA_run.dir' and changes the working directory to the specified path.\n- Utilizes the `pipeline_graph_check` from `blech_utils` to verify the pipeline's previous state and log the attempt.\n- Loads EMG data from a NumPy file and processes a specific trial based on a command-line argument.\n- Interfaces with R using `rpy2` to perform BSA on the EMG data, checking for NaN values before processing.\n- Saves the BSA results (`p` and `omega`) as NumPy files in the 'emg_BSA_results' directory.\n- Logs the completion of the process using the pipeline check utility."
    },
    {
        "filename": "emg_local_STFT_execute.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/emg/emg_local_STFT_execute.py",
        "docstring": "This module performs a local BSA (Blind Source Analysis) on a single trial of EMG (Electromyography) data, running on a High-Performance Computing (HPC) environment. It calculates the Short-Time Fourier Transform (STFT) and processes the results for further analysis.\n\n- `Logger` class: A custom logger that writes messages to both the terminal and a log file, with timestamps.\n- `calc_stft(trial, max_freq, time_range_tuple, Fs, signal_window, window_overlap)`: Computes the Short-Time Fourier Transform (STFT) of a given trial signal, returning the frequency and time vectors, and the STFT matrix.\n- `calc_stft_mode_freq(dat, BSA_output=True, **stft_params)`: Calculates the mode frequency of the STFT of a signal, optionally formatting the output to match BSA requirements. Returns frequency and time vectors, and the time-averaged mode frequency.\n- The script sets up parameters by loading them from a JSON file and checks the pipeline status using a utility function.\n- It processes a specified trial of EMG data, checking for non-zero data before running the BSA, and saves the results to files.\n- The script logs the progress and results of the analysis, ensuring traceability and error checking."
    },
    {
        "filename": "emg_reload_raw_data.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/emg/utils/emg_reload_raw_data.py",
        "docstring": "This script is designed to reload raw EMG data into an HDF5 file if it has been deleted without using the `blech_clust.py` script. It handles file directory selection, checks for necessary experimental information, and processes EMG data files.\n\n- Imports necessary Python modules and custom utilities.\n- Determines the directory containing data files, either from command-line arguments or via a GUI prompt.\n- Checks for the presence of an experimental info JSON file and exits if not found.\n- Opens an HDF5 file, removes any existing raw EMG data, and creates a new group for raw EMG data.\n- Identifies amplifier ports used and sorts them.\n- Loads experimental and electrode layout information from JSON and CSV files, respectively.\n- Calls `read_file.read_emg_channels` to read EMG channels from the data files and store them in the HDF5 file. \n\nNote: The script contains commented-out code for reading EMG data directly from amplifier channels, which is not currently executed."
    },
    {
        "filename": "update_docstrings.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/update_docstrings.py",
        "docstring": "This module is designed to update the docstrings of Python files within a specified directory, particularly for the `blech_clust` package. It inserts a standard docstring template at the beginning of files that lack one.\n\n- `update_docstring(file_path)`: Updates the docstring of a given Python file if it doesn't already have one. It uses a predefined template to insert a new docstring at the top of the file.\n- `update_all_docstrings(directory)`: Recursively walks through a directory and updates the docstrings of all Python files found, using the `update_docstring` function.\n- The script is intended to be run as a standalone program, with a hardcoded path to the `blech_clust` project directory, where it will update all Python files' docstrings."
    },
    {
        "filename": "blech_channel_profile.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/utils/blech_channel_profile.py",
        "docstring": "This module generates plots for the entire timeseries of digital inputs (DIG_INs) and amplifier (AMP) channels from data files in a specified directory. It handles two types of file structures: one file per channel and one file per signal type.\n\n- Sets up an argument parser to handle command-line inputs for plotting DIG_INs and AMP files.\n- Determines the directory path for data files using metadata from the `imp_metadata` function.\n- Creates a directory for storing plot outputs if it does not already exist.\n- Identifies the file structure (one file per channel or one file per signal type) by checking for the presence of specific files.\n- Reads amplifier data files and plots the data, handling both file structures.\n- Reads digital input data files and plots the data, handling both file structures.\n- Saves the generated plots in the specified directory."
    },
    {
        "filename": "blech_dat_file_join.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/utils/blech_dat_file_join.py",
        "docstring": "This module combines data files from two sessions and saves the combined files to a specified output directory.\n\n- Uses `easygui.diropenbox` to prompt the user for the directory of the first session's data files.\n- Changes the working directory to the first session's directory and lists all `.dat` files.\n- Prompts the user for the directory of the second session's data files.\n- Prompts the user for the output directory where the combined files will be saved.\n- Iterates over the `.dat` files from the first session, appends corresponding files from the second session, and saves the combined files to the output directory using the `os.system` command.\n- Copies the `info.rhd` file from the first session's directory to the output directory."
    },
    {
        "filename": "blech_exp_info_utils.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/utils/blech_exp_info_utils.py",
        "docstring": "This module provides utility functions for processing and validating strings, particularly in the context of experimental information.\n\n- `parse_csv(s, convert=str)`: Parses a comma-separated string into a list, with optional type conversion for each element.\n- `count_check(x)`: Checks if a string contains only numeric values.\n- `laser_check(x)`: Checks if a string contains exactly two numeric values.\n- `yn_check(x)`: Checks if a string is a yes/no response, accepting variations like 'y', 'yes', 'n', and 'no'."
    },
    {
        "filename": "blech_hdf5_repack.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/utils/blech_hdf5_repack.py",
        "docstring": "This module automates the process of cleaning and compressing an HDF5 file using the `ptrepack` tool. It involves selecting a directory, identifying the HDF5 file, and replacing it with a compressed version.\n\n- Prompts the user to select a directory using a GUI dialog and changes the working directory to the selected path.\n- Searches for an HDF5 file (with a `.h5` extension) in the selected directory.\n- Uses the `ptrepack` command-line tool to create a compressed and optimized copy of the HDF5 file named `tmp.h5`.\n- Deletes the original HDF5 file.\n- Renames the compressed file `tmp.h5` back to the original HDF5 file name."
    },
    {
        "filename": "blech_held_units_detect.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/utils/blech_held_units_detect.py",
        "docstring": "This module processes neural waveform data from two different days to identify units that are held across sessions using PCA and a J3 metric. It involves user interaction for file selection and output directory specification, and generates plots for visualization.\n\n- `calculate_J3(wf_day1, wf_day2)`: Computes the J3 metric as the ratio of J2 to J1 for given waveforms from two days.\n- `calculate_J2(wf_day1, wf_day2)`: Calculates J2 by determining the distance of daily mean waveforms from the overall mean.\n- `calculate_J1(wf_day1, wf_day2)`: Computes J1 by summing the Euclidean distances of waveforms from their daily means.\n- The script interacts with the user to select directories and files for waveform data from two days.\n- It calculates intra-unit J3 values by comparing segments of waveforms within the same day.\n- Inter-unit J3 values are calculated for units of the same type on the same electrode across days.\n- Units are marked as held if their inter-unit J3 is below a user-specified percentile of intra-unit J3.\n- Generates and saves plots comparing waveforms and distributions of J3 values.\n- Outputs a text file listing units identified as held across sessions."
    },
    {
        "filename": "blech_nex_convert.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/utils/blech_nex_convert.py",
        "docstring": "This module converts NeuroNexus (.nex) files to the blech_clust HDF5 format, specifically for data recorded on the old Plexon system. It is designed to work only with Python 2.7.x due to dependencies on the outdated Neo package.\n\n- Imports necessary libraries including Neo, SciPy, NumPy, EasyGUI, PyTables, OS, and Sys.\n- Ensures the script is run with Python 2.x.\n- Prompts the user to select a .nex file and a directory to save the converted HDF5 file.\n- Creates a new HDF5 file and reads the .nex file using Neo.\n- Retrieves event names from the recording and asks the user to confirm the digital input channels for trial splicing.\n- Collects user input for digital input channels and pre/post stimulus durations.\n- Deletes existing spike_trains node in the HDF5 file if present, and creates a new one.\n- Defines a `unit_descriptor` class for adding metadata about sorted units to a PyTables table.\n- Processes digital input channels to create spike train arrays and stores them in the HDF5 file.\n- Creates a mock `unit_descriptor` table to satisfy downstream analysis code requirements.\n- Checks for the presence of EMG data in the file and processes it if available, including downsampling and storing in a NumPy array.\n- Saves the EMG data to a .npy file and closes the HDF5 file."
    },
    {
        "filename": "blech_post_process_utils.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/utils/blech_post_process_utils.py",
        "docstring": "This module provides utilities for handling and processing electrophysiological data, focusing on sorting and clustering spike data from neural recordings. It includes classes and functions for managing sort files, handling unit descriptors, generating plots, and performing automatic processing of electrode data.\n\n- `sort_file_handler`: Manages sort files, iterates over clusters, and marks units as saved.\n  - `get_next_cluster`: Retrieves the next cluster to process.\n  - `mark_current_unit_saved`: Marks the current unit as saved.\n\n- `cluster_check`: Verifies if a string contains only cluster numbers.\n\n- `get_electrode_details`: Retrieves electrode details from the user or a sort file.\n\n- `load_data_from_disk`: Loads spike data for a given electrode and number of clusters.\n\n- `gen_select_cluster_plot`: Generates plots for user-supplied clusters.\n\n- `generate_cluster_plots`: Creates a grid of plots for each cluster based on split predictions.\n\n- `get_clustering_params`: Prompts the user for clustering parameters.\n\n- `get_split_cluster_choice`: Allows the user to select clusters for splitting.\n\n- `prepare_data`: Prepares data for clustering by normalizing and combining features.\n\n- `clean_memory_monitor_data`: Cleans up memory monitor data files.\n\n- `get_ISI_violations`: Calculates inter-spike interval violations.\n\n- `generate_datashader_plot`: Generates a datashader plot for visualizing waveforms.\n\n- `plot_merged_units`: Plots merged units with mean and standard deviation waveforms.\n\n- `gen_plot_auto_merged_clusters`: Plots all merged clusters on a sample plot.\n\n- `delete_raw_recordings`: Deletes raw recordings from an HDF5 file.\n\n- `generate_violations_warning`: Generates a warning based on ISI violations.\n\n- `unit_descriptor_handler`: Manages the unit descriptor table in an HDF5 file.\n  - Includes methods for saving units, checking and updating the descriptor table, and extracting metadata.\n\n- `sorted_unit_metadata` and `unit_descriptor`: Define metadata structures for sorted units.\n\n- `split_merge_signal`: Handles decisions about splitting or merging clusters.\n\n- `gen_autosort_plot`: Generates a summary plot for each electrode's cluster processing.\n\n- `get_cluster_props`: Calculates properties for each cluster, including waveforms, times, and chi-square p-values.\n\n- `calculate_merge_sets`: Determines which clusters to merge based on Mahalanobis distance and ISI violations.\n\n- `auto_process_electrode`: Processes a single electrode's data for automatic sorting, including loading data, calculating merge sets, and generating plots."
    },
    {
        "filename": "blech_process_utils.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/utils/blech_process_utils.py",
        "docstring": "This module is designed for handling and processing electrophysiological data, specifically focusing on clustering and classification of neural spike waveforms. It includes classes and functions for managing paths, clustering, classification, electrode data handling, and spike processing.\n\n- **path_handler**: Manages directory paths for the module.\n- **cluster_handler**: Handles clustering of spike waveforms.\n  - `check_classifier_data_exists`: Checks if classifier data exists.\n  - `return_training_set`: Returns a training set for clustering.\n  - `fit_manual_model`: Fits a Gaussian Mixture Model manually.\n  - `fit_auto_model`: Fits a Bayesian Gaussian Mixture Model automatically.\n  - `get_cluster_labels`: Retrieves cluster labels from a model.\n  - `perform_prediction`: Performs clustering and saves the model.\n  - `remove_outliers`: Removes outliers from clusters.\n  - `save_cluster_labels`: Saves cluster labels to a file.\n  - `calc_mahalanobis_distance_matrix`: Calculates and saves Mahalanobis distance matrix.\n  - `create_output_dir`: Creates directories for clustering results.\n  - `create_classifier_plots`: Generates plots for classifier results.\n  - `create_output_plots`: Generates output plots for clusters.\n\n- **classifier_handler**: Manages classification of waveforms.\n  - `download_neurecommend_models`: Downloads necessary models if not present.\n  - `return_waveform_classifier_params_path`: Returns the path to classifier parameters.\n  - `get_waveform_classifier_params`: Loads classifier parameters.\n  - `load_pipelines`: Loads feature and prediction pipelines.\n  - `classify_waveforms`: Classifies waveforms and saves results.\n  - `write_out_recommendations`: Writes out recommendations based on classification.\n  - `gen_plots`: Generates plots for predicted spikes and noise.\n\n- **electrode_handler**: Handles electrode data processing.\n  - `filter_electrode`: Filters raw electrode data.\n  - `cut_to_int_seconds`: Cuts data to integer seconds.\n  - `calc_recording_cutoff`: Calculates recording cutoff based on parameters.\n  - `make_cutoff_plot`: Generates a plot showing recording cutoff.\n  - `cutoff_electrode`: Cuts off electrode data at the calculated cutoff.\n\n- **spike_handler**: Processes spikes from electrode data.\n  - `extract_waveforms`: Extracts waveforms from filtered electrode data.\n  - `dejitter_spikes`: Dejitters spikes to correct timing.\n  - `extract_features`: Extracts features from spike waveforms.\n  - `return_feature`: Returns specific features from spike data.\n  - `write_out_spike_data`: Saves spike data to files.\n\n- **Utility Functions**:\n  - `ifisdir_rmdir`: Removes a directory if it exists.\n  - `return_cutoff_values`: Calculates cutoff values for electrode recording.\n  - `gen_window_plots`: Generates plots for data windows.\n  - `gen_datashader_plot`: Generates datashader plots for waveforms.\n  - `gen_isi_hist`: Generates inter-spike interval histograms.\n  - `remove_too_large_waveforms`: Removes waveforms that are too large.\n  - `feature_timeseries_plot`: Plots feature timeseries for clusters.\n  - `register_labels`, `calc_linkage`, `sort_label_array`, `perform_agg_clustering`, `plot_waveform_dendogram`, `trim_data`: Helper functions for agglomerative clustering."
    },
    {
        "filename": "blech_reload_amp_digs.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/utils/blech_reload_amp_digs.py",
        "docstring": "This module processes electrophysiological data files, organizes them into an HDF5 format, and reads data from various file types. It handles both single-file and multi-file data formats, creating necessary groups in the HDF5 file for raw data, EMG data, and digital inputs/outputs.\n\n- Imports necessary Python and custom modules for file handling, data processing, and path management.\n- Determines the type of data files present (either one file per signal type or one file per channel).\n- Creates or opens an HDF5 file, setting up groups for raw data, EMG data, and digital inputs/outputs.\n- Identifies and sorts lists of amplifier and digital input files based on the file type.\n- Reads metadata from an info file to determine sampling rates and other parameters.\n- Reads and processes digital input data and electrode channels, handling both single-file and multi-file formats.\n- Utilizes utility functions from imported modules to read and append data to the HDF5 file."
    },
    {
        "filename": "blech_spike_features.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/utils/blech_spike_features.py",
        "docstring": "This module provides functionality for creating a feature extraction pipeline using scikit-learn, specifically for processing and transforming data with PCA, energy, and amplitude features. It includes custom transformers and utility functions for handling data paths and metadata.\n\n- `EnergyFeature` class: A custom transformer that calculates the energy of input data.\n- `AmpFeature` class: A custom transformer that extracts the amplitude at a specified index from input data.\n- `zscore_custom(x)`: A function that applies z-score normalization along the last axis of the input data.\n- `return_feature_pipeline(data_dir_name)`: A function that constructs and returns a scikit-learn pipeline for feature extraction, including PCA, energy, and amplitude transformations, followed by feature scaling. It uses metadata to configure the pipeline based on the data directory provided."
    },
    {
        "filename": "blech_split_h5_files.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/utils/blech_split_h5_files.py",
        "docstring": "This script is designed for spike extraction and sorting from HDF5 files containing spike train data. It allows users to delete specific digital input nodes and trim spike trains to a consistent trial number.\n\n- Imports necessary libraries including numpy, tables, easygui, os, argparse, and pprint.\n- Uses argparse to get the directory containing the data files from the command line.\n- Changes the working directory to the specified directory or prompts the user to select it using a GUI.\n- Searches for an HDF5 file in the specified directory and opens it for reading and writing.\n- Lists digital input nodes under the '/spike_trains' group and retrieves their names and trial counts.\n- Prompts the user to delete any digital input nodes from the spike trains using a GUI.\n- Allows the user to trim all spike trains to a consistent trial number, specified via a GUI input.\n- Updates the spike arrays in the HDF5 file based on the user's trimming input.\n- Prints the final digital input names and their trial counts.\n- Flushes and closes the HDF5 file to save changes."
    },
    {
        "filename": "blech_utils.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/utils/blech_utils.py",
        "docstring": "This module provides utility functions and classes to support the `blech_clust` processing, including logging, path handling, and metadata management.\n\n- `Tee` class: Redirects output to both stdout/stderr and a log file.\n  - `__init__`: Initializes the Tee object with a log file.\n  - `write`: Writes data to both the log file and stdout.\n  - `flush`: Flushes the log file.\n  - `close`: Restores original stdout/stderr and closes the log file.\n\n- `path_handler` class: Manages paths related to the `blech_clust` directory.\n  - `__init__`: Initializes the path handler with the home directory and `blech_clust` directory path.\n\n- `log_wait` decorator: Implements log-waiting to handle file access issues in parallel processes.\n\n- `pipeline_graph_check` class: Ensures proper execution order of scripts based on a computation graph.\n  - `__init__`: Initializes the object, loads the computation graph, and checks it.\n  - `get_git_info`: Retrieves and prints git branch and commit information.\n  - `load_graph`: Loads the computation graph from a file.\n  - `make_full_path`: Constructs full file paths.\n  - `check_graph`: Verifies the presence of all scripts in the computation graph.\n  - `check_previous`: Checks if the previous script was executed successfully.\n  - `write_to_log`: Writes script execution attempts and completions to a log file.\n\n- `entry_checker` function: Validates user input against a check function, allowing exit with \"x\".\n\n- `imp_metadata` class: Manages metadata for a given directory, including file lists and parameters.\n  - `__init__`: Initializes the object and loads metadata.\n  - `get_dir_name`: Retrieves the directory name from arguments or user input.\n  - `get_file_list`: Lists files in the directory.\n  - `get_hdf5_name`: Finds the HDF5 file in the directory.\n  - `get_params_path`: Finds the parameters file in the directory.\n  - `get_layout_path`: Finds the layout CSV file in the directory.\n  - `load_params`: Loads parameters from the parameters file.\n  - `get_info_path`: Finds the info file in the directory.\n  - `load_info`: Loads information from the info file.\n  - `load_layout`: Loads layout data from the layout CSV file."
    },
    {
        "filename": "blech_waveforms_datashader.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/utils/blech_waveforms_datashader.py",
        "docstring": "This module provides functionality to create a datashader image from a numpy array of waveforms. It uses various libraries to process and visualize waveform data, allowing for optional downsampling and threshold marking.\n\n- `waveforms_datashader(waveforms, x_values, downsample=True, threshold=None, dir_name=\"datashader_temp\", ax=None)`: \n  - Accepts a numpy array of waveforms and x-values to create a datashader image.\n  - Optionally downsamples the waveforms to reduce the effects of upsampling.\n  - Constructs a pandas DataFrame to hold the waveform data, separating individual waveforms with NaNs.\n  - Uses datashader to create a canvas and aggregate the data, then exports the image.\n  - Reads the temporary image file and plots it using matplotlib.\n  - Optionally marks a threshold for spike selection on the plot.\n  - Cleans up temporary files and directories after processing.\n  - Returns the figure and axis for further customization or saving."
    },
    {
        "filename": "cluster_stability.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/utils/cluster_stability.py",
        "docstring": "Given an electrode, generate a hierarchical clustering of the spike waveforms data.\n\nGenerate 2 subplots for each clustering solution:\n        1. Dendrogram of the hierarchical clustering\n        2. Above plot but with the data points colored by cluster"
    },
    {
        "filename": "clustering.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/utils/clustering.py",
        "docstring": "This module provides functions for processing and analyzing electrophysiological data, specifically focusing on filtering, waveform extraction, dejittering, scaling, and clustering of neural spike data.\n\n- `get_filtered_electrode(data, freq, sampling_rate)`: Filters the input electrode data using a bandpass filter with specified frequency range and sampling rate.\n- `extract_waveforms_abu(filt_el, spike_snapshot, sampling_rate, threshold_mult)`: Extracts waveforms from filtered electrode data using a threshold-based method, returning slices, spike times, polarity, mean, and threshold.\n- `extract_waveforms_hannah(filt_el, spike_snapshot, sampling_rate, threshold_mult)`: Similar to `extract_waveforms_abu`, but uses a sliding thresholding approach to extract waveforms.\n- `extract_waveforms(filt_el, spike_snapshot, sampling_rate)`: Extracts waveforms based on threshold crossings, returning slices, spike times, mean, and threshold.\n- `dejitter(slices, spike_times, spike_snapshot, sampling_rate)`: Aligns waveforms by interpolating and finding the minimum point to reduce jitter in spike timing.\n- `dejitter_abu3(slices, spike_times, polarity, spike_snapshot, sampling_rate)`: Aligns waveforms without interpolation by flipping positive spikes and finding minima.\n- `scale_waveforms(slices_dejittered)`: Scales waveforms by their energy, returning scaled slices and their energy values.\n- `implement_pca(scaled_slices)`: Applies Principal Component Analysis (PCA) to the scaled waveforms, returning transformed data and explained variance ratios.\n- `clusterKMeans(data, n_clusters, n_iter, restarts, threshold)`: Clusters data using the KMeans algorithm, returning cluster labels.\n- `clusterGMM(data, n_clusters, n_iter, restarts, threshold)`: Clusters data using Gaussian Mixture Models (GMM), returning the best model, predictions, and Bayesian Information Criterion (BIC) score."
    },
    {
        "filename": "BAKS.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/utils/ephys_data/BAKS.py",
        "docstring": "This module provides a Python implementation of the Bayesian Adaptive Kernel Smoother (BAKS) as described in the paper with DOI: 10.1371/journal.pone.0206794. It is used to estimate the firing rate from spike times.\n\n- `BAKS(SpikeTimes, Time)`: Computes the firing rate given spike times and a time vector. It calculates a smoothing parameter `h` using Bayesian methods and applies a Gaussian kernel to estimate the firing rate over the specified time points. Returns an array representing the estimated firing rate."
    },
    {
        "filename": "ephys_data.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/utils/ephys_data/ephys_data.py",
        "docstring": "This module provides a class for streamlined electrophysiology data analysis, focusing on handling and analyzing data from multiple files. It includes features for automatic data loading, spike train and LFP data processing, firing rate calculation, digital input parsing, trial segmentation, region-based analysis, laser condition handling, and data quality checks.\n\n- `ephys_data`: Main class for data handling and analysis.\n  - `__init__`: Initializes the class with optional data directory.\n  - `calc_stft`: Computes the Short-Time Fourier Transform (STFT) of a trial.\n  - `parallelize`: Utilizes parallel processing to apply a function over an iterator.\n  - `_calc_conv_rates`: Calculates firing rates using a convolution method.\n  - `_calc_baks_rate`: Calculates firing rates using Bayesian Adaptive Kernel Smoother (BAKS).\n  - `get_hdf5_path`: Finds the HDF5 file in the specified directory.\n  - `convert_to_array`: Converts a list to a numpy array.\n  - `remove_node`: Removes a node from an HDF5 file.\n  - `extract_and_process`: Extracts and processes unit descriptors, spikes, firing rates, and LFPs.\n  - `separate_laser_data`: Separates data into laser on and off conditions.\n  - `get_unit_descriptors`: Extracts unit descriptors from an HDF5 file.\n  - `check_laser`: Checks for the presence of laser trials.\n  - `get_spikes`: Extracts spike arrays from HDF5 files.\n  - `separate_laser_spikes`: Separates spike arrays into laser on and off conditions.\n  - `extract_lfps`: Extracts LFPs from raw data files and saves them to HDF5.\n  - `get_lfp_channels`: Extracts parsed LFP channels.\n  - `get_lfps`: Initiates LFP extraction or retrieves LFP arrays from HDF5.\n  - `separate_laser_lfp`: Separates LFP arrays into laser on and off conditions.\n  - `firing_rate_method_selector`: Selects the method for firing rate calculation.\n  - `get_firing_rates`: Converts spikes to firing rates.\n  - `calc_palatability`: Calculates single neuron palatability from firing rates.\n  - `separate_laser_firing`: Separates firing rates into laser on and off conditions.\n  - `get_info_dict`: Loads information from a JSON file.\n  - `get_region_electrodes`: Extracts electrodes for each region from a JSON file.\n  - `get_region_units`: Extracts indices of units by region of electrodes.\n  - `return_region_spikes`: Returns spikes for a specified brain region.\n  - `get_region_firing`: Returns firing rates for a specified brain region.\n  - `get_lfp_electrodes`: Extracts indices of LFP electrodes by region.\n  - `get_stft`: Retrieves or calculates STFT and saves it to HDF5.\n  - `return_region_lfps`: Returns LFPs for each region and region names.\n  - `return_representative_lfp_channels`: Returns one electrode per region closest to the mean.\n  - `get_mean_stft_amplitude`: Calculates the mean STFT amplitude for each region.\n  - `get_trial_info_frame`: Loads trial information from a CSV file.\n  - `sequester_trial_inds`: Sequesters trials into categories based on tastes and laser conditions.\n  - `get_sequestered_spikes`: Sequesters spikes into categories based on tastes and laser conditions.\n  - `get_sequestered_firing`: Sequesters firing rates into categories based on tastes and laser conditions.\n  - `get_sequestered_data`: Sequesters both spikes and firing rates into categories based on tastes and laser conditions."
    },
    {
        "filename": "lfp_processing.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/utils/ephys_data/lfp_processing.py",
        "docstring": "lfp_processing.py - LFP extraction and processing utilities\n\nThis module provides functions for extracting and processing Local Field Potential (LFP) \ndata from electrophysiology recordings. Adapted from blech_clust LFP analysis tools.\n\nKey Functions:\n    extract_lfps: Main function for LFP extraction and processing\n    extract_emgs: Similar processing for EMG signals\n    get_filtered_electrode: Apply bandpass filtering to electrode signals\n    return_good_lfp_trials: Quality control for LFP trials\n    return_good_lfp_trial_inds: Get indices of good quality trials\n\nFeatures:\n    - Automatic trial segmentation based on digital inputs\n    - Configurable filtering parameters\n    - Data quality visualization\n    - EMG-specific processing\n    - Trial quality assessment using MAD thresholds\n\nDependencies:\n    - numpy, scipy, tables\n    - matplotlib for visualization\n\nUsage:\n    >>> from utils.ephys_data import lfp_processing\n    >>> \n    >>> # Extract LFPs with default parameters\n    >>> lfp_processing.extract_lfps(\n    ...     dir_name='/path/to/data',\n    ...     freq_bounds=[1, 300],          # Frequency range in Hz\n    ...     sampling_rate=30000,           # Original sampling rate\n    ...     taste_signal_choice='Start',   # Trial alignment\n    ...     fin_sampling_rate=1000,        # Final sampling rate\n    ...     dig_in_list=[0,1,2,3],        # Digital inputs to process\n    ...     trial_durations=[2000,5000]    # Pre/post trial durations\n    ... )\n    >>> \n    >>> # Extract EMGs similarly\n    >>> lfp_processing.extract_emgs(\n    ...     dir_name='/path/to/data',\n    ...     emg_electrode_nums=[0,1],      # EMG electrode numbers\n    ...     freq_bounds=[1, 300],\n    ...     sampling_rate=30000,\n    ...     taste_signal_choice='Start',\n    ...     fin_sampling_rate=1000,\n    ...     dig_in_list=[0,1,2,3],\n    ...     trial_durations=[2000,5000]\n    ... )\n    >>> \n    >>> # Filter individual electrode data\n    >>> filtered_data = lfp_processing.get_filtered_electrode(\n    ...     data=raw_data,\n    ...     low_pass=1,\n    ...     high_pass=300,\n    ...     sampling_rate=1000\n    ... )\n    >>> \n    >>> # Get good quality trials\n    >>> good_trials = lfp_processing.return_good_lfp_trials(\n    ...     data=lfp_data,\n    ...     MAD_threshold=3\n    ... )\n\nInstallation:\n    Required packages can be installed via pip:\n    $ pip install numpy scipy tables matplotlib"
    },
    {
        "filename": "visualize.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/utils/ephys_data/visualize.py",
        "docstring": "This module provides functions for visualizing neural data, including raster plots, heatmaps, and firing rate overviews.\n\n- `raster(ax, spike_array, marker='o', color=None)`: Creates a raster plot of spike data on the given axis. If no axis is provided, a new figure and axis are created.\n- `imshow(x, cmap='viridis')`: Displays a heatmap of the input data using the specified colormap, with settings for better visualization.\n- `gen_square_subplots(num, figsize=None, sharex=False, sharey=False)`: Generates a grid of subplots arranged in a square layout, returning the figure and axes.\n- `firing_overview(data, t_vec=None, y_values_vec=None, interpolation='nearest', cmap='jet', cmap_lims='individual', subplot_labels=None, zscore_bool=False, figsize=None, backend='pcolormesh')`: Generates heatmaps of firing rates from a 3D numpy array, with options for z-scoring, colormap limits, and subplot labels. Returns the figure and axes."
    },
    {
        "filename": "fix_laser_sampling_errors.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/utils/fix_laser_sampling_errors.py",
        "docstring": "This module corrects sampling errors in laser duration and onset latency data stored in an HDF5 file. It adjusts the recorded values to match intended pulse lengths and onsets, which may have been altered due to sampling at 30kHz.\n\n- Imports necessary modules and functions, including `tables`, `numpy`, `easygui`, `os`, `sys`, and `imp_metadata` from `blech_utils`.\n- Retrieves metadata and changes the working directory to the location of the HDF5 file.\n- Opens the HDF5 file in read/write mode.\n- Extracts laser parameters (onset and duration) from the metadata.\n- Iterates over digital input nodes in the HDF5 file to correct laser duration values by comparing them to intended durations and adjusting them to the closest valid value.\n- Iterates over digital input nodes to correct laser onset latency values similarly.\n- Flushes changes to the HDF5 file and closes it."
    },
    {
        "filename": "importrhdutilities.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/utils/importrhdutilities.py",
        "docstring": "This module provides functions for loading and processing Intan RHD data files, commonly used in electrophysiology research. It includes utilities for reading file headers, extracting and scaling data, applying filters, and plotting channel data.\n\n- `load_file(filename)`: Loads an RHD file, returning a dictionary of results and a boolean indicating if data is present.\n- `print_all_channel_names(result)`: Prints the names of all channels present in the result dictionary.\n- `find_channel_in_group(channel_name, signal_group)`: Finds a channel by name in a signal group, returning its presence and index.\n- `read_header(fid)`: Reads the header of an RHD file, extracting metadata and channel information.\n- `check_magic_number(fid)`: Verifies the file type by checking the magic number.\n- `read_version_number(header, fid)`: Reads and stores the version number from the file.\n- `set_num_samples_per_data_block(header)`: Sets the number of samples per data block based on the file version.\n- `read_sample_rate(header, fid)`: Reads the sample rate from the file.\n- `initialize_channels(header)`: Initializes empty lists for each data channel type.\n- `read_signal_summary(header, fid)`: Reads and stores signal group information from the header.\n- `get_bytes_per_data_block(header)`: Calculates the number of bytes per data block.\n- `read_one_data_block(data, header, indices, fid)`: Reads a data block into the data dictionary.\n- `read_analog_signals(fid, data, indices, samples_per_block, header)`: Reads analog signals into the data dictionary.\n- `read_digital_signals(fid, data, indices, samples_per_block, header)`: Reads digital signals into the data dictionary.\n- `data_to_result(header, data, result)`: Merges data into the result dictionary.\n- `plot_channel(channel_name, result)`: Plots data for a specified channel.\n- `calculate_data_size(header, filename, fid)`: Calculates the data size in the file.\n- `read_all_data_blocks(header, num_samples, num_blocks, fid)`: Reads all data blocks from the file.\n- `initialize_memory(header, num_samples)`: Pre-allocates memory for data arrays.\n- `parse_data(header, data)`: Parses raw data into readable forms.\n- `scale_timestamps(header, data)`: Scales timestamps to seconds.\n- `scale_analog_data(header, data)`: Scales analog data to appropriate units.\n- `apply_notch_filter(header, data)`: Applies a notch filter to amplifier data if needed.\n\nException Classes:\n- `UnrecognizedFileError`: Raised for unrecognized file types.\n- `UnknownChannelTypeError`: Raised for unknown channel types in the header.\n- `FileSizeError`: Raised for invalid file sizes.\n- `QStringError`: Raised for QString reading errors.\n- `ChannelNotFoundError`: Raised when a specified channel is not found for plotting."
    },
    {
        "filename": "infer_rnn_rates.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/utils/infer_rnn_rates.py",
        "docstring": "This module uses an Auto-regressive Recurrent Neural Network (RNN) to infer firing rates from electrophysiological data. It processes data for each taste separately, trains an RNN model, and saves the predicted firing rates and latent factors.\n\n- Parses command-line arguments to configure the RNN model, including data directory, training steps, hidden size, bin size, train-test split, PCA usage, retraining option, and time limits.\n- Loads configuration from a JSON file if not overridden by command-line arguments.\n- Loads spike data using the `ephys_data` class and preprocesses it, including binning and optional PCA.\n- Trains an RNN model for each taste, using a specified loss function (MSE) and saves the model and training artifacts.\n- Generates and saves various plots, including firing rate overviews, latent factors, and mean firing rates.\n- Writes the predicted firing rates and latent outputs to an HDF5 file for each taste.\n- Handles file paths and directories for saving models, plots, and outputs, ensuring necessary directories exist."
    },
    {
        "filename": "channel_corr.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/utils/qa_utils/channel_corr.py",
        "docstring": "This module provides utilities for quality assurance of channel data, focusing on correlation analysis between channels.\n\n- `get_all_channels(hf5_path, n_corr_samples=10000)`: Extracts all channels from an HDF5 file, specifically from nodes 'raw' and 'raw_emg'. It returns the channel data and their names, using a specified number of samples for correlation calculation.\n- `intra_corr(X)`: Computes the correlation matrix for all channels in the input array `X`, using Pearson correlation. It returns a matrix of correlation coefficients.\n- `gen_corr_output(corr_mat, plot_dir, threshold=0.9)`: Generates and saves plots of the raw and thresholded correlation matrices. It also outputs a table of thresholded correlation values and logs warnings for channels with correlations above the specified threshold."
    },
    {
        "filename": "drift_check.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/utils/qa_utils/drift_check.py",
        "docstring": "This module analyzes drift in firing rates across a session using statistical methods and visualizations. It performs ANOVA tests on baseline and post-stimulus firing rates, generates plots, and applies PCA and UMAP for dimensionality reduction.\n\n- `get_spike_trains(hf5_path)`: Extracts spike trains from an HDF5 file, returning a list of spike trains with dimensions (trials, units, time).\n- `array_to_df(array, dim_names)`: Converts a multi-dimensional array into a DataFrame with specified dimension names as columns.\n- Initializes the environment by setting up directories and loading metadata.\n- Loads spike train data from an HDF5 file and processes it for analysis.\n- Plots firing rates across the session, generating heatmaps and time series for each unit.\n- Performs 2-way ANOVA on baseline firing rates and repeated measures ANOVA on post-stimulus firing rates, saving results and warnings.\n- Conducts PCA and UMAP on firing rates to visualize trial-wise variations, saving the plots.\n- Logs the execution status of the script."
    },
    {
        "filename": "elbo_drift.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/utils/qa_utils/elbo_drift.py",
        "docstring": "This module uses a PyMC change point model to detect drift in neural data, performing model selection using the Evidence Lower Bound (ELBO). It processes electrophysiological data to identify changes in mean and variance over time.\n\n- `gaussian_changepoint_mean_var_2d(data_array, n_states, **kwargs)`: Constructs a PyMC model for detecting change points in a 2D Gaussian data array, modeling changes in both mean and variance across specified states.\n- Initializes the environment by setting up directories for output and artifacts, and performs a pipeline graph check.\n- Loads electrophysiological data, extracts spike trains, and processes them using PCA for dimensionality reduction.\n- Iteratively fits a change point model to the PCA-transformed data, evaluating different numbers of change points and repeats, and records ELBO values for model selection.\n- Aggregates results across different tastes, ranks the number of change points using median ELBO, and generates visualizations of the data and model fits.\n- Logs warnings if significant post-stimulus population drift is detected based on ELBO rankings."
    },
    {
        "filename": "unit_similarity.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/utils/qa_utils/unit_similarity.py",
        "docstring": "This module analyzes unit similarity in neural spike data, identifying and reporting units with high similarity. It processes data from HDF5 files, calculates similarity matrices, and generates visualizations and reports.\n\n- `unit_similarity_abu(all_spk_times)`: Computes a percentage-based similarity matrix for spike times across units, identifying overlaps within a 1 ms window.\n- `unit_similarity(this_unit_times, other_unit_times)`: A JIT-compiled function that counts overlapping spikes between two units within a 1 ms window.\n- `unit_similarity_NM(all_spk_times)`: Calculates a similarity matrix using a different method, comparing each unit pair and reporting progress.\n- `parse_collision_mat(unit_distances, similarity_cutoff)`: Parses the similarity matrix to find unit pairs exceeding a similarity threshold, returning unique pairs and their similarity values.\n- `plot_similarity_matrix(unit_distances, similarity_cutoff, output_dir)`: Generates and saves a plot of the raw and thresholded similarity matrices.\n- `write_out_similarties(unique_pairs, unique_pairs_collisions, waveform_counts, out_path, mode='w', waveform_count_cutoff=None)`: Writes unit similarity violations to a file, optionally filtering by waveform count similarity.\n- The script also includes a main execution block that processes input data, calculates similarities, generates reports, and logs the process."
    },
    {
        "filename": "ram_monitor.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/utils/ram_monitor.py",
        "docstring": "This module monitors RAM usage and logs it to a specified directory. It continuously records the used, total, and percentage of RAM usage every second until interrupted.\n\n- `monitor_ram(output_dir)`: Monitors RAM usage and writes the data to a log file named `ram_usage.log` in the specified output directory. The log includes a timestamp, used RAM in GB, total RAM in GB, and RAM usage percentage. The function runs indefinitely until a KeyboardInterrupt is received.\n- The script requires one command-line argument specifying the output directory for the log file. If the argument is not provided, it prints usage instructions and exits."
    },
    {
        "filename": "read_file.py",
        "filepath": "/home/abuzarmahmood/projects/blech_clust/utils/read_file.py",
        "docstring": "This module provides functionality for handling digital inputs and reading data from Intan format files, saving the data into HDF5 format. It includes a class for managing digital inputs and several functions for reading and processing data.\n\n- `DigInHandler` class: Manages digital input files across different formats.\n  - `__init__`: Initializes the handler with a data directory and file type.\n  - `get_dig_in_files`: Retrieves digital input files and their metadata based on the specified file type.\n  - `get_trial_data`: Extracts trial data (start and end times) from digital input files.\n  - `write_out_frame`: Saves the digital input data frame to a CSV file.\n  - `load_dig_in_frame`: Loads the digital input data frame from a CSV file.\n\n- `read_traditional_intan`: Reads traditional Intan format data and saves it to an HDF5 file, organizing amplifier and EMG data.\n\n- `read_emg_channels`: Reads EMG data from amplifier channels and saves it to an HDF5 file.\n\n- `read_electrode_channels`: Reads electrode data from amplifier channels, excluding EMG channels, and saves it to an HDF5 file.\n\n- `read_electrode_emg_channels_single_file`: Reads both electrode and EMG data from a single file and saves it to an HDF5 file, organizing data by channel index."
    }
]