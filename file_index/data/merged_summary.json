[
    {
        "__init__.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/__init__.py",
            "imports": [],
            "definitions": {
                "functions": [],
                "classes": [],
                "methods": []
            },
            "functionality": []
        }
    },
    {
        "blech_clean_slate.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/blech_clean_slate.py",
            "imports": [
                "glob",
                "os",
                "shutil",
                "sys",
                "utils.blech_utils.imp_metadata"
            ],
            "definitions": {
                "functions": [],
                "classes": [],
                "methods": []
            },
            "functionality": [
                "This script resets a data folder to an almost raw form by deleting most processing files while retaining specific file types such as info files and sorting tables."
            ]
        }
    },
    {
        "blech_clust.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/blech_clust.py",
            "imports": [
                "argparse",
                "ast.literal_eval",
                "glob",
                "json",
                "multiprocessing",
                "numpy",
                "os",
                "pandas",
                "pylab",
                "shutil",
                "sys",
                "tables",
                "utils.blech_process_utils.path_handler",
                "utils.blech_utils.entry_checker",
                "utils.blech_utils.imp_metadata",
                "utils.blech_utils.pipeline_graph_check",
                "utils.importrhdutilities.read_header",
                "utils.qa_utils.channel_corr",
                "utils.read_file"
            ],
            "definitions": {
                "functions": [
                    "generate_processing_scripts"
                ],
                "classes": [
                    "HDF5Handler"
                ],
                "methods": [
                    "HDF5Handler.initialize_groups",
                    "HDF5Handler.setup_hdf5"
                ]
            },
            "functionality": [
                "blech_clust.py - Main script for processing and clustering neural recording data",
                "- HDF5Handler: Handles HDF5 file operations for blech_clust",
                "- generate_processing_scripts: Generate bash scripts for running single and parallel processing\n\nArgs:\n    dir_name: Directory containing the data\n    blech_clust_dir: Directory containing blech_clust code\n    electrode_layout_frame: DataFrame with electrode layout info\n    all_electrodes: List of all electrode indices\n    all_params_dict: Dictionary of processing parameters",
                "- __init__: Initialize HDF5 handler\n\nArgs:\n    dir_name: Directory containing the data\n    force_run: Whether to force operations without asking user",
                "- setup_hdf5: Setup or load HDF5 file",
                "- initialize_groups: Initialize HDF5 groups"
            ]
        }
    },
    {
        "blech_common_avg_reference.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/blech_common_avg_reference.py",
            "imports": [
                "easygui",
                "glob",
                "json",
                "numpy",
                "os",
                "sys",
                "tables",
                "tqdm.tqdm",
                "utils.blech_utils.imp_metadata",
                "utils.blech_utils.pipeline_graph_check"
            ],
            "definitions": {
                "functions": [
                    "get_electrode_by_name"
                ],
                "classes": [],
                "methods": []
            },
            "functionality": [
                "blech_common_avg_reference.py - Common Average Reference (CAR) processing for neural recordings",
                "- get_electrode_by_name: Get the electrode data from the list of raw electrodes\nby the name of the electrode"
            ]
        }
    },
    {
        "blech_exp_info.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/blech_exp_info.py",
            "imports": [
                "argparse",
                "json",
                "numpy",
                "os",
                "pandas",
                "re",
                "tqdm.tqdm",
                "utils.blech_utils.entry_checker",
                "utils.blech_utils.imp_metadata",
                "utils.blech_utils.pipeline_graph_check",
                "utils.importrhdutilities.load_file",
                "utils.importrhdutilities.read_header",
                "utils.read_file.DigInHandler"
            ],
            "definitions": {
                "functions": [
                    "confirm_check",
                    "count_check",
                    "float_check",
                    "laser_check",
                    "pal_check",
                    "parse_csv",
                    "taste_check",
                    "yn_check"
                ],
                "classes": [],
                "methods": []
            },
            "functionality": [
                "This module generates a file containing relevant experimental information for a given dataset. It processes data files to extract and organize details such as animal name, experiment type, date, timestamp, regions recorded from, electrode layout, taste concentrations, palatability ranks, laser parameters, and miscellaneous notes."
            ]
        }
    },
    {
        "blech_make_arrays.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/blech_make_arrays.py",
            "imports": [
                "ast.literal_eval",
                "numpy",
                "os",
                "pandas",
                "sys",
                "tables",
                "tqdm.tqdm",
                "utils.blech_process_utils.return_cutoff_values",
                "utils.blech_utils.imp_metadata",
                "utils.blech_utils.pipeline_graph_check",
                "utils.clustering.get_filtered_electrode",
                "utils.read_file.DigInHandler"
            ],
            "definitions": {
                "functions": [
                    "create_emg_trials_for_digin",
                    "create_spike_trains_for_digin"
                ],
                "classes": [],
                "methods": []
            },
            "functionality": [
                "This module processes neural and EMG data from an HDF5 file, extracting and organizing spike trains and EMG trials based on digital input events. It also handles metadata and logs the processing steps."
            ]
        }
    },
    {
        "blech_post_process.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/blech_post_process.py",
            "imports": [
                "argparse",
                "functools.partial",
                "glob.glob",
                "matplotlib",
                "multiprocessing.Pool",
                "multiprocessing.cpu_count",
                "numpy",
                "os",
                "pandas",
                "pylab",
                "re",
                "sklearn.mixture.GaussianMixture",
                "tables",
                "utils.blech_post_process_utils",
                "utils.blech_utils.entry_checker",
                "utils.blech_utils.imp_metadata",
                "utils.blech_utils.pipeline_graph_check",
                "utils.blech_waveforms_datashader"
            ],
            "definitions": {
                "functions": [],
                "classes": [],
                "methods": []
            },
            "functionality": [
                "blech_post_process.py - Post-processing and unit sorting for neural recordings"
            ]
        }
    },
    {
        "blech_process.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/blech_process.py",
            "imports": [
                "argparse",
                "datetime",
                "feature_engineering_pipeline.*",
                "json",
                "json",
                "numpy",
                "os",
                "pathlib",
                "pylab",
                "sys",
                "utils.blech_process_utils",
                "utils.blech_spike_features",
                "utils.blech_utils.imp_metadata",
                "utils.blech_utils.pipeline_graph_check",
                "warnings"
            ],
            "definitions": {
                "functions": [],
                "classes": [],
                "methods": []
            },
            "functionality": [
                "This module processes single electrode waveforms for spike detection and clustering. It includes data loading, preprocessing, spike extraction, feature extraction, and clustering. The module also supports classification and logging of the processing steps."
            ]
        }
    },
    {
        "blech_units_characteristics.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/blech_units_characteristics.py",
            "imports": [
                "easygui",
                "glob",
                "itertools",
                "itertools.product",
                "json",
                "matplotlib.pyplot",
                "numpy",
                "os",
                "pandas",
                "pingouin",
                "scipy.stats.spearmanr",
                "scipy.stats.ttest_rel",
                "scipy.stats.zscore",
                "seaborn",
                "sys",
                "tables",
                "tqdm.tqdm",
                "utils.blech_utils.entry_checker",
                "utils.blech_utils.imp_metadata",
                "utils.blech_utils.pipeline_graph_check",
                "utils.ephys_data.ephys_data",
                "utils.ephys_data.visualize"
            ],
            "definitions": {
                "functions": [
                    "palatability_corr"
                ],
                "classes": [],
                "methods": []
            },
            "functionality": [
                "This module performs various analyses on neural data, focusing on firing rates, responsiveness, discriminability, palatability, and dynamicity of neurons. It generates plots and saves results to disk and an HDF5 file."
            ]
        }
    },
    {
        "blech_units_plot.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/blech_units_plot.py",
            "imports": [
                "easygui",
                "matplotlib.pyplot",
                "numpy",
                "os",
                "shutil",
                "sys",
                "tables",
                "tqdm.tqdm",
                "tqdm.trange",
                "utils.blech_process_utils.gen_isi_hist",
                "utils.blech_utils.imp_metadata",
                "utils.blech_utils.pipeline_graph_check",
                "utils.blech_waveforms_datashader"
            ],
            "definitions": {
                "functions": [],
                "classes": [],
                "methods": []
            },
            "functionality": [
                "This module processes neural data stored in an HDF5 file, generating and saving plots of unit waveforms, inter-spike interval (ISI) histograms, and spike count histograms. It also logs the execution of the processing pipeline."
            ]
        }
    },
    {
        "__init__.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/emg/__init__.py",
            "imports": [],
            "definitions": {
                "functions": [],
                "classes": [],
                "methods": []
            },
            "functionality": []
        }
    },
    {
        "emg_filter.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/emg/emg_filter.py",
            "imports": [
                "ast",
                "glob",
                "numpy",
                "os",
                "pandas",
                "scipy.signal.butter",
                "scipy.signal.filtfilt",
                "shutil",
                "sys",
                "tables",
                "utils.blech_utils.imp_metadata",
                "utils.blech_utils.pipeline_graph_check"
            ],
            "definitions": {
                "functions": [],
                "classes": [],
                "methods": []
            },
            "functionality": [
                "This module processes EMG (electromyography) data by subtracting signals, filtering them, and saving the results. It handles data loading, filtering, differencing, and identifying significant trials based on activity changes."
            ]
        }
    },
    {
        "emg_freq_plot.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/emg/emg_freq_plot.py",
            "imports": [
                "easygui",
                "glob",
                "json",
                "matplotlib.pyplot",
                "numpy",
                "os",
                "pandas",
                "seaborn",
                "sys",
                "tables",
                "tqdm.tqdm",
                "tqdm.trange",
                "utils.blech_utils.imp_metadata"
            ],
            "definitions": {
                "functions": [],
                "classes": [],
                "methods": []
            },
            "functionality": [
                "This module processes and visualizes EMG data related to gapes and LTPS (licking tongue protrusions) in response to different taste and laser conditions. It reads data from CSV and NPY files, processes it, and generates plots for analysis."
            ]
        }
    },
    {
        "emg_freq_post_process.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/emg/emg_freq_post_process.py",
            "imports": [
                "easygui",
                "glob",
                "json",
                "numpy",
                "os",
                "pandas",
                "sys",
                "tables",
                "utils.blech_utils.imp_metadata",
                "utils.blech_utils.pipeline_graph_check"
            ],
            "definitions": {
                "functions": [],
                "classes": [],
                "methods": []
            },
            "functionality": [
                "This module performs post-processing cleanup of files created by `emg_local_BSA_execute.py`, saving output files to an HDF5 file under the node `emg_BSA_results`. It processes EMG data, removes unnecessary nodes, and calculates specific frequency arrays."
            ]
        }
    },
    {
        "emg_freq_setup.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/emg/emg_freq_setup.py",
            "imports": [
                "glob.glob",
                "json",
                "multiprocessing",
                "numpy",
                "os",
                "pandas",
                "pylab",
                "shutil",
                "sys",
                "tables",
                "utils.blech_process_utils.path_handler",
                "utils.blech_utils.imp_metadata",
                "utils.blech_utils.pipeline_graph_check"
            ],
            "definitions": {
                "functions": [],
                "classes": [],
                "methods": []
            },
            "functionality": [
                "This module sets up EMG data for running the envelope of EMG recordings through a local Bayesian Spectrum Analysis (BSA). It requires an installation of R and the R library BaSAR. The script is a preparatory step for `emg_local_BSA_execute.py`."
            ]
        }
    },
    {
        "emg_local_BSA_execute.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/emg/emg_local_BSA_execute.py",
            "imports": [
                "datetime",
                "easygui",
                "numpy",
                "os",
                "readline",
                "rpy2.robjects",
                "rpy2.robjects.numpy2ri",
                "rpy2.robjects.packages.importr",
                "rpy2.robjects.r",
                "sys",
                "utils.blech_utils.pipeline_graph_check"
            ],
            "definitions": {
                "functions": [],
                "classes": [
                    "Logger"
                ],
                "methods": [
                    "Logger.append_time",
                    "Logger.flush",
                    "Logger.write"
                ]
            },
            "functionality": [
                "This module performs a local Bayesian Spectral Analysis (BSA) on a single trial of Electromyography (EMG) data using the High-Performance Computing (HPC) environment. It integrates Python with R to execute the BSA using the BaSAR package."
            ]
        }
    },
    {
        "emg_local_STFT_execute.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/emg/emg_local_STFT_execute.py",
            "imports": [
                "datetime",
                "easygui",
                "json",
                "numpy",
                "os",
                "scipy.signal",
                "sys",
                "utils.blech_utils.pipeline_graph_check"
            ],
            "definitions": {
                "functions": [
                    "calc_stft",
                    "calc_stft_mode_freq"
                ],
                "classes": [
                    "Logger"
                ],
                "methods": [
                    "Logger.append_time",
                    "Logger.flush",
                    "Logger.write"
                ]
            },
            "functionality": [
                "This module performs a local BSA (Blind Source Analysis) on a single trial of EMG (Electromyography) data, running on a High-Performance Computing (HPC) environment. It calculates the Short-Time Fourier Transform (STFT) and processes the results for further analysis.",
                "- calc_stft: trial : 1D array\nmax_freq : where to lob off the transform\ntime_range_tuple : (start,end) in seconds, time_lims of spectrogram\n                        from start of trial snippet`",
                "- calc_stft_mode_freq: Calculate the mode frequency of the STFT of a signal\n\nInputs:\n    dat : 1D array, signal to be analyzed\n    stft_params : dict, parameters for STFT calculation\n    BSA_output : bool, whether to output in a similar format to the BSA \n\nOutputs:\n    freq_vec : 1D array, frequency vector for STFT\n    t_vec : 1D array, time vector for STFT\n    weight_mean : 1D array, time averaged mode frequency"
            ]
        }
    },
    {
        "emg_reload_raw_data.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/emg/utils/emg_reload_raw_data.py",
            "imports": [
                "easygui",
                "glob",
                "json",
                "multiprocessing",
                "numpy",
                "os",
                "pandas",
                "sys",
                "tables",
                "tqdm.tqdm",
                "utils.read_file"
            ],
            "definitions": {
                "functions": [],
                "classes": [],
                "methods": []
            },
            "functionality": [
                "This script is designed to reload raw EMG data into an HDF5 file if it has been deleted without using the `blech_clust.py` script. It handles file directory selection, checks for necessary experimental information, and processes EMG data files."
            ]
        }
    },
    {
        "update_docstrings.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/update_docstrings.py",
            "imports": [
                "os"
            ],
            "definitions": {
                "functions": [
                    "update_all_docstrings",
                    "update_docstring"
                ],
                "classes": [],
                "methods": []
            },
            "functionality": [
                "This module is designed to update the docstrings of Python files within a specified directory, particularly for the `blech_clust` package. It inserts a standard docstring template at the beginning of files that lack one.",
                "- update_docstring: Function to update the doc-string of a given Python file.",
                "- update_all_docstrings: Walk through the directory and update all Python file doc-strings."
            ]
        }
    },
    {
        "__init__.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/utils/__init__.py",
            "imports": [],
            "definitions": {
                "functions": [],
                "classes": [],
                "methods": []
            },
            "functionality": []
        }
    },
    {
        "blech_channel_profile.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/utils/blech_channel_profile.py",
            "imports": [
                "argparse",
                "glob",
                "numpy",
                "os",
                "pylab",
                "sys",
                "tqdm.tqdm",
                "utils.blech_utils.imp_metadata"
            ],
            "definitions": {
                "functions": [],
                "classes": [],
                "methods": []
            },
            "functionality": [
                "This module generates plots for the entire timeseries of digital inputs (DIG_INs) and amplifier (AMP) channels from data files in a specified directory. It handles two types of file structures: one file per channel and one file per signal type."
            ]
        }
    },
    {
        "blech_dat_file_join.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/utils/blech_dat_file_join.py",
            "imports": [
                "easygui",
                "os",
                "sys"
            ],
            "definitions": {
                "functions": [],
                "classes": [],
                "methods": []
            },
            "functionality": [
                "This module combines data files from two sessions and saves the combined files to a specified output directory."
            ]
        }
    },
    {
        "blech_exp_info_utils.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/utils/blech_exp_info_utils.py",
            "imports": [
                "re"
            ],
            "definitions": {
                "functions": [
                    "count_check",
                    "laser_check",
                    "parse_csv",
                    "yn_check"
                ],
                "classes": [],
                "methods": []
            },
            "functionality": [
                "This module provides utility functions for processing and validating strings, particularly in the context of experimental information.",
                "- parse_csv: Parse comma-separated values with optional type conversion",
                "- count_check: Check if string contains only numbers",
                "- laser_check: Check if string contains exactly 2 numbers",
                "- yn_check: Check if string is yes/no response"
            ]
        }
    },
    {
        "blech_hdf5_repack.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/utils/blech_hdf5_repack.py",
            "imports": [
                "easygui",
                "os",
                "tables"
            ],
            "definitions": {
                "functions": [],
                "classes": [],
                "methods": []
            },
            "functionality": [
                "This module automates the process of cleaning and compressing an HDF5 file using the `ptrepack` tool. It involves selecting a directory, identifying the HDF5 file, and replacing it with a compressed version."
            ]
        }
    },
    {
        "blech_held_units_detect.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/utils/blech_held_units_detect.py",
            "imports": [
                "easygui",
                "matplotlib.pyplot",
                "numpy",
                "os",
                "scipy.spatial.distance.cdist",
                "seaborn",
                "sklearn.decomposition.PCA",
                "sys",
                "tables"
            ],
            "definitions": {
                "functions": [
                    "calculate_J1",
                    "calculate_J2",
                    "calculate_J3"
                ],
                "classes": [],
                "methods": []
            },
            "functionality": [
                "This module processes neural waveform data from two different days to identify units that are held across sessions using PCA and a J3 metric. It involves user interaction for file selection and output directory specification, and generates plots for visualization."
            ]
        }
    },
    {
        "blech_nex_convert.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/utils/blech_nex_convert.py",
            "imports": [
                "easygui",
                "neo.io.NeuroExplorerIO",
                "numpy",
                "os",
                "scipy.signal.resample",
                "sys",
                "tables"
            ],
            "definitions": {
                "functions": [],
                "classes": [
                    "unit_descriptor"
                ],
                "methods": []
            },
            "functionality": [
                "This module converts NeuroNexus (.nex) files to the blech_clust HDF5 format, specifically for data recorded on the old Plexon system. It is designed to work only with Python 2.7.x due to dependencies on the outdated Neo package."
            ]
        }
    },
    {
        "blech_post_process_utils.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/utils/blech_post_process_utils.py",
            "imports": [
                "argparse",
                "ast",
                "datetime.datetime",
                "easygui",
                "hashlib",
                "matplotlib.image",
                "numpy",
                "os",
                "pandas",
                "pylab",
                "re",
                "scipy.stats.chisquare",
                "tables",
                "tqdm.tqdm",
                "utils.blech_process_utils.gen_isi_hist",
                "utils.blech_utils.entry_checker",
                "utils.blech_utils.imp_metadata",
                "utils.blech_waveforms_datashader"
            ],
            "definitions": {
                "functions": [
                    "auto_process_electrode",
                    "calculate_merge_sets",
                    "clean_memory_monitor_data",
                    "cluster_check",
                    "delete_raw_recordings",
                    "gen_autosort_plot",
                    "gen_plot_auto_merged_clusters",
                    "gen_select_cluster_plot",
                    "generate_cluster_plots",
                    "generate_datashader_plot",
                    "generate_violations_warning",
                    "get_ISI_violations",
                    "get_cluster_props",
                    "get_clustering_params",
                    "get_electrode_details",
                    "get_split_cluster_choice",
                    "load_data_from_disk",
                    "plot_merged_units",
                    "prepare_data"
                ],
                "classes": [
                    "sort_file_handler",
                    "sorted_unit_metadata",
                    "split_merge_signal",
                    "unit_descriptor",
                    "unit_descriptor_handler"
                ],
                "methods": [
                    "sort_file_handler.get_next_cluster",
                    "sort_file_handler.mark_current_unit_saved",
                    "split_merge_signal.ask_split",
                    "split_merge_signal.check_merge_clusters",
                    "split_merge_signal.check_split_sort_file",
                    "unit_descriptor_handler._rename_unit",
                    "unit_descriptor_handler.check_table_matches_saved_units",
                    "unit_descriptor_handler.check_unit_descriptor_table",
                    "unit_descriptor_handler.generate_hash",
                    "unit_descriptor_handler.get_latest_unit_name",
                    "unit_descriptor_handler.get_metadata_from_units",
                    "unit_descriptor_handler.get_unit_properties",
                    "unit_descriptor_handler.resort_units",
                    "unit_descriptor_handler.return_unit_descriptor_table",
                    "unit_descriptor_handler.save_unit",
                    "unit_descriptor_handler.table_to_frame",
                    "unit_descriptor_handler.write_unit_descriptor_from_sorted_units"
                ]
            },
            "functionality": [
                "This module provides utilities for handling and processing electrophysiological data, focusing on sorting and clustering spike data from neural recordings. It includes classes and functions for managing sort files, handling unit descriptors, generating plots, and performing automatic processing of electrode data.",
                "- get_electrode_details: Ask user for electrode number, number of clusters, and cluster numbers",
                "- load_data_from_disk: Load data from disk",
                "- gen_select_cluster_plot: Generate plots for the clusters initially supplied by the user",
                "- generate_cluster_plots: Generate grid of plots for each cluster\n\nInputs:\n    split_predictions: array of cluster numbers for each split \n    spike_waveforms: array of waveforms\n    spike_times: array of spike times\n    n_clusters: number of clusters\n    this_cluster: cluster number to plot\n\n\n**NOTE**: This cluster specifies the cluster number in the original\nclustering, not the split clustering. Split cluster numbers are\nspecified in split_predictions\n**NOTE**: This is a stupid way of doing this.",
                "- get_clustering_params: Ask user for clustering parameters",
                "- prepare_data: Prepare data for clustering",
                "- clean_memory_monitor_data: Clean memory monitor data",
                "- get_ISI_violations: Get ISI violations",
                "- generate_datashader_plot: Generate datashader plot",
                "- plot_merged_units: Plot merged units\n\nInputs:\n    cluster_waveforms: list of arrays (n_waveforms, n_samples)\n    cluster_labels: list of cluster labels\n    max_n_per_cluster: maximum number of waveforms to plot per cluster\n    sd_bound: number of standard deviations to plot for each cluster\n\nOutputs:\n    fig, ax: figure and axis objects",
                "- gen_plot_auto_merged_clusters: Plot all merged clusters on sample plot\n**NOTE** This is different from plot_merged_units\n\nInputs:\n    spike_waveforms - (n_spikes, n_samples) array of spike waveforms\n    spike_times - (n_spikes,) array of spike times\n    split_predictions - (n_spikes,) array of cluster labels\n    sampling_rate - sampling rate of the recording\n    final_merge_sets - list of lists of clusters to merge\n\nOutputs:\n    fig - matplotlib figure handle\n    ax - matplotlib axis handle",
                "- delete_raw_recordings: Delete raw recordings from hdf5 file\n\nInputs:\n    hf5: hdf5 file object\n    hdf5_name: name of hdf5 file\n\nOutputs:\n    hf5: new hdf5 file object",
                "- unit_descriptor_handler: Class to handle the unit_descriptor table in the hdf5 file\n\nOps to handle mismatch between unit_descriptor and sorted_units:\n    1- Resort units according to electrode number using\n            unit metadata\n    2- Recreate unit_descriptor table from scratch using\n            metadata from sorted_units",
                "- gen_autosort_plot: For each electrode, generate a summary of how each\ncluster was processed\n\nInputs:\n    subcluster_prob: list of probabilities of each subcluster \n    subcluster_waveforms: list of waveforms of each subcluster \n    chi_out: chi-square p-value of each subcluster's probability distribution\n    mean_waveforms: list of mean waveforms of given subcluster\n    std_waveforms: list of std of waveforms of given subcluster \n    subcluster_times: list of times of each subcluster \n    autosort_output_dir: absolute path of directory to save output\n    n_max_plot: maximum number of waveforms to plot\n\nOutputs:\n    Plots of following quantities\n    1. prob distribution (indicate chi-square p-value in titles)\n    2. ISI distribution\n    3. Histogram of spikes over time\n    4. Mean +/- std of waveform\n    5. A finite amount of raw waveforms",
                "- get_cluster_props: Calculate the following properties for each cluster:\n- waveforms\n- times\n- probs\n- mean waveform\n- std waveform\n- chi_square p-value on classifier probability distribution\n\nInputs:\n    split_predictions: array of cluster labels\n    spike_waveforms: array of spike waveforms\n    clf_prob: array of classifier probabilities\n    spike_times: array of spike times\n\nOutputs:\n    subcluster_inds: list of indices for each cluster\n    subcluster_waveforms: list of waveforms for each cluster\n    subcluster_prob: list of probabilities for each cluster\n    subcluster_times: list of times for each cluster\n    mean_waveforms: list of mean waveforms for each cluster\n    std_waveforms: list of std waveforms for each cluster\n    fin_bool: list of booleans indicating whether the cluster is a wanted unit\n    fin_bool_dict: dictionary of booleans indicating whether the cluster is a wanted unit",
                "- calculate_merge_sets: Calculate which clusters to merge based on mahalanobis distance\nand ISI violations\n\nInputs:\n    mahal_mat: (n_clusters, n_clusters) matrix of mahalanobis distances\n    mahal_thresh: float, threshold for mahalanobis distance\n    isi_threshs: list of floats, thresholds for ISI violations\n    split_predictions: (n_spikes,) array of cluster predictions\n    spike_times: (n_spikes,) array of spike times\n    sample_rate: float, sample rate of recording\n\nOutputs:\n    final_merge_sets: list of tuples, \n                      each tuple is a pair of clusters to merge",
                "- auto_process_electrode: Process a single electrode's data for autosort\n\nArgs:\n    electrode_num: The electrode number to process\n    process_params: Tuple containing processing parameters:\n        - max_autosort_clusters\n        - auto_params\n        - chi_square_alpha  \n        - count_threshold\n        - sampling_rate\n        - data_dir",
                "- get_next_cluster: Get the next cluster to process",
                "- get_latest_unit_name: Get the name for the next unit to be saved ",
                "- generate_hash: Generate a 10 character hash for the unit based on electrode and waveform count\n\nArgs:\n    electrode_number: int, electrode number \n    waveform_count: int, number of waveforms in unit\n    \nReturns:\n    str: 10 character hash",
                "- save_unit: Save unit to hdf5 file",
                "- return_unit_descriptor_table: Return the unit descriptor table",
                "- table_to_frame: Convert the unit_descriptor table to a pandas dataframe",
                "- check_table_matches_saved_units: Check that the unit_descriptor table matches the saved units",
                "- _rename_unit: Rename units in both unit_descriptor table and sorted_units directory\nin HDF5 file, using hash as identifier",
                "- get_metadata_from_units: Extracts unit metadata from saved_units directory",
                "- resort_units: 1) Get metadata from units\n2) Rename units sorted by electrode\n3) Update unit_descriptor table",
                "- write_unit_descriptor_from_sorted_units: Generate unit descriptor table from metadata\npresent in sorted units",
                "- get_unit_properties: Ask user for unit properties and save in both unit_descriptor table\nand sorted_units directory in HDF5 file",
                "- __init__: First check whether there are multiple clusters to merge\nIf not, check whether there is a split/sort file\nIf not, ask whether to split"
            ]
        }
    },
    {
        "blech_process_utils.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/utils/blech_process_utils.py",
            "imports": [
                "glob.glob",
                "joblib.load",
                "json",
                "matplotlib",
                "matplotlib.patches.ConnectionPatch",
                "numpy",
                "os",
                "pandas",
                "pylab",
                "scipy.cluster.hierarchy.cut_tree",
                "scipy.cluster.hierarchy.dendrogram",
                "scipy.cluster.hierarchy.linkage",
                "scipy.spatial.distance.mahalanobis",
                "scipy.stats.zscore",
                "shutil",
                "sklearn.cluster.AgglomerativeClustering",
                "sklearn.cluster.KMeans",
                "sklearn.mixture.BayesianGaussianMixture",
                "sklearn.mixture.GaussianMixture",
                "subprocess",
                "tables",
                "utils.blech_waveforms_datashader",
                "utils.clustering"
            ],
            "definitions": {
                "functions": [
                    "calc_linkage",
                    "feature_timeseries_plot",
                    "gen_datashader_plot",
                    "gen_isi_hist",
                    "gen_window_plots",
                    "ifisdir_rmdir",
                    "perform_agg_clustering",
                    "plot_waveform_dendogram",
                    "register_labels",
                    "remove_too_large_waveforms",
                    "return_cutoff_values",
                    "sort_label_array",
                    "trim_data"
                ],
                "classes": [
                    "classifier_handler",
                    "cluster_handler",
                    "electrode_handler",
                    "path_handler",
                    "spike_handler"
                ],
                "methods": [
                    "classifier_handler.classify_waveforms",
                    "classifier_handler.download_neurecommend_models",
                    "classifier_handler.gen_plots",
                    "classifier_handler.get_waveform_classifier_params",
                    "classifier_handler.load_pipelines",
                    "classifier_handler.return_waveform_classifier_params_path",
                    "classifier_handler.write_out_recommendations",
                    "cluster_handler.calc_mahalanobis_distance_matrix",
                    "cluster_handler.check_classifier_data_exists",
                    "cluster_handler.create_classifier_plots",
                    "cluster_handler.create_output_dir",
                    "cluster_handler.create_output_plots",
                    "cluster_handler.fit_auto_model",
                    "cluster_handler.fit_manual_model",
                    "cluster_handler.get_cluster_labels",
                    "cluster_handler.perform_prediction",
                    "cluster_handler.remove_outliers",
                    "cluster_handler.return_training_set",
                    "cluster_handler.save_cluster_labels",
                    "electrode_handler.adjust_to_sampling_rate",
                    "electrode_handler.calc_recording_cutoff",
                    "electrode_handler.cut_to_int_seconds",
                    "electrode_handler.cutoff_electrode",
                    "electrode_handler.filter_electrode",
                    "electrode_handler.make_cutoff_plot",
                    "spike_handler.dejitter_spikes",
                    "spike_handler.extract_features",
                    "spike_handler.extract_waveforms",
                    "spike_handler.return_feature",
                    "spike_handler.write_out_spike_data"
                ]
            },
            "functionality": [
                "This module is designed for handling and processing electrophysiological data, specifically focusing on clustering and classification of neural spike waveforms. It includes classes and functions for managing paths, clustering, classification, electrode data handling, and spike processing.",
                "- cluster_handler: Class to handle clustering steps",
                "- classifier_handler: Class to handler classifier steps",
                "- electrode_handler: Class to handle electrode data",
                "- spike_handler: Class to handler processing of spikes",
                "- return_cutoff_values: Return the cutoff values for the electrode recording\n\nInputs:\n    filt_el: numpy array (in \n    sampling_rate: int\n    voltage_cutoff: float\n    max_breach_rate: float\n    max_secs_above_cutoff: float\n    max_mean_breach_rate_persec: float\n\nOutputs:\n    breach_rate: float\n    breaches_per_sec: numpy array\n    secs_above_cutoff: int\n    mean_breach_rate_persec: float\n    recording_cutoff: int",
                "- register_labels: Register labels from one level to the next\nInput:\n    x: labels at level n\n    y: labels at level n+1\nOutput:\n    map_dict: dictionary mapping labels from level n to level n+1",
                "- calc_linkage: Calculate linkage matrix from sklearn agglomerative clustering model\nInput:\n    model: sklearn agglomerative clustering model\nOutput:\n    linkage_matrix: linkage matrix",
                "- sort_label_array: Resort map_dict so that children are always in order\nWe will need a remapping at every level\n\nInput:\n    label_array: levels x samples\n\nOutput:\n    label_array: levels x samples",
                "- perform_agg_clustering: Perform agglomerative clustering on features\nInput:\n    features: samples x features\n    max_clusters: maximum number of clusters to consider\nOutput:\n    cut_label_array: levels x samples\n    map_dict: dictionary mapping labels from level n to level n+1\n    clust_range: range of clusters considered",
                "- trim_data: Agglomerative clustering doesn't like large datasets\nIf we have more than n_max samples, we will randomly sample n_max samples\n\nInput:\n    data: samples x features\n    n_max: maximum number of samples to use\n\nOutput:\n    data: samples x features",
                "- return_training_set: Return training set for clustering",
                "- fit_manual_model: Cluster waveforms",
                "- fit_auto_model: Since the dirichlet process WILL find more clusters for more data,\n(whereas we know that more spikes doens't mean more neurons),\nwe need to keep the \"COUNT\" constant\nOne way to do this is to use KMeans centroids as data points",
                "- get_cluster_labels: Get cluster labels",
                "- perform_prediction: Perform clustering\nModel needs to be saved for calculation of mahalanobis distances",
                "- remove_outliers: Clear large waveforms",
                "- calc_mahalanobis_distance_matrix: Calculates matrix of mahalanobis distances between all pairs of clusters\nSaves matrix to file",
                "- create_classifier_plots: For each cluster, plot:\n    1. Pred Spikes\n    2. Pred Noise\n    3. Distribution and timeseries of probability\n    4. Histogram of prediction probability\n    5. Histogram of times for spikes and noise\n\nInput data can come from classifier_handler",
                "- download_neurecommend_models: If models are not present in the right place\nAttempt to download them",
                "- return_waveform_classifier_params_path: Inner function for get_waveform_classifier_params\nso that it can also be called externally",
                "- load_pipelines: Load feature and prediction pipelines",
                "- classify_waveforms: Classify waveforms",
                "- write_out_recommendations: If number of predicted spikes > classifier_params['min_suggestion_count']\nWrite out electrode number, count, mean prediction probability, and 5,95th percentiles",
                "- cut_to_int_seconds: Cut data to have integer number of seconds\n\ndata: numpy array\nsampling_rate: int",
                "- make_cutoff_plot: Makes a plot showing where the recording was cut off at\n\nfilt_el: numpy array\nrecording_cutoff: int",
                "- extract_waveforms: Extract waveforms from filtered electrode",
                "- dejitter_spikes: Dejitter spikes",
                "- write_out_spike_data: Save the pca_slices, energy and amplitudes to the\nspike_waveforms folder for this electrode\nSave slices/spike waveforms and their times to their respective folders"
            ]
        }
    },
    {
        "blech_reload_amp_digs.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/utils/blech_reload_amp_digs.py",
            "imports": [
                "glob",
                "json",
                "multiprocessing",
                "numpy",
                "os",
                "pandas",
                "shutil",
                "sys",
                "sys",
                "tables",
                "utils.blech_process_utils.path_handler",
                "utils.blech_utils.entry_checker",
                "utils.blech_utils.imp_metadata",
                "utils.qa_utils",
                "utils.read_file"
            ],
            "definitions": {
                "functions": [],
                "classes": [],
                "methods": []
            },
            "functionality": [
                "This module processes electrophysiological data files, organizes them into an HDF5 format, and reads data from various file types. It handles both single-file and multi-file data formats, creating necessary groups in the HDF5 file for raw data, EMG data, and digital inputs/outputs."
            ]
        }
    },
    {
        "blech_spike_features.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/utils/blech_spike_features.py",
            "imports": [
                "numpy",
                "os",
                "scipy.stats.zscore",
                "sklearn.base.BaseEstimator",
                "sklearn.base.TransformerMixin",
                "sklearn.decomposition.PCA",
                "sklearn.pipeline.FeatureUnion",
                "sklearn.pipeline.Pipeline",
                "sklearn.preprocessing.FunctionTransformer",
                "sklearn.preprocessing.StandardScaler",
                "sys",
                "utils.blech_process_utils.path_handler",
                "utils.blech_utils.imp_metadata"
            ],
            "definitions": {
                "functions": [
                    "return_feature_pipeline",
                    "zscore_custom"
                ],
                "classes": [
                    "AmpFeature",
                    "EnergyFeature"
                ],
                "methods": [
                    "AmpFeature.fit",
                    "AmpFeature.fit_transform",
                    "AmpFeature.transform",
                    "EnergyFeature.fit",
                    "EnergyFeature.fit_transform",
                    "EnergyFeature.transform"
                ]
            },
            "functionality": [
                "This module provides functionality for creating a feature extraction pipeline using scikit-learn, specifically for processing and transforming data with PCA, energy, and amplitude features. It includes custom transformers and utility functions for handling data paths and metadata."
            ]
        }
    },
    {
        "blech_split_h5_files.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/utils/blech_split_h5_files.py",
            "imports": [
                "argparse",
                "easygui",
                "numpy",
                "os",
                "pprint.pprint",
                "tables"
            ],
            "definitions": {
                "functions": [],
                "classes": [],
                "methods": []
            },
            "functionality": [
                "This script is designed for spike extraction and sorting from HDF5 files containing spike train data. It allows users to delete specific digital input nodes and trim spike trains to a consistent trial number."
            ]
        }
    },
    {
        "blech_utils.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/utils/blech_utils.py",
            "imports": [
                "datetime.datetime",
                "easygui",
                "glob",
                "json",
                "os",
                "pandas",
                "sys",
                "time"
            ],
            "definitions": {
                "functions": [
                    "entry_checker",
                    "log_wait",
                    "wrapper"
                ],
                "classes": [
                    "Tee",
                    "imp_metadata",
                    "path_handler",
                    "pipeline_graph_check"
                ],
                "methods": [
                    "Tee.close",
                    "Tee.flush",
                    "Tee.write",
                    "imp_metadata.get_dir_name",
                    "imp_metadata.get_file_list",
                    "imp_metadata.get_hdf5_name",
                    "imp_metadata.get_info_path",
                    "imp_metadata.get_layout_path",
                    "imp_metadata.get_params_path",
                    "imp_metadata.load_info",
                    "imp_metadata.load_layout",
                    "imp_metadata.load_params",
                    "pipeline_graph_check.check_graph",
                    "pipeline_graph_check.check_previous",
                    "pipeline_graph_check.get_git_info",
                    "pipeline_graph_check.load_graph",
                    "pipeline_graph_check.make_full_path",
                    "pipeline_graph_check.write_to_log"
                ]
            },
            "functionality": [
                "This module provides utility functions and classes to support the `blech_clust` processing, including logging, path handling, and metadata management.",
                "- Tee: Tee output to both stdout/stderr and a log file",
                "- pipeline_graph_check: Check that parent scripts executed properly before running child scripts \n\n1) Check that computation graph is present\n2) Check that all scripts mentioned in computation graph are present\n3) For current run script, check that previous run script is present and executed successfully\n4) If prior exeuction is not present or failed, generate warning, give user option to override, else exit",
                "- get_git_info: Get branch and commit info, and print\nIf not in git repo, print warning",
                "- load_graph: Load computation graph from file, if file is present",
                "- check_graph: Check that all scripts mentioned in computation graph are present\nAlso, flatten out the graph for easier checking downstream",
                "- check_previous: Check that previous run script is present and executed successfully",
                "- write_to_log: Write to log file\n\ntype = 'attempted' : script was attempted\ntype = 'completed' : script was completed"
            ]
        }
    },
    {
        "blech_waveforms_datashader.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/utils/blech_waveforms_datashader.py",
            "imports": [
                "datashader",
                "datashader.transfer_functions",
                "datashader.utils.export_image",
                "functools.partial",
                "imageio.imread",
                "matplotlib.pyplot",
                "numpy",
                "os",
                "pandas",
                "shutil"
            ],
            "definitions": {
                "functions": [
                    "waveforms_datashader",
                    "y_transform"
                ],
                "classes": [],
                "methods": []
            },
            "functionality": [
                "This module provides functionality to create a datashader image from a numpy array of waveforms. It uses various libraries to process and visualize waveform data, allowing for optional downsampling and threshold marking."
            ]
        }
    },
    {
        "cluster_stability.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/utils/cluster_stability.py",
            "imports": [
                "glob.glob",
                "matplotlib",
                "matplotlib",
                "numpy",
                "os",
                "pandas",
                "pylab",
                "re",
                "seaborn",
                "sys",
                "tables"
            ],
            "definitions": {
                "functions": [
                    "load_cluster_predictions",
                    "load_electrode_data",
                    "return_clustering_solutions"
                ],
                "classes": [],
                "methods": []
            },
            "functionality": [
                "Given an electrode, generate a hierarchical clustering of the spike waveforms data.",
                "- load_electrode_data: Load data from disk",
                "- load_cluster_predictions: Load cluster predictions from disk",
                "- return_clustering_solutions: Find all clustering solutions for a given electrode"
            ]
        }
    },
    {
        "clustering.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/utils/clustering.py",
            "imports": [
                "numpy",
                "pylab",
                "scipy.interpolate.interp1d",
                "scipy.signal.butter",
                "scipy.signal.fftconvolve",
                "scipy.signal.filtfilt",
                "sklearn.cluster.KMeans",
                "sklearn.decomposition.PCA",
                "sklearn.mixture.GaussianMixture"
            ],
            "definitions": {
                "functions": [
                    "clusterGMM",
                    "clusterKMeans",
                    "dejitter",
                    "dejitter_abu3",
                    "extract_waveforms",
                    "extract_waveforms_abu",
                    "extract_waveforms_hannah",
                    "get_filtered_electrode",
                    "implement_pca",
                    "scale_waveforms"
                ],
                "classes": [],
                "methods": []
            },
            "functionality": [
                "This module provides functions for processing and analyzing electrophysiological data, specifically focusing on filtering, waveform extraction, dejittering, scaling, and clustering of neural spike data.",
                "- dejitter_abu3: Dejitter without interpolation and see what breaks :P"
            ]
        }
    },
    {
        "BAKS.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/utils/ephys_data/BAKS.py",
            "imports": [
                "numpy",
                "scipy.special"
            ],
            "definitions": {
                "functions": [
                    "BAKS"
                ],
                "classes": [],
                "methods": []
            },
            "functionality": [
                "This module provides a Python implementation of the Bayesian Adaptive Kernel Smoother (BAKS) as described in the paper with DOI: 10.1371/journal.pone.0206794. It is used to estimate the firing rate from spike times."
            ]
        }
    },
    {
        "ephys_data.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/utils/ephys_data/ephys_data.py",
            "imports": [
                ".lfp_processing",
                "BAKS.BAKS",
                "copy",
                "easygui",
                "glob",
                "itertools.product",
                "joblib.Parallel",
                "joblib.cpu_count",
                "joblib.delayed",
                "json",
                "multiprocessing",
                "numpy",
                "numpy",
                "os",
                "pandas",
                "pprint.pprint",
                "scipy",
                "scipy.signal",
                "scipy.special.gamma",
                "scipy.stats.spearmanr",
                "scipy.stats.zscore",
                "tables",
                "tqdm.tqdm",
                "warnings"
            ],
            "definitions": {
                "functions": [
                    "calc_firing_func",
                    "calc_firing_func",
                    "check_firing_rate_params"
                ],
                "classes": [
                    "ephys_data"
                ],
                "methods": [
                    "ephys_data._calc_baks_rate",
                    "ephys_data._calc_conv_rates",
                    "ephys_data.calc_palatability",
                    "ephys_data.calc_stft",
                    "ephys_data.check_laser",
                    "ephys_data.convert_to_array",
                    "ephys_data.extract_and_process",
                    "ephys_data.extract_lfps",
                    "ephys_data.firing_rate_method_selector",
                    "ephys_data.get_firing_rates",
                    "ephys_data.get_hdf5_path",
                    "ephys_data.get_info_dict",
                    "ephys_data.get_lfp_channels",
                    "ephys_data.get_lfp_electrodes",
                    "ephys_data.get_lfps",
                    "ephys_data.get_mean_stft_amplitude",
                    "ephys_data.get_region_electrodes",
                    "ephys_data.get_region_firing",
                    "ephys_data.get_region_units",
                    "ephys_data.get_sequestered_data",
                    "ephys_data.get_sequestered_firing",
                    "ephys_data.get_sequestered_spikes",
                    "ephys_data.get_spikes",
                    "ephys_data.get_stft",
                    "ephys_data.get_trial_info_frame",
                    "ephys_data.get_unit_descriptors",
                    "ephys_data.parallelize",
                    "ephys_data.remove_node",
                    "ephys_data.return_region_lfps",
                    "ephys_data.return_region_spikes",
                    "ephys_data.return_representative_lfp_channels",
                    "ephys_data.separate_laser_data",
                    "ephys_data.separate_laser_firing",
                    "ephys_data.separate_laser_lfp",
                    "ephys_data.separate_laser_spikes",
                    "ephys_data.sequester_trial_inds"
                ]
            },
            "functionality": [
                "This module provides a class for streamlined electrophysiology data analysis, focusing on handling and analyzing data from multiple files. It includes features for automatic data loading, spike train and LFP data processing, firing rate calculation, digital input parsing, trial segmentation, region-based analysis, laser condition handling, and data quality checks.",
                "- calc_stft: trial : 1D array\nmax_freq : where to lob off the transform\ntime_range_tuple : (start,end) in seconds, time_lims of spectrogram\n                        from start of trial snippet`\nFs : sampling rate\nsignal_window : window size for spectrogram\nwindow_overlap : overlap between windows",
                "- _calc_conv_rates: step_size \nwindow_size :: params :: In milliseconds. For moving window firing rate\n                        calculation\nsampling_rate :: params :: In ms, To calculate total number of bins \nspike_array :: params :: N-D array with time as last dimension",
                "- _calc_baks_rate: resolution : resolution of output firing rate (sec)\ndt : resolution of input spike trains (sec)",
                "- get_hdf5_path: Look for the hdf5 file in the directory",
                "- __init__: data_dirs : where to look for hdf5 file\n    : get_data() loads data from this directory",
                "- get_unit_descriptors: Extract unit descriptors from HDF5 file",
                "- get_spikes: Extract spike arrays from specified HD5 files",
                "- separate_laser_spikes: Separate spike arrays into laser on and off conditions",
                "- extract_lfps: Wrapper function to extract LFPs from raw data files and save to HDF5\nLoads relevant information for .info file",
                "- get_lfp_channels: Extract Parsed_LFP_channels\nThis is done separately from \"get_lfps\" to avoid\nthe overhead of reading the large lfp arrays",
                "- get_lfps: Wrapper function to either\n- initiate LFP extraction, or\n- pull LFP arrays from HDF5 file",
                "- separate_laser_lfp: Separate spike arrays into laser on and off conditions",
                "- get_firing_rates: Converts spikes to firing rates\n\nRequires:\n    - spikes\n    - firing_rate_params\n\nGenerates:\n    - firing_list : list of firing rates for each taste\n        - each element is a 3D array of shape (n_trials, n_neurons, n_timepoints)\n    - firing_array : 4D array of firing rates\n    - normalized_firing : 4D array of normalized firing rates\n    - all_firing_array : 3D array of all firing rates\n    - all_normalized_firing : 3D array of all normalized firing rates",
                "- calc_palatability: Calculate single neuron (absolute) palatability from firing rates\n\nRequires:\n    - info_dict\n        - palatability ranks\n        - taste names\n    - firing rates\n\nGenerates:\n    - pal_df : pandas dataframe\n        - shape: tastes x 3 cols (dig_ins, taste_names, pal_ranks)\n    - pal_array : np.array\n        - shape : neurons x time_bins",
                "- separate_laser_firing: Separate spike arrays into laser on and off conditions",
                "- get_region_electrodes: If the appropriate json file is present in the data_dir,\nextract the electrodes for each region",
                "- get_region_units: Extracts indices of units by region of electrodes\n`",
                "- get_lfp_electrodes: Extracts indices of lfp_electrodes according to region",
                "- get_stft: If STFT present in HDF5 then retrieve it\nIf not, then calculate it and save it into HDF5 file\n\nInputs:\n    recalculate: bool, if True then recalculate STFT\n    dat_type: list of strings, options are 'raw', 'amplitude', 'phase'\n    write_out: bool, if True then write out STFT to HDF5 file",
                "- return_region_lfps: Return list containing LFPs for each region and region names",
                "- return_representative_lfp_channels: Return one electrode per region that is closest to the mean",
                "- sequester_trial_inds: Sequester trials into different categories:\n    - Tastes\n    - Laser conditions",
                "- get_sequestered_spikes: Sequester spikes into different categories:\n    - Tastes\n    - Laser conditions",
                "- get_sequestered_firing: Sequester spikes into different categories:\n    - Tastes\n    - Laser conditions",
                "- get_sequestered_data: Sequester spikes and firing into different categories:\n    - Tastes\n    - Laser conditions"
            ]
        }
    },
    {
        "lfp_processing.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/utils/ephys_data/lfp_processing.py",
            "imports": [
                "glob",
                "matplotlib.pyplot",
                "numpy",
                "os",
                "re",
                "scipy.signal.butter",
                "scipy.signal.filtfilt",
                "scipy.stats.median_abs_deviation",
                "scipy.stats.median_absolute_deviation",
                "shutil",
                "tables",
                "tqdm.tqdm",
                "tqdm.trange"
            ],
            "definitions": {
                "functions": [
                    "extract_emgs",
                    "extract_lfps",
                    "get_filtered_electrode",
                    "return_good_lfp_trial_inds",
                    "return_good_lfp_trials"
                ],
                "classes": [],
                "methods": []
            },
            "functionality": [
                "lfp_processing.py - LFP extraction and processing utilities",
                "- extract_emgs: Extract EMG data from raw recordings\n    \nArgs:\n    dir_name: Directory containing data files\n    emg_electrode_nums: List of electrode numbers for EMG channels\n    freq_bounds: [low, high] frequency bounds for filtering\n    sampling_rate: Original sampling rate\n    taste_signal_choice: 'Start' or 'End' for trial alignment\n    fin_sampling_rate: Final sampling rate after downsampling\n    dig_in_list: List of digital input channels to process\n    trial_durations: [pre_trial, post_trial] durations in ms",
                "- return_good_lfp_trial_inds: Return boolean array of good trials (for all channels) based on MAD threshold\nRemove trials based on deviation from median LFP per trial\n\nInputs:\n    data : shape (n_channels, n_trials, n_timepoints)\n    MAD_threshold : number of MADs to use as threshold for individual timepoints\n\nOutputs:\n    good_trials_bool : boolean array of good trials",
                "- return_good_lfp_trials: Return good trials (for all channels) based on MAD threshold\ndata : shape (n_channels, n_trials, n_timepoints)\nMAD_threshold : number of MADs to use as threshold for individual timepoints"
            ]
        }
    },
    {
        "visualize.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/utils/ephys_data/visualize.py",
            "imports": [
                "numpy",
                "pylab",
                "scipy.stats.zscore"
            ],
            "definitions": {
                "functions": [
                    "firing_overview",
                    "gen_square_subplots",
                    "imshow",
                    "raster"
                ],
                "classes": [],
                "methods": []
            },
            "functionality": [
                "This module provides functions for visualizing neural data, including raster plots, heatmaps, and firing rate overviews.",
                "- imshow: Decorator function for more viewable firing rate heatmaps",
                "- gen_square_subplots: number of subplots to generate",
                "- firing_overview: Takes 3D numpy array as input and rolls over first dimension\nto generate images over last 2 dimensions\nE.g. (neuron x trial x time) will generate heatmaps of firing\n    for every neuron\n\nInputs:\n    data: 3D numpy array\n    t_vec: time vector\n    y_values_vec: y values vector\n    cmap: colormap\n    min_val: minimum value for colormap\n    max_val: maximum value for colormap\n    cmap_lims: 'individual' or 'shared'\n    subplot_labels: labels for subplots\n    zscore_bool: zscore data\n    figsize: size of figure\n    backend: 'pcolormesh' or 'imshow\n\nOutputs:\n    fig: figure handle\n    ax: axis handle"
            ]
        }
    },
    {
        "fix_laser_sampling_errors.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/utils/fix_laser_sampling_errors.py",
            "imports": [
                "blech_utils.imp_metadata",
                "easygui",
                "numpy",
                "os",
                "sys",
                "tables"
            ],
            "definitions": {
                "functions": [],
                "classes": [],
                "methods": []
            },
            "functionality": [
                "This module corrects sampling errors in laser duration and onset latency data stored in an HDF5 file. It adjusts the recorded values to match intended pulse lengths and onsets, which may have been altered due to sampling at 30kHz."
            ]
        }
    },
    {
        "importrhdutilities.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/utils/importrhdutilities.py",
            "imports": [
                "math",
                "matplotlib.pyplot",
                "numpy",
                "os",
                "struct",
                "time"
            ],
            "definitions": {
                "functions": [
                    "add_channel_information",
                    "add_num_channels",
                    "add_signal_group_information",
                    "advance_indices",
                    "append_new_channel",
                    "apply_notch_filter",
                    "bytes_per_signal_type",
                    "calculate_data_size",
                    "calculate_iir",
                    "calculate_iir_parameters",
                    "calculate_num_samples",
                    "check_end_of_file",
                    "check_magic_number",
                    "data_to_result",
                    "extract_digital_data",
                    "find_channel_in_group",
                    "find_channel_in_header",
                    "get_bytes_per_data_block",
                    "get_timestamp_signed",
                    "header_to_result",
                    "initialize_channels",
                    "initialize_memory",
                    "load_file",
                    "notch_filter",
                    "parse_data",
                    "plot_channel",
                    "plural",
                    "print_all_channel_names",
                    "print_header_summary",
                    "print_names_in_group",
                    "print_progress",
                    "print_record_time_summary",
                    "read_all_data_blocks",
                    "read_analog_signal_type",
                    "read_analog_signals",
                    "read_digital_signal_type",
                    "read_digital_signals",
                    "read_eval_board_mode",
                    "read_freq_settings",
                    "read_header",
                    "read_impedance_test_frequencies",
                    "read_new_channel",
                    "read_notch_filter_frequency",
                    "read_notes",
                    "read_num_temp_sensor_channels",
                    "read_one_data_block",
                    "read_qstring",
                    "read_reference_channel",
                    "read_sample_rate",
                    "read_signal_summary",
                    "read_timestamps",
                    "read_version_number",
                    "scale_analog_data",
                    "scale_timestamps",
                    "set_frequency_parameters",
                    "set_num_samples_per_data_block",
                    "set_sample_rates"
                ],
                "classes": [
                    "ChannelNotFoundError",
                    "FileSizeError",
                    "QStringError",
                    "UnknownChannelTypeError",
                    "UnrecognizedFileError"
                ],
                "methods": []
            },
            "functionality": [
                "This module provides functions for loading and processing Intan RHD data files, commonly used in electrophysiology research. It includes utilities for reading file headers, extracting and scaling data, applying filters, and plotting channel data.",
                "- load_file: Loads .rhd file with provided filename, returning 'result' dict and\n'data_present' Boolean.",
                "- print_all_channel_names: Searches through all present signal types in 'result' dict, and prints\nthe names of these channels. Useful, for example, to determine names of\nchannels that can be plotted.",
                "- print_names_in_group: Searches through all channels in this group and print them.\n    ",
                "- find_channel_in_group: Finds a channel with this name in this group, returning whether or not\nit's present and, if so, the position of this channel in signal_group.",
                "- find_channel_in_header: Looks through all present signal groups in header, searching for\n'channel_name'. If found, return the signal group and the index of that\nchannel within the group.",
                "- read_header: Reads the Intan File Format header from the given file.\n    ",
                "- check_magic_number: Checks magic number at beginning of file to verify this is an Intan\nTechnologies RHD data file.",
                "- read_version_number: Reads version number (major and minor) from fid. Stores them into\nheader['version']['major'] and header['version']['minor'].",
                "- set_num_samples_per_data_block: Determines how many samples are present per data block (60 or 128),\ndepending on version. Data files v2.0 or later have 128 samples per block,\notherwise 60.",
                "- read_sample_rate: Reads sample rate from fid. Stores it into header['sample_rate'].\n    ",
                "- read_freq_settings: Reads amplifier frequency settings from fid. Stores them in 'freq' dict.\n    ",
                "- read_notch_filter_frequency: Reads notch filter mode from fid, and stores frequency (in Hz) in\n'header' and 'freq' dicts.",
                "- read_impedance_test_frequencies: Reads desired and actual impedance test frequencies from fid, and stores\nthem (in Hz) in 'freq' dicts.",
                "- read_notes: Reads notes as QStrings from fid, and stores them as strings in\nheader['notes'] dict.",
                "- read_num_temp_sensor_channels: Stores number of temp sensor channels in\nheader['num_temp_sensor_channels']. Temp sensor data may be saved from\nversions 1.1 and later.",
                "- read_eval_board_mode: Stores eval board mode in header['eval_board_mode']. Board mode is saved\nfrom versions 1.3 and later.",
                "- read_reference_channel: Reads name of reference channel as QString from fid, and stores it as\na string in header['reference_channel']. Data files v2.0 or later include\nreference channel.",
                "- set_sample_rates: Determines what the sample rates are for various signal types, and\nstores them in 'freq' dict.",
                "- set_frequency_parameters: Stores frequency parameters (set in other functions) in\nheader['frequency_parameters']",
                "- initialize_channels: Creates empty lists for each type of data channel and stores them in\n'header' dict.",
                "- read_signal_summary: Reads signal summary from data file header and stores information for\nall signal groups and their channels in 'header' dict.",
                "- add_signal_group_information: Adds information for a signal group and all its channels to 'header'\ndict.",
                "- add_channel_information: Reads a new channel's information from fid and appends it to 'header'\ndict.",
                "- read_new_channel: Reads a new channel's information from fid.\n    ",
                "- append_new_channel: \"Appends 'new_channel' to 'header' dict depending on if channel is\nenabled and the signal type.",
                "- add_num_channels: Adds channel numbers for all signal types to 'header' dict.\n    ",
                "- header_to_result: Merges header information from .rhd file into a common 'result' dict.\nIf any fields have been allocated but aren't relevant (for example, no\nchannels of this type exist), does not copy those entries into 'result'.",
                "- print_header_summary: Prints summary of contents of RHD header to console.\n    ",
                "- get_timestamp_signed: Checks version (major and minor) in 'header' to determine if data\nrecorded from this version of Intan software saved timestamps as signed or\nunsigned integer. Returns True if signed, False if unsigned.",
                "- plural: Utility function to pluralize words based on the number of items.\n    ",
                "- get_bytes_per_data_block: Calculates the number of bytes in each 60 or 128 sample datablock.",
                "- bytes_per_signal_type: Calculates the number of bytes, per data block, for a signal type\nprovided the number of samples (per data block), the number of enabled\nchannels, and the size of each sample in bytes.",
                "- read_one_data_block: Reads one 60 or 128 sample data block from fid into data,\nat the location indicated by indices.",
                "- read_timestamps: Reads timestamps from binary file as a NumPy array, indexing them\ninto 'data'.",
                "- read_analog_signals: Reads all analog signal types present in RHD files: amplifier_data,\naux_input_data, supply_voltage_data, temp_sensor_data, and board_adc_data,\ninto 'data' dict.",
                "- read_digital_signals: Reads all digital signal types present in RHD files: board_dig_in_raw\nand board_dig_out_raw, into 'data' dict.",
                "- read_analog_signal_type: Reads data from binary file as a NumPy array, indexing them into\n'dest', which should be an analog signal type within 'data', for example\ndata['amplifier_data'] or data['aux_input_data']. Each sample is assumed\nto be of dtype 'uint16'.",
                "- read_digital_signal_type: Reads data from binary file as a NumPy array, indexing them into\n'dest', which should be a digital signal type within 'data', either\ndata['board_dig_in_raw'] or data['board_dig_out_raw'].",
                "- data_to_result: Merges data from all present signals into a common 'result' dict. If\nany signal types have been allocated but aren't relevant (for example,\nno channels of this type exist), does not copy those entries into 'result'.",
                "- plot_channel: Plots all data associated with channel specified as 'channel_name' in\n'result' dict.",
                "- read_qstring: Reads Qt style QString.\n\nThe first 32-bit unsigned number indicates the length of the string\n(in bytes). If this number equals 0xFFFFFFFF, the string is null.\n\nStrings are stored as unicode.",
                "- calculate_data_size: Calculates how much data is present in this file. Returns:\ndata_present: Bool, whether any data is present in file\nfilesize: Int, size (in bytes) of file\nnum_blocks: Int, number of 60 or 128-sample data blocks present\nnum_samples: Int, number of samples present in file",
                "- calculate_num_samples: Calculates number of samples for each signal type, storing the results\nin num_samples dict for later use.",
                "- print_record_time_summary: Prints summary of how much recorded data is present in RHD file\nto console.",
                "- read_all_data_blocks: Reads all data blocks present in file, allocating memory for and\nreturning 'data' dict containing all data.",
                "- initialize_memory: Pre-allocates NumPy arrays for each signal type that will be filled\nduring this read, and initializes unique indices for data access to each\nsignal type.",
                "- advance_indices: Advances indices used for data access by suitable values per data block.\n    ",
                "- check_end_of_file: Checks that the end of the file was reached at the expected position.\nIf not, raise FileSizeError.",
                "- parse_data: Parses raw data into user readable and interactable forms (for example,\nextracting raw digital data to separate channels and scaling data to units\nlike microVolts, degrees Celsius, or seconds.)",
                "- scale_timestamps: Verifies no timestamps are missing, and scales timestamps to seconds.\n    ",
                "- scale_analog_data: Scales all analog data signal types (amplifier data, aux input data,\nsupply voltage data, board ADC data, and temp sensor data) to suitable\nunits (microVolts, Volts, deg C).",
                "- extract_digital_data: Extracts digital data from raw (a single 16-bit vector where each bit\nrepresents a separate digital input channel) to a more user-friendly 16-row\nlist where each row represents a separate digital input channel. Applies to\ndigital input and digital output data.",
                "- apply_notch_filter: Checks header to determine if notch filter should be applied, and if so,\napply notch filter to all signals in data['amplifier_data'].",
                "- notch_filter: Implements a notch filter (e.g., for 50 or 60 Hz) on vector 'signal_in'.\n\nf_sample = sample rate of data (input Hz or Samples/sec)\nf_notch = filter notch frequency (input Hz)\nbandwidth = notch 3-dB bandwidth (input Hz).  A bandwidth of 10 Hz is\nrecommended for 50 or 60 Hz notch filters; narrower bandwidths lead to\npoor time-domain properties with an extended ringing response to\ntransient disturbances.\n\nExample:  If neural data was sampled at 30 kSamples/sec\nand you wish to implement a 60 Hz notch filter:\n\nout = notch_filter(signal_in, 30000, 60, 10);",
                "- calculate_iir_parameters: Calculates parameters d, b, a0, a1, a2, a, b0, b1, and b2 used for\nIIR filter and return them in a dict.",
                "- calculate_iir: Calculates a single sample of IIR filter passing signal_in through\niir_parameters, resulting in signal_out.",
                "- print_progress: Prints progress of an arbitrary process based on position i / target,\nprinting a line showing completion percentage for each print_step / 100.",
                "- UnrecognizedFileError: Exception returned when reading a file as an RHD header yields an\ninvalid magic number (indicating this is not an RHD header file).",
                "- UnknownChannelTypeError: Exception returned when a channel field in RHD header does not have\na recognized signal_type value. Accepted values are:\n0: amplifier channel\n1: aux input channel\n2: supply voltage channel\n3: board adc channel\n4: dig in channel\n5: dig out channel",
                "- FileSizeError: Exception returned when file reading fails due to the file size\nbeing invalid or the calculated file size differing from the actual\nfile size.",
                "- QStringError: Exception returned when reading a QString fails because it is too long.\n    ",
                "- ChannelNotFoundError: Exception returned when plotting fails due to the specified channel\nnot being found."
            ]
        }
    },
    {
        "infer_rnn_rates.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/utils/infer_rnn_rates.py",
            "imports": [
                "argparse",
                "json",
                "matplotlib.pyplot",
                "numpy",
                "os",
                "pprint.pprint",
                "scipy.stats.zscore",
                "sklearn.decomposition.PCA",
                "sklearn.preprocessing.StandardScaler",
                "src.model.autoencoderRNN",
                "src.train.train_model",
                "sys",
                "tables",
                "torch",
                "utils.ephys_data.ephys_data",
                "utils.ephys_data.visualize"
            ],
            "definitions": {
                "functions": [],
                "classes": [],
                "methods": []
            },
            "functionality": [
                "This module uses an Auto-regressive Recurrent Neural Network (RNN) to infer firing rates from electrophysiological data. It processes data for each taste separately, trains an RNN model, and saves the predicted firing rates and latent factors."
            ]
        }
    },
    {
        "channel_corr.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/utils/qa_utils/channel_corr.py",
            "imports": [
                "itertools.combinations",
                "matplotlib.pyplot",
                "numpy",
                "os",
                "pandas",
                "scipy.stats.pearsonr",
                "tables",
                "tqdm.tqdm"
            ],
            "definitions": {
                "functions": [
                    "gen_corr_output",
                    "get_all_channels",
                    "intra_corr"
                ],
                "classes": [],
                "methods": []
            },
            "functionality": [
                "This module provides utilities for quality assurance of channel data, focusing on correlation analysis between channels.",
                "- get_all_channels: Get all channels in a file from nodes ['raw','raw_emg']\n\nInput:\n        hf5_path: str, path to hdf5 file\n        n_corr_samples: int, number of samples to use for correlation calculation\n\nOutput:\n        all_chans: np.array (n_chans, n_samples)\n        chan_names: np.array (n_chans,)",
                "- intra_corr: Correlations between all channels in X\n\nInput:\n        X: np.array (n_chans, n_samples)\n\nOutput:\n        corr_mat: np.array (n_chans, n_chans)",
                "- gen_corr_output: Generate a plot of the raw, and thresholded correlation matrices\n\nInput:\n        corr_mat: np.array (n_chans, n_chans)\n\nOutput:\n        fig: matplotlib figure"
            ]
        }
    },
    {
        "drift_check.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/utils/qa_utils/drift_check.py",
            "imports": [
                "glob",
                "matplotlib",
                "numpy",
                "os",
                "pandas",
                "pingouin",
                "pylab",
                "scipy.stats.zscore",
                "seaborn",
                "sklearn.decomposition.PCA",
                "sys",
                "tables",
                "umap.UMAP",
                "utils.blech_utils.imp_metadata",
                "utils.blech_utils.pipeline_graph_check",
                "utils.ephys_data.ephys_data"
            ],
            "definitions": {
                "functions": [
                    "array_to_df",
                    "get_spike_trains"
                ],
                "classes": [],
                "methods": []
            },
            "functionality": [
                "This module analyzes drift in firing rates across a session using statistical methods and visualizations. It performs ANOVA tests on baseline and post-stimulus firing rates, generates plots, and applies PCA and UMAP for dimensionality reduction.",
                "- get_spike_trains: Get spike trains from hdf5 file\n\nInputs:\n    hf5_path: path to hdf5 file\n\nOutputs:\n    spike_trains: list of spike trains (trials, units, time)",
                "- array_to_df: Convert array to dataframe with dimensions as columns\n\nInputs:\n    array: array to convert\n    dim_names: list of names for each dimension\n\nOutputs:\n    df: dataframe with dimensions as columns"
            ]
        }
    },
    {
        "elbo_drift.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/utils/qa_utils/elbo_drift.py",
            "imports": [
                "matplotlib.pyplot",
                "numpy",
                "os",
                "pandas",
                "pymc",
                "pymc.variational.callbacks.CheckParametersConvergence",
                "pytensor.tensor",
                "seaborn",
                "sklearn.decomposition.PCA",
                "sys",
                "time",
                "tqdm.tqdm",
                "tqdm.trange",
                "utils.blech_utils.imp_metadata",
                "utils.blech_utils.pipeline_graph_check",
                "utils.ephys_data.ephys_data"
            ],
            "definitions": {
                "functions": [
                    "gaussian_changepoint_mean_var_2d"
                ],
                "classes": [],
                "methods": []
            },
            "functionality": [
                "This module uses a PyMC change point model to detect drift in neural data, performing model selection using the Evidence Lower Bound (ELBO). It processes electrophysiological data to identify changes in mean and variance over time.",
                "- gaussian_changepoint_mean_var_2d: Model for gaussian data on 2D array detecting changes in both\nmean and variance.\n\nArgs:\n    data_array (2D Numpy array): <dimension> x time\n    n_states (int): Number of states to model\n\nReturns:\n    pymc3 model: Model class containing graph to run inference on"
            ]
        }
    },
    {
        "unit_similarity.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/utils/qa_utils/unit_similarity.py",
            "imports": [
                "collections.Counter",
                "matplotlib.pyplot",
                "numba.jit",
                "numpy",
                "os",
                "pandas",
                "pylab",
                "scipy.stats.pearsonr",
                "sys",
                "tables",
                "tqdm.tqdm",
                "utils.blech_utils.imp_metadata",
                "utils.blech_utils.pipeline_graph_check"
            ],
            "definitions": {
                "functions": [
                    "parse_collision_mat",
                    "plot_similarity_matrix",
                    "unit_similarity",
                    "unit_similarity_NM",
                    "unit_similarity_abu",
                    "write_out_similarties"
                ],
                "classes": [],
                "methods": []
            },
            "functionality": [
                "This module analyzes unit similarity in neural spike data, identifying and reporting units with high similarity. It processes data from HDF5 files, calculates similarity matrices, and generates visualizations and reports.",
                "- parse_collision_mat: Parses the unit similarity matrix to find units that are too similar\n\nInputs:\nunit_distances: matrix of unit similarity values\nsimilarity_cutoff: similarity value above which units are considered\n\nOutputs:\nunique_pairs: list of tuples of unit numbers\nunique_pairs_collisions: list of similarity values",
                "- plot_similarity_matrix: Creates a figure showing raw similarity matrix and thresholded values\n\nInputs:\nunit_distances: matrix of unit similarity values\nsimilarity_cutoff: threshold for considering units similar\noutput_dir: directory to save plot",
                "- write_out_similarties: Writes out the unit similarity violations to a file\n\nInputs:\nunique_pairs: list of tuples of unit numbers\nunique_pairs_collisions: list of similarity values\nwaveform_counts: list of waveform counts for each unit\nout_path: path to write file to\nmode: write mode (default is 'w')\nwaveform_count_cutoff: percentage threshold for waveform count comparison"
            ]
        }
    },
    {
        "ram_monitor.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/utils/ram_monitor.py",
            "imports": [
                "datetime.datetime",
                "os",
                "psutil",
                "sys",
                "time"
            ],
            "definitions": {
                "functions": [
                    "monitor_ram"
                ],
                "classes": [],
                "methods": []
            },
            "functionality": [
                "This module monitors RAM usage and logs it to a specified directory. It continuously records the used, total, and percentage of RAM usage every second until interrupted.",
                "- monitor_ram: Monitor RAM usage and write to a log file"
            ]
        }
    },
    {
        "read_file.py": {
            "path": "/home/abuzarmahmood/projects/blech_clust/utils/read_file.py",
            "imports": [
                "numpy",
                "os",
                "pandas",
                "tables",
                "tqdm.tqdm",
                "utils.importrhdutilities.load_file",
                "utils.importrhdutilities.read_header"
            ],
            "definitions": {
                "functions": [
                    "read_electrode_channels",
                    "read_electrode_emg_channels_single_file",
                    "read_emg_channels",
                    "read_traditional_intan"
                ],
                "classes": [
                    "DigInHandler"
                ],
                "methods": [
                    "DigInHandler.get_dig_in_files",
                    "DigInHandler.get_trial_data",
                    "DigInHandler.load_dig_in_frame",
                    "DigInHandler.write_out_frame"
                ]
            },
            "functionality": [
                "This module provides functionality for handling digital inputs and reading data from Intan format files, saving the data into HDF5 format. It includes a class for managing digital inputs and several functions for reading and processing data.",
                "- DigInHandler: Class to unify handling of digital inputs across different file formats\n\nMethods:\n        get_dig_in_files: Get digital input\n        get_trial_data: Get trial data (start or end times)",
                "- read_traditional_intan: Reads traditional intan format data and saves to hdf5\n\nInput:\n        hdf5_name: str\n                Name of hdf5 file to save data to\n        file_list: list\n                List of file names to read\n        electrode_layout_frame: pandas.DataFrame\n                Dataframe containing details of electrode layout\n\nWrites:\n        hdf5 file with raw and raw_emg data\n        - raw: amplifier data\n        - raw_emg: EMG data",
                "- read_electrode_channels: # Loading should use file name \n# but writing should use channel ind so that channels from \n# multiple boards are written into a monotonic sequence\n# Note: That channels inds may not be contiguous if there are\n# EMG channels in the middle",
                "- __init__: Initializes DigInHandler object\n\nInput:\n        data_dir: str\n                Directory containing digital input files\n        file_format: str\n                Format of digital input files",
                "- get_dig_in_files: Get digital input files\nKeep track of:\n        1- Actual filename, this would be :\n                - .dat for one-file-per-channel\n                - digitalin.dat for one-file-per-signal-type\n                - all .rhd files for traditional\n        2- dig-in name\n                - str before .dat for one-file-per-channel\n                - ??? for one-file-per-signal-type\n                - 'board_dig_in_channels' for traditional\n        3- The dig-in number (NOT the index)\n\nOutput:\n        dig_in_files: list\n                List of digital input files",
                "- get_trial_data: Get trial data (start or end times)\nNOTE: Keep track of dig-ins using their indices, but also save their filenames\n\nInput:\n        trial_type: str\n                Type of trial data to get (start or end)\n\nOutput:\n        trial_data: list\n                List of trial data"
            ]
        }
    }
]