{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"blech_clust","text":"<p>Python and R based code for clustering and sorting electrophysiology data</p> <p> </p>"},{"location":"#overview","title":"Overview","text":"<p>blech_clust is a comprehensive Python and R based toolkit for clustering and sorting electrophysiology data recorded using the Intan RHD2132 chips. Originally written for cortical multi-electrode recordings in Don Katz's lab at Brandeis University, it's optimized for high-performance computing clusters but can be easily modified to work in any parallel environment.</p> <p>Visit the Katz lab website for more information.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Automated Spike Sorting: Complete pipeline from raw data to sorted units</li> <li>EMG Analysis: Multiple approaches including BSA/STFT and QDA-based gape detection</li> <li>Quality Assessment: Built-in tools for dataset quality grading and validation</li> <li>Parallel Processing: Optimized for HPC environments</li> <li>Comprehensive Documentation: Detailed API reference and tutorials</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Getting Started - Installation and setup instructions</li> <li>API Reference - Complete API documentation</li> <li>Tutorials - Step-by-step guides</li> <li>GitHub Repository</li> </ul>"},{"location":"#pipeline-overview","title":"Pipeline Overview","text":""},{"location":"#spike-sorting-pipeline","title":"Spike Sorting Pipeline","text":"<p>For the complete main spike-sorting pipeline (including the operations workflow diagram, detailed steps, and nomnoml schema), please refer to the README.</p>"},{"location":"#emg-analysis-pipelines","title":"EMG Analysis Pipelines","text":"<p>For details on EMG analysis workflows, see the README and Workflow Documentation.</p>"},{"location":"#installation","title":"Installation","text":"<p>The installation process is managed through a Makefile that handles all dependencies:</p> <pre><code># Clone the repository\ngit clone https://github.com/katzlabbrandeis/blech_clust.git\ncd blech_clust\n\n# Install everything\nmake all\n\n# Activate the environment\nconda activate blech_clust\n</code></pre> <p>For more detailed installation instructions, see the Getting Started guide.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions! Please read our CONTRIBUTING.md file for guidelines.</p>"},{"location":"#contributing-to-documentation","title":"Contributing to Documentation","text":"<p>Help us improve the documentation:</p> <ul> <li>Report issues: Found an error or unclear explanation? Open an issue</li> <li>Suggest improvements: Have ideas for better organization or content? We'd love to hear them</li> <li>Submit changes: See docs/README.md for instructions on building and updating documentation</li> </ul> <p>The documentation is built with MkDocs and automatically deployed via GitHub Actions.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use this code in your research, please cite:</p> <pre><code>@software{blech_clust_katz,\n  author       = {Mahmood, Abuzar and\n                  Mukherjee, Narendra and\n                  Stone, Bradly and\n                  Raymond, Martin and\n                  Germaine, Hannah and\n                  Lin, Jian-You and\n                  Mazzio, Christina and\n                  Katz, Donald},\n  title        = {katzlabbrandeis/blech\\_clust: v1.1.0},\n  month        = apr,\n  year         = 2025,\n  publisher    = {Zenodo},\n  version      = {1.1.0},\n  doi          = {10.5281/zenodo.15175273},\n  url          = {https://doi.org/10.5281/zenodo.15175273}\n}\n</code></pre>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<p>This work used ACCESS-allocated resources at Brandeis University through allocation BIO230103 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services &amp; Support (ACCESS) program, which is supported by U.S. National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296.</p> <p>The project titled \"Computational Processing and Modeling of Neural Ensembles in Identifying the Nonlinear Dynamics of Taste Perception\" was led by PI Abuzar Mahmood. The computational allocation was active from 2023-06-26 to 2024-06-25.</p>"},{"location":"tutorials/","title":"Tutorials","text":""},{"location":"tutorials/#workflow-walkthrough","title":"Workflow Walkthrough","text":"<p>This tutorial walks you through the complete spike sorting pipeline from raw data to analyzed units.</p>"},{"location":"tutorials/#step-1-experiment-information-setup","title":"Step 1: Experiment Information Setup","text":"<p>Open a terminal and run:</p> <pre><code>cd /path/to/blech_clust  # Make the blech_clust repository your working directory\nconda activate blech_clust  # Activate blech_clust environment\nDIR=/path/to/raw/data/files  # Save the path of the target Intan data to be sorted\npython blech_exp_info.py $DIR  # Generate metadata and electrode layout\n</code></pre>"},{"location":"tutorials/#configuring-car-groups","title":"Configuring CAR Groups","text":"<p>Once you've started running the script, it will ask you to \"fill in car groups\". Go to the Intan data folder, where you'll find a file named <code>[...]_electrode_layout.csv</code>.</p> <ol> <li>Open this file in a spreadsheet editor</li> <li>Fill in the <code>CAR_group</code> column</li> <li>Give all electrodes implanted in the same bundle the same identifier</li> <li>Use different identifiers for different bundles</li> <li>Example: All electrodes from a bundle in right GC are called <code>GC1</code>, and all electrodes from a bundle in left GC are called <code>GC2</code></li> <li>Return to the terminal and type <code>y</code> then press <code>enter</code></li> </ol>"},{"location":"tutorials/#selecting-digital-inputs","title":"Selecting Digital Inputs","text":"<p>The script will search your data folder for DIN files and print something like:</p> <pre><code>(0, 'board-DIN-09.dat'),\n(1, 'board-DIN-11.dat'),\n(2, 'board-DIN-12.dat'),\n(3, 'board-DIN-13.dat')\n</code></pre> <p>These are the files for the Intan digital inputs that correspond to stimulus presentations and/or laser activations.</p> <p>Prompt: <code>Taste dig_ins used (IN ORDER, anything separated) :: \"x\" to exit ::</code></p> <ul> <li>Select the DINs to include in later analysis steps</li> <li>Example: To include DINs 11 and 13 but not 09 or 12, type <code>1,3</code> and press <code>enter</code></li> <li>Note: If you have a DIN for laser activations, do not include it here; it will be requested later</li> </ul>"},{"location":"tutorials/#naming-tastes","title":"Naming Tastes","text":"<p>Prompt: <code>Tastes names used (IN ORDER, anything separated) :: \"x\" to exit ::</code></p> <ul> <li>Provide taste names for each selected DIN</li> <li>Example: If DIN-11 was DI H2O and DIN-13 was 300mM sucrose, enter <code>Water,Sucrose</code></li> <li>Leave off the molarity (provided in the next step)</li> </ul>"},{"location":"tutorials/#specifying-concentrations","title":"Specifying Concentrations","text":"<p>Prompt: <code>Corresponding concs used (in M, IN ORDER, COMMA separated) :: \"x\" to exit ::</code></p> <ul> <li>Provide numeric inputs for concentrations in Molarity</li> <li>Example: For DI H2O and 300mM sucrose, enter <code>0,0.3</code></li> </ul>"},{"location":"tutorials/#palatability-rankings","title":"Palatability Rankings","text":"<p>Prompt: <code>Enter palatability rankings used (anything separated), higher number = more palatable :: \"x\" to exit ::</code></p> <ul> <li>Provide numeric rankings (&gt; 0 and &lt;= number of stimuli)</li> <li>Can be non-integer and accept duplicates</li> <li>Valid examples: <code>4,3,2,1</code> or <code>0.4,0.3,0.2,0.1</code> or <code>3,2,2,1</code></li> <li>Invalid examples: <code>2,1,1,0</code> (contains 0) or <code>5,4,3,2</code> (exceeds number of stimuli)</li> <li>Example for water/sucrose: <code>1,2</code></li> </ul>"},{"location":"tutorials/#laser-configuration","title":"Laser Configuration","text":"<p>Prompt: <code>Laser dig_in index, &lt;BLANK&gt; for none::: \"x\" to exit ::</code></p> <ul> <li>If you have a laser DIN, enter its index (e.g., <code>0</code> for DIN-09)</li> <li>If no laser, just press <code>enter</code></li> </ul>"},{"location":"tutorials/#experiment-notes","title":"Experiment Notes","text":"<p>Prompt: <code>Please enter any notes about the experiment.</code></p> <ul> <li>Enter any pertinent comments or press <code>enter</code> to finish</li> </ul>"},{"location":"tutorials/#step-2-parameter-configuration","title":"Step 2: Parameter Configuration","text":"<p>Before running the clustering pipeline, set up parameter files:</p> <ol> <li>Copy <code>blech_clust/params/_templates/sorting_params_template.json</code> to <code>blech_clust/params/sorting_params_template.json</code></li> <li>Update the parameters as needed for your experiment</li> <li>Also copy and adapt:</li> <li><code>waveform_classifier_params.json</code></li> <li><code>emg_params.json</code></li> </ol>"},{"location":"tutorials/#step-3-run-the-pipeline","title":"Step 3: Run the Pipeline","text":""},{"location":"tutorials/#using-convenience-scripts","title":"Using Convenience Scripts","text":"<pre><code>bash blech_clust_pre.sh $DIR   # Perform steps up to spike extraction and UMAP\npython blech_post_process.py   # Add sorted units to HDF5 (CLI or .CSV as input)\nbash blech_clust_post.sh       # Perform steps up to PSTH generation\n</code></pre>"},{"location":"tutorials/#or-use-the-automated-script","title":"Or Use the Automated Script","text":"<pre><code>bash blech_autosort.sh &lt;data_directory&gt; [--force]\n</code></pre> <ul> <li><code>&lt;data_directory&gt;</code>: Path to the directory containing the raw data files</li> <li><code>--force</code>: Optional flag to force re-processing even if previous results exist</li> </ul> <p>The <code>blech_autosort.sh</code> script:</p> <ul> <li>Checks for required parameter files</li> <li>Verifies that specific settings are enabled</li> <li>Executes the pre-processing, clustering, and post-processing steps in sequence</li> </ul>"},{"location":"tutorials/#step-4-quality-assessment","title":"Step 4: Quality Assessment","text":"<p>After processing, assess the quality of your dataset:</p> <pre><code>python blech_units_characteristics.py  # Analyze unit characteristics\npython utils/blech_data_summary.py    # Generate comprehensive dataset summary\npython utils/grade_dataset.py         # Grade dataset quality based on metrics\n</code></pre>"},{"location":"tutorials/#emg-analysis-tutorial","title":"EMG Analysis Tutorial","text":""},{"location":"tutorials/#shared-setup","title":"Shared Setup","text":"<ol> <li>Complete spike sorting through <code>blech_make_arrays.py</code></li> <li>Filter EMG signals:    <pre><code>python emg_filter.py\n</code></pre></li> </ol>"},{"location":"tutorials/#bsastft-branch","title":"BSA/STFT Branch","text":"<p>For Bayesian Spectrum Analysis and Short-Time Fourier Transform:</p> <pre><code>python emg_freq_setup.py              # Configure parameters and generate parallel processing scripts\nbash blech_emg_jetstream_parallel.sh  # Run the generated parallel processing script\npython emg_freq_post_process.py       # Aggregate and process results\npython emg_freq_plot.py               # Generate visualizations\n</code></pre> <p>Note: The <code>emg_freq_setup.py</code> script generates the <code>blech_emg_jetstream_parallel.sh</code> script, which uses GNU parallel to process EMG signals in parallel.</p>"},{"location":"tutorials/#qda-branch","title":"QDA Branch","text":"<p>For Quadratic Discriminant Analysis (gape detection):</p> <pre><code>python emg_freq_setup.py                      # Setup parameters for gape detection\npython emg/gape_QDA_classifier/get_gapes_Li.py  # Detect gapes using QDA classifier\n</code></pre>"},{"location":"tutorials/#testing-your-installation","title":"Testing Your Installation","text":""},{"location":"tutorials/#local-testing-with-prefect","title":"Local Testing with Prefect","text":"<ol> <li> <p>Start the Prefect server in a separate terminal:    <pre><code>prefect server start\n</code></pre></p> </li> <li> <p>In another terminal, run the tests:    <pre><code>cd &lt;path_to_blech_clust&gt;\nmake prefect  # Install/update Prefect\n</code></pre></p> </li> <li> <p>Run specific test suites:    <pre><code># Run all tests\npython pipeline_testing/prefect_pipeline.py --all\n\n# Run only spike sorting tests\npython pipeline_testing/prefect_pipeline.py -s\n\n# Run only EMG analysis tests\npython pipeline_testing/prefect_pipeline.py -e\n\n# Run spike sorting followed by EMG analysis\npython pipeline_testing/prefect_pipeline.py --spike-emg\n</code></pre></p> </li> </ol> <p>Monitor test progress at http://localhost:4200</p>"},{"location":"tutorials/#advanced-topics","title":"Advanced Topics","text":""},{"location":"tutorials/#rnn-based-firing-rate-inference","title":"RNN-based Firing Rate Inference","text":"<p>Use the <code>infer_rnn_rates.py</code> utility to infer firing rates from spike trains:</p> <pre><code>python utils/infer_rnn_rates.py &lt;data_dir&gt; [options]\n</code></pre> <p>Options:</p> <ul> <li><code>--override_config</code>: Override config file and use provided arguments</li> <li><code>--train_steps TRAIN_STEPS</code>: Number of training steps (default: 15000)</li> <li><code>--hidden_size HIDDEN_SIZE</code>: Hidden size of RNN (default: 8)</li> <li><code>--bin_size BIN_SIZE</code>: Bin size for binning spikes (default: 25)</li> <li><code>--train_test_split TRAIN_TEST_SPLIT</code>: Fraction of data for training (default: 0.75)</li> <li><code>--no_pca</code>: Do not use PCA for preprocessing</li> <li><code>--retrain</code>: Force retraining of model</li> <li><code>--time_lims TIME_LIMS TIME_LIMS</code>: Time limits for inferred firing rates (default: [1500, 4500])</li> </ul>"},{"location":"tutorials/#additional-resources","title":"Additional Resources","text":"<ul> <li>Module Documentation: Detailed documentation for the ephys_data module</li> </ul>"},{"location":"workflow/","title":"Workflow Diagrams","text":"<p>This page provides additional visual representations and workflow details for the blech_clust pipeline.</p>"},{"location":"workflow/#main-spike-sorting-pipeline","title":"Main Spike-Sorting Pipeline","text":"<p>For the complete spike-sorting pipeline workflow (including the operations workflow diagram, detailed steps, and nomnoml schema), please refer to the README.</p>"},{"location":"workflow/#quality-assessment-workflow","title":"Quality Assessment Workflow","text":"<pre><code>blech_units_characteristics.py \u2192 blech_data_summary.py \u2192 grade_dataset.py\n</code></pre>"},{"location":"workflow/#additional-workflow-details","title":"Additional Workflow Details","text":""},{"location":"workflow/#spike-sorting-text-flow","title":"Spike Sorting Text Flow","text":"<pre><code>[blech_exp_info] -&gt; [blech_init]\n[blech_init] -&gt; [blech_common_average_reference]\n[blech_common_average_reference] -&gt; [bash blech_run_process.sh]\n[bash blech_run_process.sh] -&gt; [blech_post_process]\n[blech_post_process] -&gt; [blech_units_plot]\n[blech_units_plot] -&gt; [blech_make_arrays]\n[blech_make_arrays] -&gt; [bash blech_run_QA.sh]\n[bash blech_run_QA.sh] -&gt; [blech_units_characteristics]\n[blech_units_characteristics] -&gt; [blech_data_summary]\n[blech_data_summary] -&gt; [grade_dataset]\n</code></pre>"},{"location":"workflow/#emg-analysis-workflows","title":"EMG Analysis Workflows","text":"<p>Shared Steps:</p> <pre><code>[blech_init] -&gt; [blech_make_arrays]\n[blech_make_arrays] -&gt; [emg_filter]\n</code></pre> <p>BSA/STFT Branch:</p> <pre><code>[emg_filter] -&gt; [emg_freq_setup]\n[emg_freq_setup] -&gt; [bash blech_emg_jetstream_parallel.sh (generated)]\n[bash blech_emg_jetstream_parallel.sh] -&gt; [emg_freq_post_process]\n[emg_freq_post_process] -&gt; [emg_freq_plot]\n</code></pre> <p>Note: <code>emg_freq_setup.py</code> generates the <code>blech_emg_jetstream_parallel.sh</code> script for parallel processing.</p> <p>QDA Branch:</p> <pre><code>[emg_filter] -&gt; [emg_freq_setup]\n[emg_freq_setup] -&gt; [gape_QDA_classifier/get_gapes_Li]\n</code></pre>"},{"location":"workflow/#using-the-diagrams","title":"Using the Diagrams","text":""},{"location":"workflow/#viewing-the-workflow","title":"Viewing the Workflow","text":"<p>The operations workflow visual (available in the README) provides a high-level overview of the entire pipeline, showing how different components interact.</p>"},{"location":"workflow/#generating-custom-diagrams","title":"Generating Custom Diagrams","text":"<ol> <li>Visit nomnoml.com</li> <li>Copy the nomnoml schema code from the README</li> <li>Paste it into the editor</li> <li>Modify as needed for your specific use case</li> <li>Export as PNG or SVG</li> </ol>"},{"location":"workflow/#understanding-the-flow","title":"Understanding the Flow","text":"<ul> <li>Spike Sorting: The main pipeline processes raw Intan data through clustering and quality assessment</li> <li>EMG Analysis: Two parallel branches for different analysis approaches</li> <li>BSA/STFT: Frequency-based analysis using Bayesian methods</li> <li>QDA: Gape detection using quadratic discriminant analysis</li> </ul>"},{"location":"workflow/#see-also","title":"See Also","text":"<ul> <li>Main Spike-Sorting Pipeline (README)</li> <li>Core Pipeline Documentation</li> <li>Tutorials</li> <li>Getting Started</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing blech_clust, ensure you have:</p> <ul> <li>Conda/Miniconda: Required for environment management</li> <li>Git: For cloning repositories</li> <li>System packages: GNU parallel (optional, for parallel processing)</li> </ul>"},{"location":"getting-started/installation/#quick-start-recommended","title":"Quick Start (Recommended)","text":"<ol> <li> <p>Clone the repository: <pre><code>git clone https://github.com/katzlabbrandeis/blech_clust.git\ncd blech_clust\n</code></pre></p> </li> <li> <p>Install everything: <pre><code>make all\n</code></pre>    This installs the base environment, EMG analysis tools, neuRecommend classifier, and all optional dependencies.</p> </li> <li> <p>Activate the environment: <pre><code>conda activate blech_clust\n</code></pre></p> </li> </ol>"},{"location":"getting-started/installation/#custom-installation","title":"Custom Installation","text":"<p>For more control over what gets installed:</p> <pre><code># Core spike sorting functionality only\nmake core\n\n# Or install components individually:\nmake base      # Base environment and core dependencies (required)\nmake emg       # EMG analysis requirements (BSA/STFT, QDA)\nmake neurec    # neuRecommend waveform classifier\nmake prefect   # Prefect workflow management (for testing)\nmake dev       # Development dependencies\nmake optional  # Optional analysis tools\n</code></pre>"},{"location":"getting-started/installation/#parameter-setup","title":"Parameter Setup","text":"<p>After installation, set up parameter templates:</p> <pre><code># Copy parameter templates (if none exist)\nmake params\n\n# Edit the parameter files according to your experimental setup\n</code></pre>"},{"location":"getting-started/installation/#parameter-files","title":"Parameter Files","text":"<p>The following parameter files control pipeline behavior:</p> <ul> <li> <p>sorting_params.json - Spike sorting parameters:</p> <ul> <li><code>bandpass_lower_cutoff</code> / <code>bandpass_upper_cutoff</code>: Filter frequencies (default: 300-3000 Hz)</li> <li><code>waveform_threshold</code>: Spike detection threshold in standard deviations (default: 5)</li> <li><code>spike_snapshot_before</code> / <code>spike_snapshot_after</code>: Waveform extraction window in ms</li> <li><code>clustering_params</code>: GMM clustering settings</li> <li><code>qa_params</code>: Quality assurance thresholds</li> <li><code>psth_params</code>: PSTH calculation parameters</li> </ul> </li> <li> <p>emg_params.json - EMG analysis parameters:</p> <ul> <li><code>stft_params</code>: Short-time Fourier transform settings</li> <li><code>use_BSA</code>: Whether to use Bayesian Spectrum Analysis</li> </ul> </li> <li> <p>waveform_classifier_params.json - neuRecommend classifier settings</p> </li> </ul>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":"<ul> <li> <p>Clean installation: If you encounter issues, remove the environment and start fresh:   <pre><code>make clean\nmake all\n</code></pre></p> </li> <li> <p>Partial installation: If a component fails, you can retry individual components:   <pre><code>make base    # Retry base installation\nmake emg     # Retry EMG components\n</code></pre></p> </li> <li> <p>Environment activation: Always activate the environment before running scripts:   <pre><code>conda activate blech_clust\n</code></pre></p> </li> </ul>"},{"location":"getting-started/installation/#test-dataset","title":"Test Dataset","text":"<p>A test dataset is available to verify your installation:</p> <p>Test Dataset on Google Drive</p>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the API Reference to understand available functions</li> <li>Read the Tutorials for step-by-step guides</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":""},{"location":"getting-started/quickstart/#first-steps","title":"First Steps","text":"<p>Once installed, you can start processing your data:</p> <ol> <li>Prepare your data: Ensure you have Intan RHD2132 recordings</li> <li>Set up experiment info: Run <code>python blech_exp_info.py</code> to annotate channels</li> <li>Configure parameters: Edit the parameter files in the <code>params/</code> directory</li> <li>Run the pipeline: Use the convenience scripts or run individual modules</li> </ol>"},{"location":"getting-started/quickstart/#basic-workflow","title":"Basic Workflow","text":""},{"location":"getting-started/quickstart/#1-experiment-setup","title":"1. Experiment Setup","text":"<pre><code># Activate the environment\nconda activate blech_clust\n\n# Navigate to your data directory\ncd /path/to/your/data\n\n# Run experiment info setup\npython /path/to/blech_clust/blech_exp_info.py\n</code></pre> <p>This will guide you through annotating your channels and setting up experimental parameters.</p>"},{"location":"getting-started/quickstart/#2-data-initialization","title":"2. Data Initialization","text":"<pre><code># Initialize directory structure and prepare data\npython /path/to/blech_clust/blech_init.py\n</code></pre> <p>This creates the necessary directory structure and organizes data files.</p>"},{"location":"getting-started/quickstart/#3-common-average-referencing","title":"3. Common Average Referencing","text":"<pre><code># Perform common average referencing\npython /path/to/blech_clust/blech_common_avg_reference.py\n</code></pre>"},{"location":"getting-started/quickstart/#4-spike-extraction-and-clustering","title":"4. Spike Extraction and Clustering","text":"<pre><code># Run parallel processing\nbash /path/to/blech_clust/blech_run_process.sh\n</code></pre> <p>This runs spike extraction and clustering in parallel across electrodes.</p>"},{"location":"getting-started/quickstart/#5-post-processing","title":"5. Post-Processing","text":"<pre><code># Add selected units to HDF5\npython /path/to/blech_clust/blech_post_process.py\n\n# Plot waveforms\npython /path/to/blech_clust/blech_units_plot.py\n\n# Generate spike arrays\npython /path/to/blech_clust/blech_make_arrays.py\n</code></pre>"},{"location":"getting-started/quickstart/#6-quality-assessment","title":"6. Quality Assessment","text":"<pre><code># Run QA checks\nbash /path/to/blech_clust/blech_run_QA.sh\n\n# Analyze unit characteristics\npython /path/to/blech_clust/blech_units_characteristics.py\n\n# Generate data summary\npython /path/to/blech_clust/blech_data_summary.py\n\n# Grade dataset quality\npython /path/to/blech_clust/grade_dataset.py\n</code></pre>"},{"location":"getting-started/quickstart/#emg-analysis","title":"EMG Analysis","text":"<p>If you have EMG data, you can run the EMG analysis pipeline:</p> <pre><code># Filter EMG signals\npython /path/to/blech_clust/emg/emg_filter.py\n\n# Setup frequency analysis parameters\npython /path/to/blech_clust/emg/emg_freq_setup.py\n\n# Choose your analysis approach:\n\n# Option 1: BSA/STFT frequency analysis\npython /path/to/blech_clust/emg/emg_freq_post_process.py\npython /path/to/blech_clust/emg/emg_freq_plot.py\n\n# Option 2: QDA-based gape detection\npython /path/to/blech_clust/emg/gape_QDA_classifier/get_gapes_Li.py\n</code></pre>"},{"location":"getting-started/quickstart/#parameter-configuration","title":"Parameter Configuration","text":"<p>Key parameter files to configure:</p> <ul> <li>clustering_params.json: Clustering algorithm parameters</li> <li>spike_detection_params.json: Spike detection thresholds</li> <li>emg_params.json: EMG analysis parameters (if using EMG)</li> </ul>"},{"location":"getting-started/quickstart/#tips","title":"Tips","text":"<ul> <li>Always activate the conda environment before running scripts</li> <li>Check log files in the output directories for debugging</li> <li>Use the test dataset to verify your installation</li> <li>Refer to the API Reference for detailed function documentation</li> </ul>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the Tutorials for detailed walkthroughs</li> <li>Check the API Reference for function details</li> </ul>"},{"location":"getting-started/migration-guide/","title":"Migration Guide from Original blech_clust","text":"<p>This guide documents the changes between the original blech_clust and the current katzlabbrandeis fork.</p>"},{"location":"getting-started/migration-guide/#overview-of-changes","title":"Overview of Changes","text":"<p>The katzlabbrandeis fork represents a significant modernization of the original codebase:</p> Metric Count Files added 131 Files removed 80 Files modified 5 <p>The changes focus on improved reproducibility, testing, documentation, and user experience while maintaining the core spike-sorting functionality.</p>"},{"location":"getting-started/migration-guide/#migration-guide-sections","title":"Migration Guide Sections","text":"<ul> <li>Removed Features - Components removed from the original and their alternatives</li> <li>File Mapping - Where relocated files now live</li> <li>Quality Assurance - New QA tools and drift detection</li> </ul>"},{"location":"getting-started/migration-guide/#quick-reference-new-features","title":"Quick Reference: New Features","text":""},{"location":"getting-started/migration-guide/#installation-and-environment","title":"Installation and Environment","text":"Feature Description Makefile-based installation <code>make all</code> handles conda environment, dependencies, and R packages Pip-installable package Alternative installation via pip Dev container support <code>.devcontainer/</code> for consistent development environments Pre-commit hooks Code quality enforcement Structured requirements Separate files for base, dev, test, docs, and optional dependencies <p>See Installation for setup instructions.</p>"},{"location":"getting-started/migration-guide/#testing-infrastructure","title":"Testing Infrastructure","text":"Feature Description GitHub Actions CI/CD Automated testing on push/PR pytest test suite Unit tests in <code>tests/</code> Pipeline testing End-to-end validation in <code>pipeline_testing/</code>"},{"location":"getting-started/migration-guide/#metadata-and-parameter-recording","title":"Metadata and Parameter Recording","text":"<p>The original blech_clust used text entry boxes with no record of parameters. The current version provides:</p> Feature Description <code>blech_exp_info.py</code> Pre-clustering annotation of channels and experimental parameters Parameter templates JSON templates in <code>params/_templates/</code> Dependency graph <code>params/dependency_graph.json</code> tracks parameter dependencies Example metadata Sample files in <code>example_meta_files/</code>"},{"location":"getting-started/migration-guide/#common-average-reference-improvements","title":"Common Average Reference Improvements","text":"Feature Description Dead channel visualization Plots to identify dead/different channels within CAR groups Channel clustering Automatic clustering of channels within CAR groups"},{"location":"getting-started/migration-guide/#unit-quality-and-classification","title":"Unit Quality and Classification","text":"Feature Description Waveform classifier ML classifier to recommend units Feature visualization Spike features over recording duration <code>blech_units_characteristics.py</code> Unit characteristic analysis <code>utils/cluster_stability.py</code> Cluster stability assessment Hierarchical clustering plots Visual assessment of cluster quality"},{"location":"getting-started/migration-guide/#rnn-based-firing-rate-inference","title":"RNN-Based Firing Rate Inference","text":"<p>The <code>utils/infer_rnn_rates.py</code> module uses recurrent neural networks to infer firing rates from spike trains:</p> <pre><code>python utils/infer_rnn_rates.py /path/to/data --train_steps 15000 --hidden_size 8\n</code></pre> <p>Configuration via <code>params/_templates/blechrnn_params.json</code>.</p>"},{"location":"getting-started/migration-guide/#performance-improvements","title":"Performance Improvements","text":"Improvement Description Collision calculation Optimized from O(n\u00b2) to O(n) Parallel electrode processing Concurrent processing across electrodes Memory-efficient CAR Finite samples instead of downsampled recording"},{"location":"getting-started/migration-guide/#shell-scripts-for-automation","title":"Shell Scripts for Automation","text":"Script Description <code>blech_autosort.sh</code> Main autosorting script <code>blech_autosort_batch.sh</code> Batch processing <code>blech_clust_pre.sh</code> Pre-clustering setup <code>blech_clust_post.sh</code> Post-clustering steps <code>blech_run_process.sh</code> Parallel spike extraction <code>emg/emg_run_pipeline.sh</code> EMG pipeline automation <p>See Quick Start and Tutorials for workflow details.</p>"},{"location":"getting-started/migration-guide/#code-organization","title":"Code Organization","text":"<p>The codebase has been reorganized from a flat structure to modular directories:</p> Directory Contents <code>utils/</code> Utility functions and helper modules <code>emg/</code> EMG analysis pipeline <code>params/</code> Parameter templates and configuration <code>tests/</code> pytest test suite <code>docs/</code> MkDocs documentation <code>pipeline_testing/</code> End-to-end pipeline tests"},{"location":"getting-started/migration-guide/#ephys_data-module","title":"ephys_data Module","text":"<p>The <code>utils/ephys_data/</code> module provides utilities for loading, processing, and visualizing data. See Ephys Data Reference for details.</p>"},{"location":"getting-started/migration-guide/#emg-analysis","title":"EMG Analysis","text":"<p>EMG functionality has been reorganized into the <code>emg/</code> directory. See EMG Analysis Reference for details.</p>"},{"location":"getting-started/migration-guide/#key-improvements-summary","title":"Key Improvements Summary","text":"Area Original Current Installation Manual dependency management <code>make all</code> automated setup Testing None pytest suite + CI/CD Parameters Text entry, no record JSON templates, full recording Documentation Minimal README Full MkDocs site CAR Basic Visualization + channel clustering Unit quality Manual inspection Classifier + feature plots QA None Drift detection, grading metrics File formats One format Multiple format support Code organization Flat structure Modular directories"},{"location":"getting-started/migration-guide/#getting-help","title":"Getting Help","text":"<ul> <li>Documentation: katzlabbrandeis.github.io/blech_clust</li> <li>Issues: GitHub Issues</li> <li>Original repo: narendramukherjee/blech_clust</li> </ul>"},{"location":"getting-started/migration-guide/file-mapping/","title":"File Mapping","text":"<p>This page provides a complete reference for files that were relocated from the original blech_clust to the current fork.</p>"},{"location":"getting-started/migration-guide/file-mapping/#root-level-to-utils","title":"Root Level to utils/","text":"<p>Utility scripts moved from the repository root to <code>utils/</code>:</p> Original Location New Location <code>clustering.py</code> <code>utils/clustering.py</code> <code>read_file.py</code> <code>utils/read_file.py</code> <code>split_h5_files.py</code> <code>utils/blech_split_h5_files.py</code> <code>blech_waveforms_datashader.py</code> <code>utils/blech_waveforms_datashader.py</code> <code>blech_held_units_detect.py</code> <code>utils/blech_held_units_detect.py</code> <code>blech_hdf5_repack.py</code> <code>utils/blech_hdf5_repack.py</code> <code>blech_nex_convert.py</code> <code>utils/blech_nex_convert.py</code> <code>fix_laser_sampling_errors.py</code> <code>utils/fix_laser_sampling_errors.py</code> <code>memory_monitor.py</code> <code>utils/ram_monitor.py</code>"},{"location":"getting-started/migration-guide/file-mapping/#import-changes","title":"Import Changes","text":"<p>If you have scripts that imported from the original locations, update your imports:</p> <pre><code># Original\nfrom clustering import ...\nfrom read_file import ...\n\n# Current\nfrom utils.clustering import ...\nfrom utils.read_file import ...\n</code></pre>"},{"location":"getting-started/migration-guide/file-mapping/#emg-scripts-to-emg","title":"EMG Scripts to emg/","text":"<p>All EMG-related scripts have been consolidated in the <code>emg/</code> directory:</p> Original Location New Location <code>filter_emg.py</code> <code>emg/emg_filter.py</code> <code>emg_local_BSA.py</code> <code>emg/emg_local_BSA_execute.py</code> <code>emg_local_BSA_execute.py</code> <code>emg/emg_local_BSA_execute.py</code> <code>emg_local_BSA_post_process.py</code> <code>emg/emg_freq_post_process.py</code> <code>emg_BSA_segmentation.py</code> <code>emg/emg_freq_setup.py</code> <code>emg_BSA_segmentation_plot.py</code> <code>emg/emg_freq_plot.py</code> <code>get_gapes_Li.py</code> <code>emg/gape_QDA_classifier/get_gapes_Li.py</code> <code>detect_peaks.py</code> <code>emg/gape_QDA_classifier/detect_peaks.py</code> <code>QDA_nostd_no_first.mat</code> <code>emg/gape_QDA_classifier/QDA_nostd_no_first.mat</code>"},{"location":"getting-started/migration-guide/file-mapping/#merged-functionality","title":"Merged Functionality","text":"<p>Some EMG scripts were merged or integrated:</p> Original Current Notes <code>emg_make_arrays.py</code> <code>blech_make_arrays.py</code> EMG array creation integrated into main array generation"},{"location":"getting-started/migration-guide/file-mapping/#renamed-scripts","title":"Renamed Scripts","text":"<p>Some scripts were renamed for clarity or consistency:</p> Original Name New Name Notes <code>blech_unit_visualize.py</code> <code>blech_units_plot.py</code> Enhanced with additional visualizations <code>blech_units_similarity.py</code> <code>utils/qa_utils/unit_similarity.py</code> Moved to QA utilities <code>memory_monitor.py</code> <code>utils/ram_monitor.py</code> Renamed for clarity"},{"location":"getting-started/migration-guide/file-mapping/#new-utils-structure","title":"New utils/ Structure","text":"<p>The <code>utils/</code> directory contains several new subdirectories:</p> <pre><code>utils/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 blech_channel_profile.py\n\u251c\u2500\u2500 blech_data_summary.py\n\u251c\u2500\u2500 blech_hdf5_repack.py\n\u251c\u2500\u2500 blech_held_units_detect.py\n\u251c\u2500\u2500 blech_nex_convert.py\n\u251c\u2500\u2500 blech_post_process_utils.py\n\u251c\u2500\u2500 blech_process_utils.py\n\u251c\u2500\u2500 blech_reload_amp_digs.py\n\u251c\u2500\u2500 blech_spike_features.py\n\u251c\u2500\u2500 blech_split_h5_files.py\n\u251c\u2500\u2500 blech_utils.py\n\u251c\u2500\u2500 blech_waveforms_datashader.py\n\u251c\u2500\u2500 cluster_stability.py\n\u251c\u2500\u2500 clustering.py\n\u251c\u2500\u2500 fix_laser_sampling_errors.py\n\u251c\u2500\u2500 grade_dataset.py\n\u251c\u2500\u2500 grading_metrics.json\n\u251c\u2500\u2500 importrhdutilities.py\n\u251c\u2500\u2500 infer_rnn_rates.py\n\u251c\u2500\u2500 makeRaisedCosBasis.py\n\u251c\u2500\u2500 ram_monitor.py\n\u251c\u2500\u2500 read_file.py\n\u251c\u2500\u2500 ephys_data/\n\u2502   \u251c\u2500\u2500 BAKS.py\n\u2502   \u251c\u2500\u2500 ephys_data.py\n\u2502   \u251c\u2500\u2500 lfp_processing.py\n\u2502   \u251c\u2500\u2500 visualize.py\n\u2502   \u251c\u2500\u2500 convenience_scripts/\n\u2502   \u2514\u2500\u2500 tests/\n\u251c\u2500\u2500 qa_utils/\n\u2502   \u251c\u2500\u2500 channel_corr.py\n\u2502   \u251c\u2500\u2500 drift_check.py\n\u2502   \u251c\u2500\u2500 elbo_drift.py\n\u2502   \u2514\u2500\u2500 unit_similarity.py\n\u2514\u2500\u2500 umap_plotting/\n    \u251c\u2500\u2500 bash_umap_parallel.sh\n    \u2514\u2500\u2500 umap_plots.py\n</code></pre>"},{"location":"getting-started/migration-guide/file-mapping/#new-emg-structure","title":"New emg/ Structure","text":"<p>The <code>emg/</code> directory organization:</p> <pre><code>emg/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 emg_filter.py\n\u251c\u2500\u2500 emg_freq_plot.py\n\u251c\u2500\u2500 emg_freq_post_process.py\n\u251c\u2500\u2500 emg_freq_setup.py\n\u251c\u2500\u2500 emg_local_BSA_execute.py\n\u251c\u2500\u2500 emg_local_STFT_execute.py\n\u251c\u2500\u2500 emg_run_pipeline.sh\n\u251c\u2500\u2500 _archive/\n\u2502   \u2514\u2500\u2500 emg_process_comparison.py\n\u251c\u2500\u2500 gape_QDA_classifier/\n\u2502   \u251c\u2500\u2500 QDA_classifier.py\n\u2502   \u251c\u2500\u2500 QDA_nostd_no_first.mat\n\u2502   \u251c\u2500\u2500 detect_peaks.py\n\u2502   \u251c\u2500\u2500 get_gapes_Li.py\n\u2502   \u2514\u2500\u2500 _experimental/\n\u2514\u2500\u2500 utils/\n    \u2514\u2500\u2500 emg_reload_raw_data.py\n</code></pre>"},{"location":"getting-started/migration-guide/file-mapping/#command-line-usage-changes","title":"Command Line Usage Changes","text":"<p>If you were running scripts from the command line, update your paths:</p> <pre><code># Original\npython filter_emg.py\npython get_gapes_Li.py\npython blech_held_units_detect.py\n\n# Current\npython emg/emg_filter.py\npython emg/gape_QDA_classifier/get_gapes_Li.py\npython utils/blech_held_units_detect.py\n</code></pre> <p>Or use the convenience scripts:</p> <pre><code># EMG pipeline\nbash emg/emg_run_pipeline.sh /path/to/data\n\n# Main pipeline\nbash blech_autosort.sh /path/to/data\n</code></pre>"},{"location":"getting-started/migration-guide/file-mapping/#finding-functionality","title":"Finding Functionality","text":"<p>If you're looking for specific functionality from the original codebase:</p> Looking for... Now in... Spike clustering <code>utils/clustering.py</code> File reading <code>utils/read_file.py</code>, <code>utils/importrhdutilities.py</code> Waveform visualization <code>utils/blech_waveforms_datashader.py</code> Held unit detection <code>utils/blech_held_units_detect.py</code> HDF5 operations <code>utils/blech_hdf5_repack.py</code>, <code>utils/blech_split_h5_files.py</code> NEX file conversion <code>utils/blech_nex_convert.py</code> Memory monitoring <code>utils/ram_monitor.py</code> EMG filtering <code>emg/emg_filter.py</code> Gape detection <code>emg/gape_QDA_classifier/get_gapes_Li.py</code> LFP processing <code>utils/ephys_data/lfp_processing.py</code> Unit similarity <code>utils/qa_utils/unit_similarity.py</code> Drift detection <code>utils/qa_utils/drift_check.py</code> PSTH overlay <code>blech_units_characteristics.py</code>, <code>utils/ephys_data/</code>"},{"location":"getting-started/migration-guide/qa-improvements/","title":"Quality Assurance Improvements","text":"<p>The current fork includes quality assurance tools that were not present in the original blech_clust. These tools help identify issues with recordings and sorted units.</p>"},{"location":"getting-started/migration-guide/qa-improvements/#overview","title":"Overview","text":"Tool Purpose <code>blech_run_QA.sh</code> Run all QA checks <code>utils/qa_utils/drift_check.py</code> Detect drift during recording <code>utils/qa_utils/channel_corr.py</code> Channel correlation analysis <code>utils/qa_utils/unit_similarity.py</code> Unit similarity metrics <code>utils/qa_utils/elbo_drift.py</code> ELBO-based drift detection <code>utils/grade_dataset.py</code> Dataset quality grading <code>utils/grading_metrics.json</code> Configurable grading criteria <code>utils/blech_data_summary.py</code> At-a-glance dataset insights"},{"location":"getting-started/migration-guide/qa-improvements/#running-qa-checks","title":"Running QA Checks","text":"<p>After completing the spike sorting pipeline, run QA checks:</p> <pre><code>bash blech_run_QA.sh /path/to/data\n</code></pre> <p>This script runs the QA utilities and generates reports.</p>"},{"location":"getting-started/migration-guide/qa-improvements/#drift-detection","title":"Drift Detection","text":""},{"location":"getting-started/migration-guide/qa-improvements/#population-level-drift","title":"Population-Level Drift","text":"<p>The <code>drift_check.py</code> module detects drift at the population level by analyzing firing rate stability across the recording:</p> <pre><code>from utils.qa_utils import drift_check\n\n# Run drift analysis\ndrift_results = drift_check.analyze_drift(data_dir='/path/to/data')\n</code></pre>"},{"location":"getting-started/migration-guide/qa-improvements/#single-unit-drift","title":"Single-Unit Drift","text":"<p>Drift can also be assessed at the single-unit level to identify units that may have drifted during the recording:</p> <ul> <li>Firing rate changes over time</li> <li>Waveform amplitude changes</li> <li>ISI distribution changes</li> </ul>"},{"location":"getting-started/migration-guide/qa-improvements/#elbo-based-drift","title":"ELBO-Based Drift","text":"<p>The <code>elbo_drift.py</code> module uses Evidence Lower Bound (ELBO) metrics from variational inference to detect population drift. This approach provides a probabilistic framework for identifying when neural activity patterns change significantly.</p> <pre><code>from utils.qa_utils import elbo_drift\n\n# Analyze ELBO drift\nelbo_results = elbo_drift.analyze(data_dir='/path/to/data')\n</code></pre> <p>Key features: - Uses <code>pymc</code> for Bayesian statistical modeling - Performs drift detection on spike-time histograms (more accurate than PCA of firing rates) - Supports flexible changepoints - Exports results to CSV for further analysis</p>"},{"location":"getting-started/migration-guide/qa-improvements/#channel-correlation","title":"Channel Correlation","text":"<p>The <code>channel_corr.py</code> module analyzes correlations between channels to identify:</p> <ul> <li>Dead channels</li> <li>Channels with excessive noise</li> <li>Cross-talk between channels</li> <li>Channels that should be excluded from common average reference</li> </ul> <pre><code>from utils.qa_utils import channel_corr\n\n# Analyze channel correlations\ncorr_results = channel_corr.analyze(data_dir='/path/to/data')\n</code></pre> <p>This enhancement bridges the gap between raw channel data and meaningful quality metrics.</p>"},{"location":"getting-started/migration-guide/qa-improvements/#unit-similarity","title":"Unit Similarity","text":"<p>The <code>unit_similarity.py</code> module (relocated from <code>blech_units_similarity.py</code>) computes similarity metrics between sorted units:</p> <ul> <li>Waveform similarity</li> <li>Firing pattern correlation</li> <li>Potential duplicate detection</li> </ul> <pre><code>from utils.qa_utils import unit_similarity\n\n# Analyze unit similarity\nsimilarity_results = unit_similarity.analyze(data_dir='/path/to/data')\n</code></pre>"},{"location":"getting-started/migration-guide/qa-improvements/#data-summary","title":"Data Summary","text":"<p>The <code>blech_data_summary.py</code> script provides at-a-glance dataset insights:</p> <pre><code>python utils/blech_data_summary.py /path/to/data\n</code></pre> <p>This generates a summary including: - Number of units and their quality - Recording duration and stability - Trial counts per condition - Basic firing rate statistics</p>"},{"location":"getting-started/migration-guide/qa-improvements/#dataset-grading","title":"Dataset Grading","text":""},{"location":"getting-started/migration-guide/qa-improvements/#grading-script","title":"Grading Script","text":"<p>The <code>grade_dataset.py</code> script assigns quality grades based on configurable metrics, automating the assessment process:</p> <pre><code>python utils/grade_dataset.py /path/to/data\n</code></pre>"},{"location":"getting-started/migration-guide/qa-improvements/#grading-metrics","title":"Grading Metrics","text":"<p>The <code>grading_metrics.json</code> file defines the criteria for grading:</p> <pre><code>{\n    \"unit_count\": {\n        \"weight\": 0.3,\n        \"thresholds\": {\n            \"A\": 20,\n            \"B\": 10,\n            \"C\": 5,\n            \"D\": 1\n        }\n    },\n    \"firing_rate_stability\": {\n        \"weight\": 0.2,\n        \"thresholds\": {\n            \"A\": 0.9,\n            \"B\": 0.7,\n            \"C\": 0.5,\n            \"D\": 0.3\n        }\n    }\n    // ... additional metrics\n}\n</code></pre> <p>You can customize these thresholds for your experimental requirements.</p>"},{"location":"getting-started/migration-guide/qa-improvements/#cluster-stability","title":"Cluster Stability","text":"<p>The <code>utils/cluster_stability.py</code> module assesses the stability of clusters over time:</p> <pre><code>from utils import cluster_stability\n\n# Analyze cluster stability\nstability = cluster_stability.analyze(data_dir='/path/to/data')\n</code></pre> <p>This helps identify:</p> <ul> <li>Clusters that split or merge during recording</li> <li>Units with unstable waveforms</li> <li>Potential sorting errors</li> </ul>"},{"location":"getting-started/migration-guide/qa-improvements/#hierarchical-clustering-visualization","title":"Hierarchical Clustering Visualization","text":"<p>The pipeline includes hierarchical clustering plots for visual assessment of cluster quality. These plots help identify:</p> <ul> <li>Well-separated clusters</li> <li>Clusters that may need to be merged</li> <li>Outlier waveforms</li> </ul>"},{"location":"getting-started/migration-guide/qa-improvements/#common-average-reference-qa","title":"Common Average Reference QA","text":"<p>The enhanced <code>blech_common_avg_reference.py</code> includes QA features:</p>"},{"location":"getting-started/migration-guide/qa-improvements/#dead-channel-detection","title":"Dead Channel Detection","text":"<p>Automatically identifies channels that are:</p> <ul> <li>Flat (no signal)</li> <li>Saturated</li> <li>Significantly different from other channels in the CAR group</li> </ul>"},{"location":"getting-started/migration-guide/qa-improvements/#channel-clustering","title":"Channel Clustering","text":"<p>Clusters channels within CAR groups to identify:</p> <ul> <li>Channels that should be excluded from CAR</li> <li>Potential grouping issues</li> </ul>"},{"location":"getting-started/migration-guide/qa-improvements/#visualization","title":"Visualization","text":"<p>Generates plots showing:</p> <ul> <li>Channel distributions within CAR groups</li> <li>Outlier channels</li> <li>CAR effectiveness</li> </ul>"},{"location":"getting-started/migration-guide/qa-improvements/#unit-quality-visualization","title":"Unit Quality Visualization","text":""},{"location":"getting-started/migration-guide/qa-improvements/#feature-over-time-plots","title":"Feature Over Time Plots","text":"<p><code>blech_units_plot.py</code> generates plots showing spike features over the recording duration:</p> <ul> <li>Amplitude stability</li> <li>Waveform shape consistency</li> <li>ISI distribution changes</li> </ul>"},{"location":"getting-started/migration-guide/qa-improvements/#waveform-classifier","title":"Waveform Classifier","text":"<p>The waveform classifier provides quality recommendations:</p> <pre><code># Classifier recommendations are stored in the HDF5 file\n# Access via:\nimport tables\nwith tables.open_file('data.h5', 'r') as hf5:\n    recommendations = hf5.root.sorted_units.classifier_recommendations[:]\n</code></pre>"},{"location":"getting-started/migration-guide/qa-improvements/#integration-with-pipeline","title":"Integration with Pipeline","text":"<p>QA checks are integrated into the pipeline workflow:</p> <ol> <li>During CAR: Dead channel detection and visualization</li> <li>After sorting: Cluster stability and unit similarity</li> <li>Post-processing: Drift detection and grading</li> <li>Final: Dataset grade assignment</li> </ol>"},{"location":"getting-started/migration-guide/qa-improvements/#automated-qa","title":"Automated QA","text":"<p>Use <code>blech_autosort.sh</code> with the <code>--qa</code> flag to include QA in the automated pipeline:</p> <pre><code>bash blech_autosort.sh /path/to/data --qa\n</code></pre> <p>Or run QA separately:</p> <pre><code>bash blech_run_QA.sh /path/to/data\n</code></pre>"},{"location":"getting-started/migration-guide/qa-improvements/#interpreting-qa-results","title":"Interpreting QA Results","text":""},{"location":"getting-started/migration-guide/qa-improvements/#drift-indicators","title":"Drift Indicators","text":"Indicator Interpretation Low drift score Recording is stable High drift score Significant drift detected Unit-specific drift Individual unit may have moved"},{"location":"getting-started/migration-guide/qa-improvements/#grade-interpretation","title":"Grade Interpretation","text":"Grade Interpretation A High quality, suitable for all analyses B Good quality, suitable for most analyses C Acceptable quality, use with caution D Poor quality, may need re-sorting or exclusion"},{"location":"getting-started/migration-guide/qa-improvements/#recommended-actions","title":"Recommended Actions","text":"Issue Recommended Action High drift Consider splitting recording into segments Low unit count Review sorting parameters Channel issues Exclude problematic channels from CAR Similar units Review for potential duplicates"},{"location":"getting-started/migration-guide/removed-features/","title":"Removed Features","text":"<p>This page documents components removed from the original blech_clust and their alternatives in the current fork.</p>"},{"location":"getting-started/migration-guide/removed-features/#legacy-code","title":"Legacy Code","text":""},{"location":"getting-started/migration-guide/removed-features/#python-2-backups","title":"Python 2 Backups","text":"<p>The <code>python2_legacy_code/</code> directory has been removed. This contained <code>.bak</code> files from the Python 2 to Python 3 migration:</p> <ul> <li><code>blech_clust.py.bak</code></li> <li><code>blech_held_units_detect.py.bak</code></li> <li><code>blech_multinomial_hmm.py.bak</code></li> <li><code>blech_palatability_identity_plot.py.bak</code></li> <li><code>blech_palatability_identity_setup.py.bak</code></li> <li><code>blech_post_process.py.bak</code></li> <li><code>blech_process.py.bak</code></li> <li><code>blech_setup_hmm.py.bak</code></li> <li><code>blech_units_distance.py.bak</code></li> <li><code>detect_peaks.py.bak</code></li> <li><code>emg_BSA_segmentation_plot.py.bak</code></li> <li><code>emg_local_BSA.py.bak</code></li> <li><code>emg_local_BSA_post_process.py.bak</code></li> <li><code>emg_make_arrays.py.bak</code></li> <li><code>memory_monitor.py.bak</code></li> <li><code>units_make_arrays.py.bak</code></li> </ul> <p>Rationale: Python 2 reached end-of-life in 2020. The current codebase is Python 3 only.</p>"},{"location":"getting-started/migration-guide/removed-features/#original-entry-point","title":"Original Entry Point","text":"<p><code>blech_clust.py</code> has been removed and replaced by a modular pipeline:</p> Original Current <code>blech_clust.py</code> <code>blech_exp_info.py</code> \u2192 <code>blech_init.py</code> \u2192 <code>blech_common_avg_reference.py</code> \u2192 <code>blech_run_process.sh</code> \u2192 ... <p>The modular approach allows:</p> <ul> <li>Running individual steps independently</li> <li>Better error recovery (restart from failed step)</li> <li>Clearer separation of concerns</li> </ul>"},{"location":"getting-started/migration-guide/removed-features/#lfp-analysis","title":"LFP Analysis","text":"<p>The entire <code>LFP_analysis/</code> directory has been removed:</p> Removed File Description <code>Compare_frequency_envelopes.py</code> Frequency envelope comparison <code>GFP_frequency_band_extraction.py</code> Global field power extraction <code>LFP_Processing_Final.py</code> Main LFP processing <code>LFP_Spectrogram_stone.py</code> Spectrogram generation <code>LFP_create_m_file.py</code> MATLAB file creation <code>LFP_create_m_file_daniel.py</code> MATLAB file creation (variant) <code>LFP_spike_lock_setup_Final.py</code> Spike-LFP locking setup <code>Laser_LFP_Parse_Final.py</code> Laser trial LFP parsing <code>compare_passive_FRH_dumps.py</code> Passive firing rate comparison <code>passive_FRH_grouped.py</code> Grouped passive firing rates <code>passive_FRH_grouped_sumchange.py</code> Summed change analysis <code>passive_firing_rates.py</code> Passive firing rate calculation <code>passive_firing_rates_delta.py</code> Delta firing rate analysis <code>_old/</code> subdirectory Archived versions"},{"location":"getting-started/migration-guide/removed-features/#alternative","title":"Alternative","text":"<p>LFP functionality is now available through the <code>ephys_data</code> module:</p> <pre><code>from utils.ephys_data.ephys_data import ephys_data\nfrom utils.ephys_data import lfp_processing\n\n# Load data\ndata = ephys_data(data_dir='/path/to/data')\n\n# Set LFP parameters\ndata.lfp_params = {\n    'freq_bounds': [1, 300],\n    'sampling_rate': 30000,\n    'taste_signal_choice': 'Start',\n    'fin_sampling_rate': 1000,\n    'trial_durations': [2000, 5000]\n}\n\n# Extract and load LFPs\ndata.extract_lfps()\ndata.get_lfps()\nlfp_data = data.lfp_array\n</code></pre> <p>See Ephys Data Reference for complete documentation.</p>"},{"location":"getting-started/migration-guide/removed-features/#hmm-analysis","title":"HMM Analysis","text":"<p>Hidden Markov Model analysis scripts have been removed:</p> Removed File Description <code>blech_hmm.py</code> Basic HMM implementation <code>blech_multinomial_hmm.py</code> Multinomial HMM <code>blech_poisson_hmm.py</code> Poisson HMM <code>blech_setup_hmm.py</code> HMM setup and configuration <code>variational_HMM_implement.py</code> Variational HMM implementation <code>variational_HMM_line_up_palatability_plot.py</code> Palatability alignment plots <code>variational_HMM_setup.py</code> Variational HMM setup"},{"location":"getting-started/migration-guide/removed-features/#alternatives","title":"Alternatives","text":"<p>HMM analysis is available in dedicated repositories:</p> <ul> <li>PyHMM - Original HMM implementation for neural data</li> <li>pytau - Updated HMM analysis toolkit</li> </ul> <p>These repositories provide HMM functionality with:</p> <ul> <li>Dedicated dependencies and environment management</li> <li>Active development and maintenance</li> <li>Documentation specific to HMM analysis</li> </ul>"},{"location":"getting-started/migration-guide/removed-features/#palatabilityidentity-analysis","title":"Palatability/Identity Analysis","text":"<p>The <code>additional_analyses/</code> directory has been removed:</p> Removed File Description <code>blech_palatability_identity_plot.py</code> Palatability/identity visualization <code>blech_palatability_identity_setup.py</code> Analysis setup <code>blech_palatability_regression.py</code> Regression analysis <code>blech_hmm_emg_plot.py</code> HMM-EMG combined plots <code>blech_identity_palatability_switch.py</code> Switch point analysis <code>identity_palatability_switch_EM.py</code> EM-based switch detection <code>identity_palatability_switch_EM_correlation_plot.py</code> Correlation visualization <code>identity_palatability_switch_EM_implement.py</code> EM implementation <code>identity_palatability_switch_correlation_plot.py</code> Correlation plots <code>identity_palatability_switch_functions.py</code> Helper functions <code>identity_palatability_switch_process.py</code> Processing pipeline <code>identity_palatability_switch_setup.py</code> Setup configuration"},{"location":"getting-started/migration-guide/removed-features/#alternative_1","title":"Alternative","text":"<p>Basic palatability analysis is available through the <code>ephys_data</code> module:</p> <pre><code>from utils.ephys_data.ephys_data import ephys_data\n\ndata = ephys_data(data_dir='/path/to/data')\ndata.get_unit_descriptors()\ndata.get_spikes()\ndata.get_firing_rates()\n\n# Calculate palatability correlations\ndata.calc_palatability()\n\n# Access results\npal_df = data.pal_df       # Palatability rankings\npal_array = data.pal_array # Palatability correlations over time\n</code></pre> <p>For advanced identity/palatability switch analysis, contact the lab.</p>"},{"location":"getting-started/migration-guide/removed-features/#laser-effect-analysis","title":"Laser Effect Analysis","text":"<p>The <code>laser_effect_analysis/</code> directory has been removed:</p> Removed File Description <code>compare_laser_effects.py</code> Laser effect comparison <code>laser_effects_plot.py</code> Laser effect visualization"},{"location":"getting-started/migration-guide/removed-features/#alternative_2","title":"Alternative","text":"<p>Laser trial separation is available through the <code>ephys_data</code> module:</p> <pre><code>from utils.ephys_data.ephys_data import ephys_data\n\ndata = ephys_data(data_dir='/path/to/data')\ndata.get_spikes()\ndata.get_firing_rates()\n\n# Check for laser trials\ndata.check_laser()\n\nif data.laser_exists:\n    # Separate data by laser condition\n    data.separate_laser_data()\n\n    # Access separated data\n    on_spikes = data.on_spikes\n    off_spikes = data.off_spikes\n    on_firing = data.on_firing\n    off_firing = data.off_firing\n</code></pre>"},{"location":"getting-started/migration-guide/removed-features/#miscellaneous-removed-files","title":"Miscellaneous Removed Files","text":""},{"location":"getting-started/migration-guide/removed-features/#examples-directory","title":"Examples Directory","text":"<p>The <code>examples/</code> directory has been removed:</p> Removed Description <code>Half-Gaussian PSTH example.ipynb</code> Jupyter notebook example <p>Example usage is now documented in the Tutorials and API reference pages.</p>"},{"location":"getting-started/migration-guide/removed-features/#root-level-scripts","title":"Root-Level Scripts","text":"<p>Several root-level scripts have been removed or relocated:</p> Removed Status <code>blech_dat_file_join.py</code> Functionality integrated into data loading <code>overlay_psth.py</code> Functionality in <code>blech_units_characteristics.py</code> and <code>ephys_data</code> module <p>See File Mapping for scripts that were relocated rather than removed.</p>"},{"location":"reference/","title":"API Reference","text":"<p>This section contains the API documentation for blech_clust modules.</p>"},{"location":"reference/#overview","title":"Overview","text":"<p>The blech_clust codebase is organized into several key modules:</p> <ul> <li>Core Pipeline - Main spike sorting pipeline modules</li> <li>Utilities - Helper functions and utility classes</li> <li>Ephys Data - Electrophysiology data handling and analysis</li> <li>EMG Analysis - EMG signal processing and analysis</li> </ul>"},{"location":"reference/#module-organization","title":"Module Organization","text":""},{"location":"reference/#core-pipeline-modules","title":"Core Pipeline Modules","text":"<p>Located in the repository root, these modules form the main spike sorting pipeline:</p> <ul> <li><code>blech_exp_info.py</code> - Experiment setup and metadata</li> <li><code>blech_init.py</code> - Directory initialization and data preparation</li> <li><code>blech_common_avg_reference.py</code> - Common average referencing</li> <li><code>blech_process.py</code> - Spike extraction and clustering</li> <li><code>blech_post_process.py</code> - Post-processing and unit selection</li> <li><code>blech_units_plot.py</code> - Waveform visualization</li> <li><code>blech_make_arrays.py</code> - Spike train array generation</li> </ul>"},{"location":"reference/#utility-modules","title":"Utility Modules","text":"<p>Located in <code>utils/</code>, these provide supporting functionality:</p> <ul> <li><code>blech_utils.py</code> - Core utility functions</li> <li><code>clustering/</code> - Clustering algorithms</li> <li><code>ephys_data/</code> - Electrophysiology data handling</li> <li><code>qa_utils/</code> - Quality assurance tools</li> </ul>"},{"location":"reference/#emg-modules","title":"EMG Modules","text":"<p>Located in <code>emg/</code>, these handle EMG signal analysis:</p> <ul> <li><code>emg_filter.py</code> - EMG signal filtering</li> <li><code>emg_freq_setup.py</code> - Frequency analysis setup</li> <li><code>get_gapes_Li.py</code> - Gape detection using QDA</li> </ul>"},{"location":"reference/#using-the-api","title":"Using the API","text":""},{"location":"reference/#importing-modules","title":"Importing Modules","text":"<pre><code># Import utility functions\nfrom utils.blech_utils import Tee, path_handler, imp_metadata\n\n# Import ephys data tools\nfrom utils.ephys_data import ephys_data\n\n# Import clustering utilities\nfrom utils.clustering import clustering\n</code></pre>"},{"location":"reference/#example-usage","title":"Example Usage","text":"<pre><code># Load experimental data\nfrom utils.ephys_data import ephys_data\n\n# Create data handler\ndata = ephys_data('/path/to/data')\n\n# Access spike trains\nspike_trains = data.spikes\n\n# Get unit information\nunits = data.units\n</code></pre>"},{"location":"reference/#documentation-format","title":"Documentation Format","text":"<p>Each module page includes:</p> <ul> <li>Overview - Module purpose and functionality</li> <li>Key Functions/Classes - Main components with descriptions</li> <li>Usage Examples - Code examples demonstrating usage</li> <li>Parameters - Detailed parameter descriptions</li> <li>Returns - Return value descriptions</li> </ul>"},{"location":"reference/#contributing","title":"Contributing","text":"<p>To improve the API documentation:</p> <ol> <li>Update docstrings in the source code following NumPy format</li> <li>Submit a pull request with your changes</li> <li>Documentation will be automatically rebuilt</li> </ol> <p>See CONTRIBUTING.md for guidelines.</p>"},{"location":"reference/core-pipeline/","title":"Core Pipeline","text":"<p>The core pipeline modules handle the main spike sorting workflow from raw data to sorted units.</p>"},{"location":"reference/core-pipeline/#pipeline-modules","title":"Pipeline Modules","text":""},{"location":"reference/core-pipeline/#blech_exp_infopy","title":"blech_exp_info.py","text":"<p>Pre-clustering step to annotate channels and save experimental parameters.</p> <p>Key Functions:</p> <ul> <li>Electrode layout configuration</li> <li>Digital input selection</li> <li>Taste/stimulus naming</li> <li>Palatability ranking</li> <li>Laser configuration</li> </ul> <p>Usage:</p> <pre><code>python blech_exp_info.py /path/to/data\n</code></pre>"},{"location":"reference/core-pipeline/#blech_initpy","title":"blech_init.py","text":"<p>Initialize directories and prepare data for clustering.</p> <p>Key Functions:</p> <ul> <li>Directory structure creation</li> <li>Data file organization</li> <li>Initial parameter setup</li> </ul> <p>Usage:</p> <pre><code>python blech_init.py\n</code></pre>"},{"location":"reference/core-pipeline/#blech_common_avg_referencepy","title":"blech_common_avg_reference.py","text":"<p>Perform common average referencing on electrode data.</p> <p>Key Functions:</p> <ul> <li>CAR group processing</li> <li>Signal referencing</li> <li>Artifact reduction</li> </ul> <p>Usage:</p> <pre><code>python blech_common_avg_reference.py\n</code></pre>"},{"location":"reference/core-pipeline/#blech_processpy","title":"blech_process.py","text":"<p>Core spike extraction and clustering module.</p> <p>Key Functions:</p> <ul> <li>Spike detection</li> <li>Feature extraction</li> <li>Clustering algorithms</li> <li>UMAP dimensionality reduction</li> </ul> <p>Usage:</p> <pre><code># Usually called via blech_run_process.sh for parallel execution\npython blech_process.py &lt;electrode_number&gt;\n</code></pre>"},{"location":"reference/core-pipeline/#blech_post_processpy","title":"blech_post_process.py","text":"<p>Add selected units to HDF5 file after manual curation.</p> <p>Key Functions:</p> <ul> <li>Unit selection</li> <li>HDF5 file updates</li> <li>Metadata management</li> </ul> <p>Usage:</p> <pre><code>python blech_post_process.py\n</code></pre>"},{"location":"reference/core-pipeline/#blech_units_plotpy","title":"blech_units_plot.py","text":"<p>Plot waveforms of selected spikes for visualization.</p> <p>Key Functions:</p> <ul> <li>Waveform plotting</li> <li>Unit visualization</li> <li>Quality metrics display</li> </ul> <p>Usage:</p> <pre><code>python blech_units_plot.py\n</code></pre>"},{"location":"reference/core-pipeline/#blech_make_arrayspy","title":"blech_make_arrays.py","text":"<p>Generate spike-train arrays for analysis.</p> <p>Key Functions:</p> <ul> <li>Spike train generation</li> <li>Trial alignment</li> <li>Array formatting</li> </ul> <p>Usage:</p> <pre><code>python blech_make_arrays.py\n</code></pre>"},{"location":"reference/core-pipeline/#pipeline-flow","title":"Pipeline Flow","text":"<pre><code>Raw Data\n    \u2193\nblech_exp_info.py (Setup)\n    \u2193\nblech_init.py (Initialization)\n    \u2193\nblech_common_avg_reference.py (Referencing)\n    \u2193\nblech_run_process.sh (Parallel Processing)\n    \u2193\nblech_post_process.py (Unit Selection)\n    \u2193\nblech_units_plot.py (Visualization)\n    \u2193\nblech_make_arrays.py (Array Generation)\n    \u2193\nSorted Units\n</code></pre>"},{"location":"reference/core-pipeline/#configuration-files","title":"Configuration Files","text":"<p>The pipeline uses several JSON configuration files:</p> <ul> <li><code>sorting_params.json</code> - Clustering parameters</li> <li><code>spike_detection_params.json</code> - Detection thresholds</li> <li><code>waveform_classifier_params.json</code> - Classifier settings</li> </ul>"},{"location":"reference/core-pipeline/#see-also","title":"See Also","text":"<ul> <li>Getting Started</li> <li>Tutorials</li> <li>Utilities</li> </ul>"},{"location":"reference/emg-analysis/","title":"EMG Analysis","text":"<p>Tools for analyzing electromyography (EMG) signals, including frequency analysis and gape detection.</p>"},{"location":"reference/emg-analysis/#overview","title":"Overview","text":"<p>The EMG analysis pipeline uses BSA/STFT (Bayesian Spectrum Analysis and Short-Time Fourier Transform) for frequency analysis.</p>"},{"location":"reference/emg-analysis/#shared-setup","title":"Shared Setup","text":""},{"location":"reference/emg-analysis/#emg_filterpy","title":"emg_filter.py","text":"<p>Filter EMG signals before analysis.</p> <p>Usage:</p> <pre><code>python emg/emg_filter.py\n</code></pre> <p>Filtering Steps:</p> <ol> <li>Differencing EMG channels within CAR groups (if multiple channels)</li> <li>Highpass filtering at 300 Hz (2nd order Butterworth)</li> <li>Rectification (absolute value)</li> <li>Lowpass filtering at 15 Hz for envelope extraction</li> </ol> <p>Output:</p> <ul> <li>Filtered EMG signals saved to HDF5 file</li> <li>Filter parameters logged</li> </ul>"},{"location":"reference/emg-analysis/#bsastft-branch","title":"BSA/STFT Branch","text":"<p>Frequency-based analysis of EMG signals.</p>"},{"location":"reference/emg-analysis/#emg_freq_setuppy","title":"emg_freq_setup.py","text":"<p>Configure parameters for frequency analysis.</p> <p>Usage:</p> <pre><code>python emg/emg_freq_setup.py\n</code></pre> <p>Parameters:</p> <ul> <li>Frequency bands of interest</li> <li>Window sizes</li> <li>Overlap parameters</li> <li>Output directories</li> </ul>"},{"location":"reference/emg-analysis/#parallel-processing","title":"Parallel Processing","text":"<p>Run frequency analysis in parallel:</p> <pre><code>bash blech_emg_jetstream_parallel.sh\n</code></pre> <p>Note: This script is automatically generated by <code>emg_freq_setup.py</code> and uses GNU parallel for distributed processing.</p> <p>The script:</p> <ol> <li>Divides trials across processors</li> <li>Runs BSA/STFT on each subset</li> <li>Saves intermediate results</li> </ol>"},{"location":"reference/emg-analysis/#emg_freq_post_processpy","title":"emg_freq_post_process.py","text":"<p>Aggregate and process frequency analysis results.</p> <p>Usage:</p> <pre><code>python emg/emg_freq_post_process.py\n</code></pre> <p>Processing Steps:</p> <ol> <li>Combine results from parallel jobs</li> <li>Normalize power spectra</li> <li>Calculate summary statistics</li> <li>Identify significant frequency changes</li> </ol>"},{"location":"reference/emg-analysis/#emg_freq_plotpy","title":"emg_freq_plot.py","text":"<p>Generate visualizations of frequency analysis.</p> <p>Usage:</p> <pre><code>python emg/emg_freq_plot.py\n</code></pre> <p>Plots Generated:</p> <ul> <li>Spectrograms</li> <li>Power spectrum time courses</li> <li>Frequency band comparisons</li> <li>Trial-averaged responses</li> </ul>"},{"location":"reference/emg-analysis/#emg-data-structure","title":"EMG Data Structure","text":""},{"location":"reference/emg-analysis/#hdf5-organization","title":"HDF5 Organization","text":"<pre><code>data.h5\n\u251c\u2500\u2500 emg_data/\n\u2502   \u251c\u2500\u2500 dig_in_&lt;N&gt;/\n\u2502   \u2502   \u251c\u2500\u2500 emg_array           # Raw EMG data\n\u2502   \u2502   \u2514\u2500\u2500 processed_emg/\n\u2502   \u2502       \u251c\u2500\u2500 &lt;car&gt;_emg_filt  # Highpass filtered signal\n\u2502   \u2502       \u2514\u2500\u2500 &lt;car&gt;_emg_env   # Envelope (lowpass filtered)\n\u2502   \u251c\u2500\u2500 ind_electrode_map       # Electrode mapping\n\u2502   \u2514\u2500\u2500 emg_sig_trials          # Significant trial indicators\n\u251c\u2500\u2500 emg_BSA_results/            # BSA/STFT frequency analysis\n\u2502   \u251c\u2500\u2500 omega                   # Frequency values\n\u2502   \u251c\u2500\u2500 &lt;car&gt;/\n\u2502   \u2502   \u2514\u2500\u2500 taste&lt;N&gt;_p          # Power spectrum per taste\n\u2502   \u251c\u2500\u2500 gapes                   # Detected gape events\n\u2502   \u251c\u2500\u2500 licking                 # Detected licking events\n\u2502   \u2514\u2500\u2500 emg_BSA_results_final   # Combined results\n</code></pre>"},{"location":"reference/emg-analysis/#configuration","title":"Configuration","text":""},{"location":"reference/emg-analysis/#emg_paramsjson","title":"emg_params.json","text":"<p>EMG analysis parameters:</p> <pre><code>{\n    \"emg_env\": \"/path/to/conda/envs/emg_env\",\n    \"stft_params\": {\n        \"max_freq\": 20,\n        \"time_range_tuple\": [0, 7],\n        \"Fs\": 1000,\n        \"signal_window\": 400,\n        \"window_overlap\": 399\n    },\n    \"use_BSA\": true\n}\n</code></pre> <ul> <li><code>emg_env</code>: Path to conda environment for EMG processing</li> <li><code>stft_params</code>: Short-time Fourier transform parameters<ul> <li><code>max_freq</code>: Maximum frequency to analyze (Hz)</li> <li><code>time_range_tuple</code>: Time window for analysis (seconds)</li> <li><code>Fs</code>: Sampling frequency (Hz)</li> <li><code>signal_window</code>: Window size for STFT (samples)</li> <li><code>window_overlap</code>: Overlap between windows (samples)</li> </ul> </li> <li><code>use_BSA</code>: Whether to use Bayesian Spectrum Analysis (true) or STFT (false)</li> </ul>"},{"location":"reference/emg-analysis/#usage-examples","title":"Usage Examples","text":""},{"location":"reference/emg-analysis/#complete-bsastft-workflow","title":"Complete BSA/STFT Workflow","text":"<pre><code># Filter EMG signals\npython emg/emg_filter.py\n\n# Setup frequency analysis\npython emg/emg_freq_setup.py\n\n# Run parallel analysis\nbash blech_emg_jetstream_parallel.sh\n\n# Post-process results\npython emg/emg_freq_post_process.py\n\n# Generate plots\npython emg/emg_freq_plot.py\n</code></pre>"},{"location":"reference/emg-analysis/#programmatic-access","title":"Programmatic Access","text":"<pre><code>import tables\nimport numpy as np\n\n# Load filtered EMG data\nwith tables.open_file('data.h5', 'r') as hf5:\n    emg_filtered = hf5.root.emg.filtered[:]\n\n# Load gape events\nwith tables.open_file('data.h5', 'r') as hf5:\n    gape_times = hf5.root.emg.gapes.onset_times[:]\n    gape_durations = hf5.root.emg.gapes.durations[:]\n\n# Analyze gape timing\nprint(f\"Detected {len(gape_times)} gapes\")\nprint(f\"Mean duration: {np.mean(gape_durations):.2f} ms\")\n</code></pre>"},{"location":"reference/emg-analysis/#analysis-tips","title":"Analysis Tips","text":""},{"location":"reference/emg-analysis/#frequency-analysis","title":"Frequency Analysis","text":"<ul> <li>Use appropriate frequency bands for your species/preparation</li> <li>Adjust window size based on temporal resolution needs</li> <li>Consider trial-to-trial variability</li> </ul>"},{"location":"reference/emg-analysis/#quality-control","title":"Quality Control","text":"<ul> <li>Inspect filtered signals visually</li> <li>Check for artifacts</li> <li>Validate detected events manually for subset of trials</li> </ul>"},{"location":"reference/emg-analysis/#references","title":"References","text":"<ul> <li>Mukherjee, N., Wachutka, J., &amp; Katz, D. B. (2019). Impact of precisely-timed inhibition of gustatory cortex on taste behavior depends on single-trial ensemble dynamics. eLife, 8, e45968.</li> <li>Li JX, Maier JX, Reid EE, Katz DB. Sensory Cortical Activity Is Related to the Selection of a Rhythmic Motor Action Pattern. J Neurosci. 2016 May 18;36(20):5596-607. doi: 10.1523/JNEUROSCI.3949-15.2016. PMID: 27194338; PMCID: PMC4871991.</li> </ul>"},{"location":"reference/emg-analysis/#see-also","title":"See Also","text":"<ul> <li>Core Pipeline</li> <li>Utilities</li> <li>Tutorials</li> </ul>"},{"location":"reference/ephys-data/","title":"Ephys Data Module","text":"<p>The <code>ephys_data</code> module provides tools for analyzing electrophysiology data, including spike trains, local field potentials (LFPs), and EMG signals.</p>"},{"location":"reference/ephys-data/#modules","title":"Modules","text":""},{"location":"reference/ephys-data/#ephys_datapy","title":"ephys_data.py","text":"<p>Core class for loading and analyzing electrophysiology data.</p>"},{"location":"reference/ephys-data/#visualizepy","title":"visualize.py","text":"<p>Functions for visualizing neural data, including raster plots and firing rate heatmaps.</p>"},{"location":"reference/ephys-data/#lfp_processingpy","title":"lfp_processing.py","text":"<p>Tools for extracting and processing LFP data from raw recordings.</p>"},{"location":"reference/ephys-data/#bakspy","title":"BAKS.py","text":"<p>Implementation of Bayesian Adaptive Kernel Smoother for firing rate estimation.</p>"},{"location":"reference/ephys-data/#basic-usage","title":"Basic Usage","text":"<pre><code>from utils.ephys_data.ephys_data import ephys_data\n\n# Initialize with data directory\ndata = ephys_data(data_dir='/path/to/data')\n\n# Load basic data\ndata.get_unit_descriptors()  # Get unit information\ndata.get_spikes()           # Extract spike data\n\n# Calculating firing rates and LFPs needs additional parameters\n# If not set, the module will use default values\n# See `Default Parameters` section for details\n\n# data.firing_rate_params = { ... }\n# or\n# data.firing_rate_params = data.default_firing_params\n\n# data.stft_params = { ... }\n# or\n# data.lfp_params = data.default_lfp_params\n\ndata.get_firing_rates()     # Calculate firing rates\ndata.get_lfps()            # Extract LFP data\n\n# Same for STFT calculation\ndata.stft_params = { ... }\ndata.get_stft()            # Calculate spectrograms\n</code></pre>"},{"location":"reference/ephys-data/#key-features","title":"Key Features","text":""},{"location":"reference/ephys-data/#spike-and-firing-rate-analysis","title":"Spike and Firing Rate Analysis","text":"<pre><code># Access spike data\nspikes = data.spikes       # List of spike arrays per taste\nfiring = data.firing_array # 4D array of firing rates\n</code></pre>"},{"location":"reference/ephys-data/#lfp-analysis","title":"LFP Analysis","text":"<pre><code># Access LFP data\nlfps = data.lfp_array     # Raw LFP data\nstft = data.stft_array    # Complex Spectrograms\namplitude = data.amplitude_array # STFT Amplitude (power)\nphase = data.phase_array  # STFT Phase\n</code></pre>"},{"location":"reference/ephys-data/#region-based-analysis","title":"Region-Based Analysis","text":"<pre><code># Get units by brain region\ndata.get_region_units()\nregion_spikes = data.return_region_spikes('region_name')\nregion_firing = data.get_region_firing('region_name')\n\n# Get LFPs by region\nregion_lfps, region_names = data.return_region_lfps()\n</code></pre>"},{"location":"reference/ephys-data/#laser-condition-analysis","title":"Laser Condition Analysis","text":"<pre><code># Check for laser trials\ndata.check_laser()\n\n# Separate data by laser condition\ndata.separate_laser_data()  # Separates spikes, firing rates, and LFPs\n\n# Access separated data\non_spikes = data.on_spikes\noff_spikes = data.off_spikes\non_firing = data.on_firing\noff_firing = data.off_firing\n</code></pre>"},{"location":"reference/ephys-data/#trial-information","title":"Trial Information","text":"<pre><code># Get trial information\ndata.get_trial_info_frame()\n\n# Sequester trials by condition\ndata.sequester_trial_inds()\ndata.get_sequestered_spikes()\ndata.get_sequestered_firing()\n</code></pre>"},{"location":"reference/ephys-data/#palatability-analysis","title":"Palatability Analysis","text":"<pre><code># Calculate palatability correlations\ndata.calc_palatability()\n\n# Access results\npal_df = data.pal_df       # Palatability rankings\npal_array = data.pal_array # Palatability correlations\n</code></pre>"},{"location":"reference/ephys-data/#example-workflows","title":"Example Workflows","text":""},{"location":"reference/ephys-data/#ephys-data-processing","title":"Ephys Data Processing","text":"<pre><code>from utils.ephys_data.ephys_data import ephys_data\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load data\ndata = ephys_data(data_dir='/path/to/data')\n\n# Get basic information\ndata.get_unit_descriptors()\ndata.get_spikes()\ndata.get_firing_rates()\ndata.get_lfps()\n\n# Analyze by brain region\ndata.get_region_units()\nprint(f\"Available regions: {data.region_names}\")\n\n# Get spikes for a specific region\nregion_spikes = data.return_region_spikes('GC')  # Gustatory cortex example\n\n# Analyze laser conditions (if applicable)\ndata.check_laser()\nif data.laser_exists:\n    data.separate_laser_data()\n    # Compare firing rates between conditions\n    plt.figure(figsize=(12, 6))\n    plt.subplot(1, 2, 1)\n    plt.title('Laser OFF')\n    plt.imshow(np.mean(data.off_firing[0], axis=0), aspect='auto', cmap='viridis')\n    plt.subplot(1, 2, 2)\n    plt.title('Laser ON')\n    plt.imshow(np.mean(data.on_firing[0], axis=0), aspect='auto', cmap='viridis')\n    plt.tight_layout()\n    plt.show()\n\n# Calculate palatability correlation\ndata.calc_palatability()\nplt.figure(figsize=(10, 6))\nplt.imshow(data.pal_array, aspect='auto', cmap='viridis')\nplt.colorbar(label='|Palatability Correlation|')\nplt.xlabel('Time (bins)')\nplt.ylabel('Neuron')\nplt.title('Palatability Correlation Over Time')\nplt.show()\n</code></pre>"},{"location":"reference/ephys-data/#visualization","title":"Visualization","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom utils.ephys_data.visualize import raster, firing_overview\n\n# Create sample spike data (binary array where 1 indicates a spike)\nspike_array = np.zeros((10, 100))  # 10 trials, 100 time points\nspike_array[2, 20:25] = 1  # Add some spikes\nspike_array[5, 40:45] = 1\nspike_array[7, 60:65] = 1\n\n# Create a figure and axis\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Generate raster plot\nraster(ax, spike_array, marker='|', color='black')\nax.set_xlabel('Time (ms)')\nax.set_ylabel('Trial')\nax.set_title('Example Raster Plot')\nplt.show()\n\n# Create sample firing rate data for multiple neurons\n# Shape: (neurons, trials, time points)\nn_neurons = 4\nn_trials = 10\nn_timepoints = 100\ndata = np.random.rand(n_neurons, n_trials, n_timepoints)\n\n# Add some structure to the data\nfor i in range(n_neurons):\n    # Create a peak at different times for each neuron\n    peak_time = 20 + i*15\n    data[i, :, peak_time-5:peak_time+5] += 2\n\n# Generate firing rate overview\nfig, ax = firing_overview(\n    data,\n    t_vec=np.arange(n_timepoints) * 10,  # 10ms bins\n    cmap='viridis',\n    cmap_lims='shared',\n    subplot_labels=np.arange(n_neurons),\n    zscore_bool=True,\n    figsize=(12, 10)\n)\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"reference/ephys-data/#lfp-processing","title":"LFP Processing","text":"<pre><code>from utils.ephys_data.ephys_data import ephys_data\nfrom utils.ephys_data import lfp_processing\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load data using the ephys_data class\ndata = ephys_data(data_dir='/path/to/data')\n\n# Set LFP parameters (optional - defaults will be used if not set)\ndata.lfp_params = {\n    'freq_bounds': [1, 300],          # Frequency range in Hz\n    'sampling_rate': 30000,           # Original sampling rate\n    'taste_signal_choice': 'Start',   # Trial alignment\n    'fin_sampling_rate': 1000,        # Final sampling rate\n    'trial_durations': [2000, 5000]   # Pre/post trial durations in ms\n}\n\n# Extract LFPs (handles trial_info_frame internally)\ndata.extract_lfps()\n\n# Load the extracted LFP data\ndata.get_lfps()\nlfp_data = data.lfp_array  # Shape: (tastes, channels, trials, time)\n\n# Identify good quality trials for a single taste\ngood_trials_bool = lfp_processing.return_good_lfp_trial_inds(\n    data=lfp_data[0],  # First taste, shape: (channels, trials, time)\n    MAD_threshold=3    # Number of MADs to use as threshold\n)\n\n# Get only the good trials\ngood_lfp_data = lfp_data[0][:, good_trials_bool, :]\n\n# Plot mean of good trials\nplt.figure(figsize=(12, 6))\nplt.plot(np.mean(good_lfp_data, axis=(0, 1)), 'b-')\nplt.xlabel('Time (ms)')\nplt.ylabel('Amplitude')\nplt.title('Mean LFP (Good Trials Only)')\nplt.show()\n</code></pre>"},{"location":"reference/ephys-data/#default-parameters","title":"Default Parameters","text":"<p>The module comes with sensible defaults for various analyses:</p> <pre><code># Default firing rate parameters\ndefault_firing_params = {\n    'type': 'conv',\n    'step_size': 25,\n    'window_size': 250,\n    'dt': 1,\n    'baks_resolution': 25e-3,\n    'baks_dt': 1e-3\n}\n\n# Default LFP parameters\ndefault_lfp_params = {\n    'freq_bounds': [1,300],\n    'sampling_rate': 30000,\n    'taste_signal_choice': 'Start',\n    'fin_sampling_rate': 1000,\n    'trial_durations': [2000,5000]\n}\n</code></pre>"},{"location":"reference/ephys-data/#data-structure","title":"Data Structure","text":"<p>The module expects data organized in a specific way: - HDF5 file containing spike trains and LFP data - JSON info file with experimental parameters - Trial information CSV file</p> <p>See the main repository documentation for details on data organization.</p>"},{"location":"reference/utilities/","title":"Utilities","text":"<p>Utility modules provide supporting functionality for the spike sorting pipeline.</p>"},{"location":"reference/utilities/#blech_utils","title":"blech_utils","text":"<p>Core utility functions used throughout the codebase.</p>"},{"location":"reference/utilities/#key-classes-and-functions","title":"Key Classes and Functions","text":""},{"location":"reference/utilities/#tee","title":"Tee","text":"<p>Redirect stdout to both console and file for logging.</p> <pre><code>from utils.blech_utils import Tee\nimport sys\n\n# Redirect output to file and console\nsys.stdout = Tee('/path/to/data/dir', name='logfile.txt')\nprint(\"This goes to both console and file\")\n</code></pre>"},{"location":"reference/utilities/#path_handler","title":"path_handler","text":"<p>Handle file paths and directory operations.</p> <pre><code>from utils.blech_utils import path_handler\n\n# Instantiate path handler\nph = path_handler()\n\n# Access blech_clust directory\nblech_dir = ph.blech_clust_dir\n\n# Access home directory\nhome = ph.home_dir\n</code></pre>"},{"location":"reference/utilities/#imp_metadata","title":"imp_metadata","text":"<p>Import and manage experimental metadata.</p> <pre><code>from utils.blech_utils import imp_metadata\nimport sys\n\n# Load metadata (requires sys.argv or list with directory path)\nmetadata = imp_metadata([sys.argv, '/path/to/data'])\n\n# Access metadata attributes\nhdf5_name = metadata.hdf5_name\ninfo_dict = metadata.info_dict\nlayout_df = metadata.layout\n</code></pre>"},{"location":"reference/utilities/#clustering-utilities","title":"Clustering Utilities","text":"<p>Located in <code>utils/clustering/</code>, these modules provide clustering algorithms and tools.</p>"},{"location":"reference/utilities/#key-functions","title":"Key Functions","text":"<ul> <li>Spike clustering algorithms</li> <li>Feature extraction</li> <li>Cluster validation</li> <li>Merge/split operations</li> </ul>"},{"location":"reference/utilities/#data-management","title":"Data Management","text":""},{"location":"reference/utilities/#ephys_data-module","title":"ephys_data Module","text":"<p>Comprehensive data handling for electrophysiology recordings.</p> <p>See Ephys Data for detailed documentation.</p>"},{"location":"reference/utilities/#helper-scripts","title":"Helper Scripts","text":""},{"location":"reference/utilities/#infer_rnn_ratespy","title":"infer_rnn_rates.py","text":"<p>Infer firing rates from spike trains using RNN.</p> <pre><code>python utils/infer_rnn_rates.py &lt;data_dir&gt; [options]\n</code></pre> <p>Options:</p> <ul> <li><code>--train_steps</code>: Number of training steps</li> <li><code>--hidden_size</code>: RNN hidden layer size</li> <li><code>--bin_size</code>: Spike binning size</li> <li><code>--retrain</code>: Force model retraining</li> </ul>"},{"location":"reference/utilities/#blech_data_summarypy","title":"blech_data_summary.py","text":"<p>Generate comprehensive dataset summary.</p> <pre><code>python utils/blech_data_summary.py\n</code></pre>"},{"location":"reference/utilities/#grade_datasetpy","title":"grade_dataset.py","text":"<p>Grade dataset quality based on metrics.</p> <pre><code>python utils/grade_dataset.py\n</code></pre>"},{"location":"reference/utilities/#configuration-management","title":"Configuration Management","text":""},{"location":"reference/utilities/#parameter-files","title":"Parameter Files","text":"<p>Utilities for loading and managing parameter files:</p> <ul> <li>JSON parameter loading</li> <li>Parameter validation</li> <li>Default value handling</li> </ul>"},{"location":"reference/utilities/#example","title":"Example","text":"<pre><code>import json\n\n# Load parameters\nwith open('params/sorting_params.json', 'r') as f:\n    params = json.load(f)\n\n# Access parameters\nmax_clusters = params['max_clusters']\nmin_cluster_size = params['min_cluster_size']\n</code></pre>"},{"location":"reference/utilities/#file-io","title":"File I/O","text":""},{"location":"reference/utilities/#hdf5-operations","title":"HDF5 Operations","text":"<p>Functions for reading and writing HDF5 files:</p> <pre><code>import tables\n\n# Open HDF5 file\nwith tables.open_file('data.h5', 'r') as hf5:\n    # Read spike times\n    spike_times = hf5.root.spike_times[:]\n\n    # Read unit information\n    units = hf5.root.units[:]\n</code></pre>"},{"location":"reference/utilities/#binary-data","title":"Binary Data","text":"<p>Functions for reading Intan binary data:</p> <ul> <li>Amplifier data (<code>.dat</code> files)</li> <li>Digital input data (DIN files)</li> <li>Auxiliary input data</li> </ul>"},{"location":"reference/utilities/#see-also","title":"See Also","text":"<ul> <li>Core Pipeline</li> <li>Ephys Data</li> </ul>"}]}