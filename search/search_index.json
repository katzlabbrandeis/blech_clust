{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"blech_clust","text":"<p>Python and R based code for clustering and sorting electrophysiology data</p> <p> </p>"},{"location":"#overview","title":"Overview","text":"<p>blech_clust is a comprehensive Python and R based toolkit for clustering and sorting electrophysiology data recorded using the Intan RHD2132 chips. Originally written for cortical multi-electrode recordings in Don Katz's lab at Brandeis University, it's optimized for high-performance computing clusters but can be easily modified to work in any parallel environment.</p> <p>Visit the Katz lab website for more information.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Automated Spike Sorting: Complete pipeline from raw data to sorted units</li> <li>EMG Analysis: Multiple approaches including BSA/STFT and QDA-based gape detection</li> <li>Quality Assessment: Built-in tools for dataset quality grading and validation</li> <li>Parallel Processing: Optimized for HPC environments</li> <li>Comprehensive Documentation: Detailed API reference and tutorials</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Getting Started - Installation and setup instructions</li> <li>API Reference - Complete API documentation</li> <li>Tutorials - Step-by-step guides</li> <li>GitHub Repository</li> <li>Blog</li> </ul>"},{"location":"#pipeline-overview","title":"Pipeline Overview","text":""},{"location":"#spike-sorting-pipeline","title":"Spike Sorting Pipeline","text":"<p>For the complete main spike-sorting pipeline (including the operations workflow diagram, detailed steps, and nomnoml schema), please refer to the README.</p>"},{"location":"#emg-analysis-pipelines","title":"EMG Analysis Pipelines","text":"<p>For details on EMG analysis workflows, see the README and Workflow Documentation.</p>"},{"location":"#installation","title":"Installation","text":"<p>The installation process is managed through a Makefile that handles all dependencies:</p> <pre><code># Clone the repository\ngit clone https://github.com/katzlabbrandeis/blech_clust.git\ncd blech_clust\n\n# Install everything\nmake all\n\n# Activate the environment\nconda activate blech_clust\n</code></pre> <p>For more detailed installation instructions, see the Getting Started guide.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions! Please read our CONTRIBUTING.md file for guidelines.</p>"},{"location":"#contributing-to-documentation","title":"Contributing to Documentation","text":"<p>Help us improve the documentation:</p> <ul> <li>Report issues: Found an error or unclear explanation? Open an issue</li> <li>Suggest improvements: Have ideas for better organization or content? We'd love to hear them</li> <li>Submit changes: See docs/README.md for instructions on building and updating documentation</li> </ul> <p>The documentation is built with MkDocs and automatically deployed via GitHub Actions.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use this code in your research, please cite:</p> <pre><code>@software{blech_clust_katz,\n  author       = {Mahmood, Abuzar and\n                  Mukherjee, Narendra and\n                  Stone, Bradly and\n                  Raymond, Martin and\n                  Germaine, Hannah and\n                  Lin, Jian-You and\n                  Mazzio, Christina and\n                  Katz, Donald},\n  title        = {katzlabbrandeis/blech\\_clust: v1.1.0},\n  month        = apr,\n  year         = 2025,\n  publisher    = {Zenodo},\n  version      = {1.1.0},\n  doi          = {10.5281/zenodo.15175273},\n  url          = {https://doi.org/10.5281/zenodo.15175273}\n}\n</code></pre>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<p>This work used ACCESS-allocated resources at Brandeis University through allocation BIO230103 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services &amp; Support (ACCESS) program, which is supported by U.S. National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296.</p> <p>The project titled \"Computational Processing and Modeling of Neural Ensembles in Identifying the Nonlinear Dynamics of Taste Perception\" was led by PI Abuzar Mahmood. The computational allocation was active from 2023-06-26 to 2024-06-25.</p>"},{"location":"tutorials/","title":"Tutorials","text":""},{"location":"tutorials/#workflow-walkthrough","title":"Workflow Walkthrough","text":"<p>This tutorial walks you through the complete spike sorting pipeline from raw data to analyzed units.</p>"},{"location":"tutorials/#step-1-experiment-information-setup","title":"Step 1: Experiment Information Setup","text":"<p>Open a terminal and run:</p> <pre><code>cd /path/to/blech_clust  # Make the blech_clust repository your working directory\nconda activate blech_clust  # Activate blech_clust environment\nDIR=/path/to/raw/data/files  # Save the path of the target Intan data to be sorted\npython blech_exp_info.py $DIR  # Generate metadata and electrode layout\n</code></pre>"},{"location":"tutorials/#configuring-car-groups","title":"Configuring CAR Groups","text":"<p>Once you've started running the script, it will ask you to \"fill in car groups\". Go to the Intan data folder, where you'll find a file named <code>[...]_electrode_layout.csv</code>.</p> <ol> <li>Open this file in a spreadsheet editor</li> <li>Fill in the <code>CAR_group</code> column</li> <li>Give all electrodes implanted in the same bundle the same identifier</li> <li>Use different identifiers for different bundles</li> <li>Example: All electrodes from a bundle in right GC are called <code>GC1</code>, and all electrodes from a bundle in left GC are called <code>GC2</code></li> <li>Return to the terminal and type <code>y</code> then press <code>enter</code></li> </ol>"},{"location":"tutorials/#selecting-digital-inputs","title":"Selecting Digital Inputs","text":"<p>The script will search your data folder for DIN files and print something like:</p> <pre><code>(0, 'board-DIN-09.dat'),\n(1, 'board-DIN-11.dat'),\n(2, 'board-DIN-12.dat'),\n(3, 'board-DIN-13.dat')\n</code></pre> <p>These are the files for the Intan digital inputs that correspond to stimulus presentations and/or laser activations.</p> <p>Prompt: <code>Taste dig_ins used (IN ORDER, anything separated) :: \"x\" to exit ::</code></p> <ul> <li>Select the DINs to include in later analysis steps</li> <li>Example: To include DINs 11 and 13 but not 09 or 12, type <code>1,3</code> and press <code>enter</code></li> <li>Note: If you have a DIN for laser activations, do not include it here; it will be requested later</li> </ul>"},{"location":"tutorials/#naming-tastes","title":"Naming Tastes","text":"<p>Prompt: <code>Tastes names used (IN ORDER, anything separated) :: \"x\" to exit ::</code></p> <ul> <li>Provide taste names for each selected DIN</li> <li>Example: If DIN-11 was DI H2O and DIN-13 was 300mM sucrose, enter <code>Water,Sucrose</code></li> <li>Leave off the molarity (provided in the next step)</li> </ul>"},{"location":"tutorials/#specifying-concentrations","title":"Specifying Concentrations","text":"<p>Prompt: <code>Corresponding concs used (in M, IN ORDER, COMMA separated) :: \"x\" to exit ::</code></p> <ul> <li>Provide numeric inputs for concentrations in Molarity</li> <li>Example: For DI H2O and 300mM sucrose, enter <code>0,0.3</code></li> </ul>"},{"location":"tutorials/#palatability-rankings","title":"Palatability Rankings","text":"<p>Prompt: <code>Enter palatability rankings used (anything separated), higher number = more palatable :: \"x\" to exit ::</code></p> <ul> <li>Provide numeric rankings (&gt; 0 and &lt;= number of stimuli)</li> <li>Can be non-integer and accept duplicates</li> <li>Valid examples: <code>4,3,2,1</code> or <code>0.4,0.3,0.2,0.1</code> or <code>3,2,2,1</code></li> <li>Invalid examples: <code>2,1,1,0</code> (contains 0) or <code>5,4,3,2</code> (exceeds number of stimuli)</li> <li>Example for water/sucrose: <code>1,2</code></li> </ul>"},{"location":"tutorials/#laser-configuration","title":"Laser Configuration","text":"<p>Prompt: <code>Laser dig_in index, &lt;BLANK&gt; for none::: \"x\" to exit ::</code></p> <ul> <li>If you have a laser DIN, enter its index (e.g., <code>0</code> for DIN-09)</li> <li>If no laser, just press <code>enter</code></li> </ul>"},{"location":"tutorials/#experiment-notes","title":"Experiment Notes","text":"<p>Prompt: <code>Please enter any notes about the experiment.</code></p> <ul> <li>Enter any pertinent comments or press <code>enter</code> to finish</li> </ul>"},{"location":"tutorials/#step-2-parameter-configuration","title":"Step 2: Parameter Configuration","text":"<p>Before running the clustering pipeline, set up parameter files:</p> <ol> <li>Copy <code>blech_clust/params/_templates/sorting_params_template.json</code> to <code>blech_clust/params/sorting_params_template.json</code></li> <li>Update the parameters as needed for your experiment</li> <li>Also copy and adapt:</li> <li><code>waveform_classifier_params.json</code></li> <li><code>emg_params.json</code></li> </ol>"},{"location":"tutorials/#step-3-run-the-pipeline","title":"Step 3: Run the Pipeline","text":""},{"location":"tutorials/#using-convenience-scripts","title":"Using Convenience Scripts","text":"<pre><code>bash blech_clust_pre.sh $DIR   # Perform steps up to spike extraction and UMAP\npython blech_post_process.py   # Add sorted units to HDF5 (CLI or .CSV as input)\nbash blech_clust_post.sh       # Perform steps up to PSTH generation\n</code></pre>"},{"location":"tutorials/#or-use-the-automated-script","title":"Or Use the Automated Script","text":"<pre><code>bash blech_autosort.sh &lt;data_directory&gt; [--force]\n</code></pre> <ul> <li><code>&lt;data_directory&gt;</code>: Path to the directory containing the raw data files</li> <li><code>--force</code>: Optional flag to force re-processing even if previous results exist</li> </ul> <p>The <code>blech_autosort.sh</code> script:</p> <ul> <li>Checks for required parameter files</li> <li>Verifies that specific settings are enabled</li> <li>Executes the pre-processing, clustering, and post-processing steps in sequence</li> </ul>"},{"location":"tutorials/#step-4-quality-assessment","title":"Step 4: Quality Assessment","text":"<p>After processing, assess the quality of your dataset:</p> <pre><code>python blech_unit_characteristics.py  # Analyze unit characteristics\npython utils/blech_data_summary.py    # Generate comprehensive dataset summary\npython utils/grade_dataset.py         # Grade dataset quality based on metrics\n</code></pre>"},{"location":"tutorials/#emg-analysis-tutorial","title":"EMG Analysis Tutorial","text":""},{"location":"tutorials/#shared-setup","title":"Shared Setup","text":"<ol> <li>Complete spike sorting through <code>blech_make_arrays.py</code></li> <li>Filter EMG signals:    <pre><code>python emg_filter.py\n</code></pre></li> </ol>"},{"location":"tutorials/#bsastft-branch","title":"BSA/STFT Branch","text":"<p>For Bayesian Spectrum Analysis and Short-Time Fourier Transform:</p> <pre><code>python emg_freq_setup.py              # Configure parameters for frequency analysis\nbash blech_emg_jetstream_parallel.sh  # Parallel processing of EMG signals\npython emg_freq_post_process.py       # Aggregate and process results\npython emg_freq_plot.py               # Generate visualizations\n</code></pre>"},{"location":"tutorials/#qda-branch","title":"QDA Branch","text":"<p>For Quadratic Discriminant Analysis (gape detection):</p> <pre><code>python emg_freq_setup.py  # Setup parameters for gape detection\npython get_gapes_Li.py    # Detect gapes using QDA classifier\n</code></pre>"},{"location":"tutorials/#testing-your-installation","title":"Testing Your Installation","text":""},{"location":"tutorials/#local-testing-with-prefect","title":"Local Testing with Prefect","text":"<ol> <li> <p>Start the Prefect server in a separate terminal:    <pre><code>prefect server start\n</code></pre></p> </li> <li> <p>In another terminal, run the tests:    <pre><code>cd &lt;path_to_blech_clust&gt;\nmake prefect  # Install/update Prefect\n</code></pre></p> </li> <li> <p>Run specific test suites:    <pre><code># Run all tests\npython pipeline_testing/prefect_pipeline.py --all\n\n# Run only spike sorting tests\npython pipeline_testing/prefect_pipeline.py -s\n\n# Run only EMG analysis tests\npython pipeline_testing/prefect_pipeline.py -e\n\n# Run spike sorting followed by EMG analysis\npython pipeline_testing/prefect_pipeline.py --spike-emg\n</code></pre></p> </li> </ol> <p>Monitor test progress at http://localhost:4200</p>"},{"location":"tutorials/#advanced-topics","title":"Advanced Topics","text":""},{"location":"tutorials/#rnn-based-firing-rate-inference","title":"RNN-based Firing Rate Inference","text":"<p>Use the <code>infer_rnn_rates.py</code> utility to infer firing rates from spike trains:</p> <pre><code>python utils/infer_rnn_rates.py &lt;data_dir&gt; [options]\n</code></pre> <p>Options:</p> <ul> <li><code>--override_config</code>: Override config file and use provided arguments</li> <li><code>--train_steps TRAIN_STEPS</code>: Number of training steps (default: 15000)</li> <li><code>--hidden_size HIDDEN_SIZE</code>: Hidden size of RNN (default: 8)</li> <li><code>--bin_size BIN_SIZE</code>: Bin size for binning spikes (default: 25)</li> <li><code>--train_test_split TRAIN_TEST_SPLIT</code>: Fraction of data for training (default: 0.75)</li> <li><code>--no_pca</code>: Do not use PCA for preprocessing</li> <li><code>--retrain</code>: Force retraining of model</li> <li><code>--time_lims TIME_LIMS TIME_LIMS</code>: Time limits for inferred firing rates (default: [1500, 4500])</li> </ul>"},{"location":"tutorials/#additional-resources","title":"Additional Resources","text":"<ul> <li>Module Documentation: Detailed documentation for the ephys_data module</li> <li>Wiki: Additional guides and information</li> <li>Blog: Insights and updates from the development team</li> </ul>"},{"location":"workflow/","title":"Workflow Diagrams","text":"<p>This page provides additional visual representations and workflow details for the blech_clust pipeline.</p>"},{"location":"workflow/#main-spike-sorting-pipeline","title":"Main Spike-Sorting Pipeline","text":"<p>For the complete spike-sorting pipeline workflow (including the operations workflow diagram, detailed steps, and nomnoml schema), please refer to the README.</p>"},{"location":"workflow/#quality-assessment-workflow","title":"Quality Assessment Workflow","text":"<pre><code>blech_unit_characteristics.py \u2192 blech_data_summary.py \u2192 grade_dataset.py\n</code></pre>"},{"location":"workflow/#additional-workflow-details","title":"Additional Workflow Details","text":""},{"location":"workflow/#spike-sorting-text-flow","title":"Spike Sorting Text Flow","text":"<pre><code>[blech_exp_info] -&gt; [blech_init]\n[blech_init] -&gt; [blech_common_average_reference]\n[blech_common_average_reference] -&gt; [bash blech_run_process.sh]\n[bash blech_run_process.sh] -&gt; [blech_post_process]\n[blech_post_process] -&gt; [blech_units_plot]\n[blech_units_plot] -&gt; [blech_make_arrays]\n[blech_make_arrays] -&gt; [bash blech_run_QA.sh]\n[bash blech_run_QA.sh] -&gt; [blech_unit_characteristics]\n[blech_unit_characteristics] -&gt; [blech_data_summary]\n[blech_data_summary] -&gt; [grade_dataset]\n</code></pre>"},{"location":"workflow/#emg-analysis-workflows","title":"EMG Analysis Workflows","text":"<p>Shared Steps:</p> <pre><code>[blech_init] -&gt; [blech_make_arrays]\n[blech_make_arrays] -&gt; [emg_filter]\n</code></pre> <p>BSA/STFT Branch:</p> <pre><code>[emg_filter] -&gt; [emg_freq_setup]\n[emg_freq_setup] -&gt; [bash blech_emg_jetstream_parallel.sh]\n[bash blech_emg_jetstream_parallel.sh] -&gt; [emg_freq_post_process]\n[emg_freq_post_process] -&gt; [emg_freq_plot]\n</code></pre> <p>QDA Branch (Jenn Li):</p> <pre><code>[emg_freq_setup] -&gt; [get_gapes_Li]\n</code></pre>"},{"location":"workflow/#using-the-diagrams","title":"Using the Diagrams","text":""},{"location":"workflow/#viewing-the-workflow","title":"Viewing the Workflow","text":"<p>The operations workflow visual (available in the README) provides a high-level overview of the entire pipeline, showing how different components interact.</p>"},{"location":"workflow/#generating-custom-diagrams","title":"Generating Custom Diagrams","text":"<ol> <li>Visit nomnoml.com</li> <li>Copy the nomnoml schema code from the README</li> <li>Paste it into the editor</li> <li>Modify as needed for your specific use case</li> <li>Export as PNG or SVG</li> </ol>"},{"location":"workflow/#understanding-the-flow","title":"Understanding the Flow","text":"<ul> <li>Spike Sorting: The main pipeline processes raw Intan data through clustering and quality assessment</li> <li>EMG Analysis: Two parallel branches for different analysis approaches</li> <li>BSA/STFT: Frequency-based analysis using Bayesian methods</li> <li>QDA: Gape detection using quadratic discriminant analysis</li> </ul>"},{"location":"workflow/#see-also","title":"See Also","text":"<ul> <li>Main Spike-Sorting Pipeline (README)</li> <li>Core Pipeline Documentation</li> <li>Tutorials</li> <li>Getting Started</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing blech_clust, ensure you have:</p> <ul> <li>Conda/Miniconda: Required for environment management</li> <li>Git: For cloning repositories</li> <li>System packages: GNU parallel (optional, for parallel processing)</li> </ul>"},{"location":"getting-started/installation/#quick-start-recommended","title":"Quick Start (Recommended)","text":"<ol> <li> <p>Clone the repository: <pre><code>git clone https://github.com/katzlabbrandeis/blech_clust.git\ncd blech_clust\n</code></pre></p> </li> <li> <p>Install everything: <pre><code>make all\n</code></pre>    This installs the base environment, EMG analysis tools, neuRecommend classifier, BlechRNN, and all optional dependencies.</p> </li> <li> <p>Activate the environment: <pre><code>conda activate blech_clust\n</code></pre></p> </li> </ol>"},{"location":"getting-started/installation/#custom-installation","title":"Custom Installation","text":"<p>For more control over what gets installed:</p> <pre><code># Core spike sorting functionality only\nmake core\n\n# Or install components individually:\nmake base      # Base environment and core dependencies (required)\nmake emg       # EMG analysis requirements (BSA/STFT, QDA)\nmake neurec    # neuRecommend waveform classifier\nmake blechrnn  # BlechRNN for firing rate estimation\nmake prefect   # Prefect workflow management (for testing)\nmake dev       # Development dependencies\nmake optional  # Optional analysis tools\n</code></pre>"},{"location":"getting-started/installation/#parameter-setup","title":"Parameter Setup","text":"<p>After installation, set up parameter templates:</p> <pre><code># Copy parameter templates (if none exist)\nmake params\n\n# Edit the parameter files according to your experimental setup\n</code></pre> <p>See the Getting Started wiki for detailed parameter configuration.</p>"},{"location":"getting-started/installation/#gpu-support-optional","title":"GPU Support (Optional)","text":"<p>If you plan to use GPU acceleration with BlechRNN:</p> <ol> <li>Install CUDA toolkit separately (see PyTorch GPU installation guide)</li> <li>Reinstall PyTorch with GPU support:    <pre><code>conda activate blech_clust\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n</code></pre></li> </ol>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":"<ul> <li> <p>Clean installation: If you encounter issues, remove the environment and start fresh:   <pre><code>make clean\nmake all\n</code></pre></p> </li> <li> <p>Partial installation: If a component fails, you can retry individual components:   <pre><code>make base    # Retry base installation\nmake emg     # Retry EMG components\n</code></pre></p> </li> <li> <p>Environment activation: Always activate the environment before running scripts:   <pre><code>conda activate blech_clust\n</code></pre></p> </li> </ul>"},{"location":"getting-started/installation/#test-dataset","title":"Test Dataset","text":"<p>A test dataset is available to verify your installation:</p> <p>Test Dataset on Google Drive</p>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the API Reference to understand available functions</li> <li>Read the Tutorials for step-by-step guides</li> <li>Check out the Blog for insights and updates</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":""},{"location":"getting-started/quickstart/#first-steps","title":"First Steps","text":"<p>Once installed, you can start processing your data:</p> <ol> <li>Prepare your data: Ensure you have Intan RHD2132 recordings</li> <li>Set up experiment info: Run <code>python blech_exp_info.py</code> to annotate channels</li> <li>Configure parameters: Edit the parameter files in the <code>params/</code> directory</li> <li>Run the pipeline: Use the convenience scripts or run individual modules</li> </ol>"},{"location":"getting-started/quickstart/#basic-workflow","title":"Basic Workflow","text":""},{"location":"getting-started/quickstart/#1-experiment-setup","title":"1. Experiment Setup","text":"<pre><code># Activate the environment\nconda activate blech_clust\n\n# Navigate to your data directory\ncd /path/to/your/data\n\n# Run experiment info setup\npython /path/to/blech_clust/blech_exp_info.py\n</code></pre> <p>This will guide you through annotating your channels and setting up experimental parameters.</p>"},{"location":"getting-started/quickstart/#2-clustering-setup","title":"2. Clustering Setup","text":"<pre><code># Run clustering setup\npython /path/to/blech_clust/blech_clust.py\n</code></pre> <p>This creates the necessary directory structure and parameter files.</p>"},{"location":"getting-started/quickstart/#3-common-average-referencing","title":"3. Common Average Referencing","text":"<pre><code># Perform common average referencing\npython /path/to/blech_clust/blech_common_avg_reference.py\n</code></pre>"},{"location":"getting-started/quickstart/#4-spike-extraction-and-clustering","title":"4. Spike Extraction and Clustering","text":"<pre><code># Run parallel processing\nbash /path/to/blech_clust/blech_run_process.sh\n</code></pre> <p>This runs spike extraction and clustering in parallel across electrodes.</p>"},{"location":"getting-started/quickstart/#5-post-processing","title":"5. Post-Processing","text":"<pre><code># Add selected units to HDF5\npython /path/to/blech_clust/blech_post_process.py\n\n# Plot waveforms\npython /path/to/blech_clust/blech_units_plot.py\n\n# Generate spike arrays\npython /path/to/blech_clust/blech_make_arrays.py\n</code></pre>"},{"location":"getting-started/quickstart/#6-quality-assessment","title":"6. Quality Assessment","text":"<pre><code># Run QA checks\nbash /path/to/blech_clust/blech_run_QA.sh\n\n# Analyze unit characteristics\npython /path/to/blech_clust/blech_units_characteristics.py\n\n# Generate data summary\npython /path/to/blech_clust/blech_data_summary.py\n\n# Grade dataset quality\npython /path/to/blech_clust/grade_dataset.py\n</code></pre>"},{"location":"getting-started/quickstart/#emg-analysis","title":"EMG Analysis","text":"<p>If you have EMG data, you can run the EMG analysis pipeline:</p> <pre><code># Filter EMG signals\npython /path/to/blech_clust/emg/emg_filter.py\n\n# Choose your analysis approach:\n\n# Option 1: BSA/STFT analysis\npython /path/to/blech_clust/emg/emg_BSA_STFT.py\n\n# Option 2: QDA-based gape detection\npython /path/to/blech_clust/emg/emg_QDA.py\n</code></pre>"},{"location":"getting-started/quickstart/#parameter-configuration","title":"Parameter Configuration","text":"<p>Key parameter files to configure:</p> <ul> <li>clustering_params.json: Clustering algorithm parameters</li> <li>spike_detection_params.json: Spike detection thresholds</li> <li>emg_params.json: EMG analysis parameters (if using EMG)</li> </ul> <p>See the Getting Started wiki for detailed parameter descriptions.</p>"},{"location":"getting-started/quickstart/#tips","title":"Tips","text":"<ul> <li>Always activate the conda environment before running scripts</li> <li>Check log files in the output directories for debugging</li> <li>Use the test dataset to verify your installation</li> <li>Refer to the API Reference for detailed function documentation</li> </ul>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the Tutorials for detailed walkthroughs</li> <li>Check the API Reference for function details</li> <li>Visit the Blog for tips and updates</li> </ul>"},{"location":"reference/","title":"API Reference","text":"<p>This section contains the API documentation for blech_clust modules.</p>"},{"location":"reference/#overview","title":"Overview","text":"<p>The blech_clust codebase is organized into several key modules:</p> <ul> <li>Core Pipeline - Main spike sorting pipeline modules</li> <li>Utilities - Helper functions and utility classes</li> <li>Ephys Data - Electrophysiology data analysis tools</li> <li>QA Tools - Quality assurance and validation tools</li> <li>EMG Analysis - EMG signal processing and analysis</li> </ul>"},{"location":"reference/#module-organization","title":"Module Organization","text":""},{"location":"reference/#core-pipeline-modules","title":"Core Pipeline Modules","text":"<p>Located in the repository root, these modules form the main spike sorting pipeline:</p> <ul> <li><code>blech_exp_info.py</code> - Experiment setup and metadata</li> <li><code>blech_clust.py</code> - Clustering configuration</li> <li><code>blech_common_avg_reference.py</code> - Common average referencing</li> <li><code>blech_process.py</code> - Spike extraction and clustering</li> <li><code>blech_post_process.py</code> - Post-processing and unit selection</li> <li><code>blech_units_plot.py</code> - Waveform visualization</li> <li><code>blech_make_arrays.py</code> - Spike train array generation</li> </ul>"},{"location":"reference/#utility-modules","title":"Utility Modules","text":"<p>Located in <code>utils/</code>, these provide supporting functionality:</p> <ul> <li><code>blech_utils.py</code> - Core utility functions</li> <li><code>clustering/</code> - Clustering algorithms</li> <li><code>ephys_data/</code> - Electrophysiology data handling</li> <li><code>qa_utils/</code> - Quality assurance tools</li> </ul>"},{"location":"reference/#emg-modules","title":"EMG Modules","text":"<p>Located in <code>emg/</code>, these handle EMG signal analysis:</p> <ul> <li><code>emg_filter.py</code> - EMG signal filtering</li> <li><code>emg_freq_setup.py</code> - Frequency analysis setup</li> <li><code>get_gapes_Li.py</code> - Gape detection using QDA</li> </ul>"},{"location":"reference/#using-the-api","title":"Using the API","text":""},{"location":"reference/#importing-modules","title":"Importing Modules","text":"<pre><code># Import utility functions\nfrom blech_utils import Tee, path_handler, imp_metadata\n\n# Import ephys data tools\nfrom utils.ephys_data import ephys_data\n\n# Import clustering utilities\nfrom utils.clustering import clustering\n</code></pre>"},{"location":"reference/#example-usage","title":"Example Usage","text":"<pre><code># Load experimental data\nfrom utils.ephys_data import ephys_data\n\n# Create data handler\ndata = ephys_data('/path/to/data')\n\n# Access spike trains\nspike_trains = data.spikes\n\n# Get unit information\nunits = data.units\n</code></pre>"},{"location":"reference/#documentation-format","title":"Documentation Format","text":"<p>Each module page includes:</p> <ul> <li>Overview - Module purpose and functionality</li> <li>Key Functions/Classes - Main components with descriptions</li> <li>Usage Examples - Code examples demonstrating usage</li> <li>Parameters - Detailed parameter descriptions</li> <li>Returns - Return value descriptions</li> </ul>"},{"location":"reference/#contributing","title":"Contributing","text":"<p>To improve the API documentation:</p> <ol> <li>Update docstrings in the source code following NumPy format</li> <li>Submit a pull request with your changes</li> <li>Documentation will be automatically rebuilt</li> </ol> <p>See CONTRIBUTING.md for guidelines.</p>"},{"location":"reference/core-pipeline/","title":"Core Pipeline","text":"<p>The core pipeline modules handle the main spike sorting workflow from raw data to sorted units.</p>"},{"location":"reference/core-pipeline/#pipeline-modules","title":"Pipeline Modules","text":""},{"location":"reference/core-pipeline/#blech_exp_infopy","title":"blech_exp_info.py","text":"<p>Pre-clustering step to annotate channels and save experimental parameters.</p> <p>Key Functions:</p> <ul> <li>Electrode layout configuration</li> <li>Digital input selection</li> <li>Taste/stimulus naming</li> <li>Palatability ranking</li> <li>Laser configuration</li> </ul> <p>Usage:</p> <pre><code>python blech_exp_info.py /path/to/data\n</code></pre>"},{"location":"reference/core-pipeline/#blech_clustpy","title":"blech_clust.py","text":"<p>Setup directories and define clustering parameters.</p> <p>Key Functions:</p> <ul> <li>Directory structure creation</li> <li>Parameter file initialization</li> <li>Clustering configuration</li> </ul> <p>Usage:</p> <pre><code>python blech_clust.py\n</code></pre>"},{"location":"reference/core-pipeline/#blech_common_avg_referencepy","title":"blech_common_avg_reference.py","text":"<p>Perform common average referencing on electrode data.</p> <p>Key Functions:</p> <ul> <li>CAR group processing</li> <li>Signal referencing</li> <li>Artifact reduction</li> </ul> <p>Usage:</p> <pre><code>python blech_common_avg_reference.py\n</code></pre>"},{"location":"reference/core-pipeline/#blech_processpy","title":"blech_process.py","text":"<p>Core spike extraction and clustering module.</p> <p>Key Functions:</p> <ul> <li>Spike detection</li> <li>Feature extraction</li> <li>Clustering algorithms</li> <li>UMAP dimensionality reduction</li> </ul> <p>Usage:</p> <pre><code># Usually called via blech_run_process.sh for parallel execution\npython blech_process.py &lt;electrode_number&gt;\n</code></pre>"},{"location":"reference/core-pipeline/#blech_post_processpy","title":"blech_post_process.py","text":"<p>Add selected units to HDF5 file after manual curation.</p> <p>Key Functions:</p> <ul> <li>Unit selection</li> <li>HDF5 file updates</li> <li>Metadata management</li> </ul> <p>Usage:</p> <pre><code>python blech_post_process.py\n</code></pre>"},{"location":"reference/core-pipeline/#blech_units_plotpy","title":"blech_units_plot.py","text":"<p>Plot waveforms of selected spikes for visualization.</p> <p>Key Functions:</p> <ul> <li>Waveform plotting</li> <li>Unit visualization</li> <li>Quality metrics display</li> </ul> <p>Usage:</p> <pre><code>python blech_units_plot.py\n</code></pre>"},{"location":"reference/core-pipeline/#blech_make_arrayspy","title":"blech_make_arrays.py","text":"<p>Generate spike-train arrays for analysis.</p> <p>Key Functions:</p> <ul> <li>Spike train generation</li> <li>Trial alignment</li> <li>Array formatting</li> </ul> <p>Usage:</p> <pre><code>python blech_make_arrays.py\n</code></pre>"},{"location":"reference/core-pipeline/#pipeline-flow","title":"Pipeline Flow","text":"<pre><code>Raw Data\n    \u2193\nblech_exp_info.py (Setup)\n    \u2193\nblech_clust.py (Configuration)\n    \u2193\nblech_common_avg_reference.py (Referencing)\n    \u2193\nblech_run_process.sh (Parallel Processing)\n    \u2193\nblech_post_process.py (Unit Selection)\n    \u2193\nblech_units_plot.py (Visualization)\n    \u2193\nblech_make_arrays.py (Array Generation)\n    \u2193\nSorted Units\n</code></pre>"},{"location":"reference/core-pipeline/#configuration-files","title":"Configuration Files","text":"<p>The pipeline uses several JSON configuration files:</p> <ul> <li><code>sorting_params.json</code> - Clustering parameters</li> <li><code>spike_detection_params.json</code> - Detection thresholds</li> <li><code>waveform_classifier_params.json</code> - Classifier settings</li> </ul>"},{"location":"reference/core-pipeline/#see-also","title":"See Also","text":"<ul> <li>Getting Started</li> <li>Tutorials</li> <li>Utilities</li> </ul>"},{"location":"reference/emg-analysis/","title":"EMG Analysis","text":"<p>Tools for analyzing electromyography (EMG) signals, including frequency analysis and gape detection.</p>"},{"location":"reference/emg-analysis/#overview","title":"Overview","text":"<p>The EMG analysis pipeline provides two main approaches:</p> <ol> <li>BSA/STFT: Bayesian Spectrum Analysis and Short-Time Fourier Transform for frequency analysis</li> <li>QDA: Quadratic Discriminant Analysis for gape detection</li> </ol>"},{"location":"reference/emg-analysis/#shared-setup","title":"Shared Setup","text":""},{"location":"reference/emg-analysis/#emg_filterpy","title":"emg_filter.py","text":"<p>Filter EMG signals before analysis.</p> <p>Usage:</p> <pre><code>python emg/emg_filter.py\n</code></pre> <p>Filtering Steps:</p> <ol> <li>Bandpass filtering (typically 300-3000 Hz)</li> <li>Notch filtering (remove line noise at 60 Hz)</li> <li>Rectification</li> <li>Smoothing</li> </ol> <p>Output:</p> <ul> <li>Filtered EMG signals saved to HDF5 file</li> <li>Filter parameters logged</li> </ul>"},{"location":"reference/emg-analysis/#bsastft-branch","title":"BSA/STFT Branch","text":"<p>Frequency-based analysis of EMG signals.</p>"},{"location":"reference/emg-analysis/#emg_freq_setuppy","title":"emg_freq_setup.py","text":"<p>Configure parameters for frequency analysis.</p> <p>Usage:</p> <pre><code>python emg/emg_freq_setup.py\n</code></pre> <p>Parameters:</p> <ul> <li>Frequency bands of interest</li> <li>Window sizes</li> <li>Overlap parameters</li> <li>Output directories</li> </ul>"},{"location":"reference/emg-analysis/#parallel-processing","title":"Parallel Processing","text":"<p>Run frequency analysis in parallel:</p> <pre><code>bash blech_emg_jetstream_parallel.sh\n</code></pre> <p>This script:</p> <ol> <li>Divides trials across processors</li> <li>Runs BSA/STFT on each subset</li> <li>Saves intermediate results</li> </ol>"},{"location":"reference/emg-analysis/#emg_freq_post_processpy","title":"emg_freq_post_process.py","text":"<p>Aggregate and process frequency analysis results.</p> <p>Usage:</p> <pre><code>python emg/emg_freq_post_process.py\n</code></pre> <p>Processing Steps:</p> <ol> <li>Combine results from parallel jobs</li> <li>Normalize power spectra</li> <li>Calculate summary statistics</li> <li>Identify significant frequency changes</li> </ol>"},{"location":"reference/emg-analysis/#emg_freq_plotpy","title":"emg_freq_plot.py","text":"<p>Generate visualizations of frequency analysis.</p> <p>Usage:</p> <pre><code>python emg/emg_freq_plot.py\n</code></pre> <p>Plots Generated:</p> <ul> <li>Spectrograms</li> <li>Power spectrum time courses</li> <li>Frequency band comparisons</li> <li>Trial-averaged responses</li> </ul>"},{"location":"reference/emg-analysis/#qda-branch","title":"QDA Branch","text":"<p>Gape detection using Quadratic Discriminant Analysis.</p>"},{"location":"reference/emg-analysis/#get_gapes_lipy","title":"get_gapes_Li.py","text":"<p>Detect gapes using QDA classifier based on Li et al.'s methodology.</p> <p>Usage:</p> <pre><code>python emg/get_gapes_Li.py\n</code></pre> <p>Method:</p> <ol> <li>Extract EMG features</li> <li>Train QDA classifier on labeled data</li> <li>Predict gape events in test data</li> <li>Validate predictions</li> </ol> <p>Features Used:</p> <ul> <li>EMG amplitude</li> <li>Frequency content</li> <li>Temporal patterns</li> <li>Derivative features</li> </ul> <p>Output:</p> <ul> <li>Gape onset times</li> <li>Gape durations</li> <li>Confidence scores</li> <li>Validation metrics</li> </ul>"},{"location":"reference/emg-analysis/#emg-data-structure","title":"EMG Data Structure","text":""},{"location":"reference/emg-analysis/#hdf5-organization","title":"HDF5 Organization","text":"<pre><code>data.h5\n\u251c\u2500\u2500 emg/\n\u2502   \u251c\u2500\u2500 raw/              # Raw EMG signals\n\u2502   \u251c\u2500\u2500 filtered/         # Filtered EMG signals\n\u2502   \u251c\u2500\u2500 frequency/        # Frequency analysis results\n\u2502   \u2502   \u251c\u2500\u2500 power/\n\u2502   \u2502   \u251c\u2500\u2500 phase/\n\u2502   \u2502   \u2514\u2500\u2500 coherence/\n\u2502   \u2514\u2500\u2500 gapes/            # Detected gape events\n\u2502       \u251c\u2500\u2500 onset_times/\n\u2502       \u251c\u2500\u2500 durations/\n\u2502       \u2514\u2500\u2500 confidence/\n</code></pre>"},{"location":"reference/emg-analysis/#configuration","title":"Configuration","text":""},{"location":"reference/emg-analysis/#emg_paramsjson","title":"emg_params.json","text":"<p>EMG analysis parameters:</p> <pre><code>{\n  \"filter\": {\n    \"bandpass\": [300, 3000],\n    \"notch\": 60,\n    \"order\": 4\n  },\n  \"frequency\": {\n    \"bands\": {\n      \"low\": [300, 1000],\n      \"mid\": [1000, 2000],\n      \"high\": [2000, 3000]\n    },\n    \"window_size\": 100,\n    \"overlap\": 50\n  },\n  \"gape_detection\": {\n    \"features\": [\"amplitude\", \"frequency\", \"derivative\"],\n    \"classifier\": \"qda\",\n    \"validation_split\": 0.2\n  }\n}\n</code></pre>"},{"location":"reference/emg-analysis/#usage-examples","title":"Usage Examples","text":""},{"location":"reference/emg-analysis/#complete-bsastft-workflow","title":"Complete BSA/STFT Workflow","text":"<pre><code># Filter EMG signals\npython emg/emg_filter.py\n\n# Setup frequency analysis\npython emg/emg_freq_setup.py\n\n# Run parallel analysis\nbash blech_emg_jetstream_parallel.sh\n\n# Post-process results\npython emg/emg_freq_post_process.py\n\n# Generate plots\npython emg/emg_freq_plot.py\n</code></pre>"},{"location":"reference/emg-analysis/#complete-qda-workflow","title":"Complete QDA Workflow","text":"<pre><code># Filter EMG signals\npython emg/emg_filter.py\n\n# Setup gape detection\npython emg/emg_freq_setup.py\n\n# Detect gapes\npython emg/get_gapes_Li.py\n</code></pre>"},{"location":"reference/emg-analysis/#programmatic-access","title":"Programmatic Access","text":"<pre><code>import tables\nimport numpy as np\n\n# Load filtered EMG data\nwith tables.open_file('data.h5', 'r') as hf5:\n    emg_filtered = hf5.root.emg.filtered[:]\n\n# Load gape events\nwith tables.open_file('data.h5', 'r') as hf5:\n    gape_times = hf5.root.emg.gapes.onset_times[:]\n    gape_durations = hf5.root.emg.gapes.durations[:]\n\n# Analyze gape timing\nprint(f\"Detected {len(gape_times)} gapes\")\nprint(f\"Mean duration: {np.mean(gape_durations):.2f} ms\")\n</code></pre>"},{"location":"reference/emg-analysis/#analysis-tips","title":"Analysis Tips","text":""},{"location":"reference/emg-analysis/#frequency-analysis","title":"Frequency Analysis","text":"<ul> <li>Use appropriate frequency bands for your species/preparation</li> <li>Adjust window size based on temporal resolution needs</li> <li>Consider trial-to-trial variability</li> </ul>"},{"location":"reference/emg-analysis/#gape-detection","title":"Gape Detection","text":"<ul> <li>Manually label training data carefully</li> <li>Validate classifier performance on held-out data</li> <li>Adjust confidence threshold based on false positive/negative trade-off</li> </ul>"},{"location":"reference/emg-analysis/#quality-control","title":"Quality Control","text":"<ul> <li>Inspect filtered signals visually</li> <li>Check for artifacts</li> <li>Validate detected events manually for subset of trials</li> </ul>"},{"location":"reference/emg-analysis/#references","title":"References","text":"<p>Li, J. X., et al. (2016). \"Gape detection using quadratic discriminant analysis.\" Journal of Neurophysiology, 116(4), 1748-1763.</p>"},{"location":"reference/emg-analysis/#see-also","title":"See Also","text":"<ul> <li>Core Pipeline</li> <li>Utilities</li> <li>Tutorials</li> </ul>"},{"location":"reference/ephys-data/","title":"Ephys Data","text":"<p>The <code>ephys_data</code> module provides comprehensive tools for handling electrophysiology data.</p>"},{"location":"reference/ephys-data/#overview","title":"Overview","text":"<p>Located in <code>utils/ephys_data/</code>, this module handles:</p> <ul> <li>Data loading and management</li> <li>Spike train access</li> <li>Unit information</li> <li>Trial alignment</li> <li>Metadata handling</li> </ul>"},{"location":"reference/ephys-data/#ephys_data-class","title":"ephys_data Class","text":"<p>Main class for accessing electrophysiology data.</p>"},{"location":"reference/ephys-data/#initialization","title":"Initialization","text":"<pre><code>from utils.ephys_data import ephys_data\n\n# Create data handler\ndata = ephys_data('/path/to/data')\n</code></pre>"},{"location":"reference/ephys-data/#key-attributes","title":"Key Attributes","text":""},{"location":"reference/ephys-data/#spike-data","title":"Spike Data","text":"<pre><code># Access spike trains\nspike_trains = data.spikes  # All spike trains\nunit_spikes = data.spikes[unit_num]  # Specific unit\n\n# Get spike times\nspike_times = data.spike_times\n</code></pre>"},{"location":"reference/ephys-data/#unit-information","title":"Unit Information","text":"<pre><code># Get unit descriptors\nunits = data.units\n\n# Get number of units\nn_units = data.n_units\n\n# Get unit types\nunit_types = data.unit_types\n</code></pre>"},{"location":"reference/ephys-data/#trial-data","title":"Trial Data","text":"<pre><code># Get trial information\ntrials = data.trials\n\n# Get stimulus information\ndig_in_trials = data.dig_in_trials\n\n# Get trial times\ntrial_times = data.trial_times\n</code></pre>"},{"location":"reference/ephys-data/#metadata","title":"Metadata","text":"<pre><code># Access experimental metadata\nmetadata = data.metadata\n\n# Get taste names\ntaste_names = data.taste_names\n\n# Get sampling rate\nsampling_rate = data.sampling_rate\n</code></pre>"},{"location":"reference/ephys-data/#key-methods","title":"Key Methods","text":""},{"location":"reference/ephys-data/#data-loading","title":"Data Loading","text":"<pre><code># Load spike data\ndata.load_spikes()\n\n# Load unit information\ndata.load_units()\n\n# Load trial data\ndata.load_trials()\n</code></pre>"},{"location":"reference/ephys-data/#data-filtering","title":"Data Filtering","text":"<pre><code># Filter by unit type\ngood_units = data.get_units_by_type('single')\n\n# Filter by quality metrics\nhigh_quality = data.get_units_by_quality(threshold=0.8)\n</code></pre>"},{"location":"reference/ephys-data/#trial-alignment","title":"Trial Alignment","text":"<pre><code># Get aligned spike trains\naligned_spikes = data.get_aligned_spikes(\n    unit_num=0,\n    dig_in=0,\n    time_window=[-1000, 2000]\n)\n</code></pre>"},{"location":"reference/ephys-data/#data-structure","title":"Data Structure","text":""},{"location":"reference/ephys-data/#hdf5-file-organization","title":"HDF5 File Organization","text":"<pre><code>data.h5\n\u251c\u2500\u2500 spike_times/          # Spike times for each unit\n\u251c\u2500\u2500 spike_waveforms/      # Spike waveforms\n\u251c\u2500\u2500 unit_descriptor/      # Unit metadata\n\u251c\u2500\u2500 digital_in/           # Digital input data\n\u2502   \u251c\u2500\u2500 dig_in_0/\n\u2502   \u251c\u2500\u2500 dig_in_1/\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 metadata/             # Experimental metadata\n</code></pre>"},{"location":"reference/ephys-data/#spike-train-format","title":"Spike Train Format","text":"<p>Spike trains are stored as arrays of spike times in milliseconds:</p> <pre><code># Example spike train\nspike_train = [125.5, 342.1, 567.8, 891.2, ...]\n</code></pre>"},{"location":"reference/ephys-data/#unit-descriptor-format","title":"Unit Descriptor Format","text":"<p>Unit information includes:</p> <ul> <li>Unit number</li> <li>Electrode number</li> <li>Single/multi-unit classification</li> <li>Quality metrics</li> <li>Waveform statistics</li> </ul>"},{"location":"reference/ephys-data/#usage-examples","title":"Usage Examples","text":""},{"location":"reference/ephys-data/#basic-data-access","title":"Basic Data Access","text":"<pre><code>from utils.ephys_data import ephys_data\n\n# Load data\ndata = ephys_data('/path/to/data')\n\n# Get all spike trains\nall_spikes = data.spikes\n\n# Get specific unit\nunit_0_spikes = data.spikes[0]\n\n# Get unit information\nunits = data.units\nprint(f\"Found {len(units)} units\")\n</code></pre>"},{"location":"reference/ephys-data/#trial-aligned-analysis","title":"Trial-Aligned Analysis","text":"<pre><code># Get spikes aligned to stimulus onset\naligned_spikes = data.get_aligned_spikes(\n    unit_num=0,\n    dig_in=0,\n    time_window=[-1000, 2000]  # -1s to +2s around stimulus\n)\n\n# Calculate firing rate\nimport numpy as np\nbin_size = 50  # ms\nbins = np.arange(-1000, 2000, bin_size)\nfiring_rate, _ = np.histogram(aligned_spikes, bins=bins)\nfiring_rate = firing_rate / (bin_size / 1000)  # Convert to Hz\n</code></pre>"},{"location":"reference/ephys-data/#quality-filtering","title":"Quality Filtering","text":"<pre><code># Get high-quality single units\ngood_units = [\n    unit for unit in data.units\n    if unit['single_unit'] and unit['quality'] &gt; 0.8\n]\n\nprint(f\"Found {len(good_units)} high-quality single units\")\n</code></pre>"},{"location":"reference/ephys-data/#advanced-features","title":"Advanced Features","text":""},{"location":"reference/ephys-data/#custom-data-loading","title":"Custom Data Loading","text":"<pre><code># Load specific data subsets\ndata.load_spikes(units=[0, 1, 2])  # Load only specific units\ndata.load_trials(dig_ins=[0, 1])   # Load only specific trials\n</code></pre>"},{"location":"reference/ephys-data/#data-export","title":"Data Export","text":"<pre><code># Export to common formats\ndata.export_to_nwb('/path/to/output.nwb')  # Neurodata Without Borders\ndata.export_to_mat('/path/to/output.mat')  # MATLAB format\n</code></pre>"},{"location":"reference/ephys-data/#see-also","title":"See Also","text":"<ul> <li>Core Pipeline</li> <li>Utilities</li> <li>Tutorials</li> <li>Module Documentation</li> </ul>"},{"location":"reference/qa-tools/","title":"QA Tools","text":"<p>Quality assurance tools for validating and grading electrophysiology datasets.</p>"},{"location":"reference/qa-tools/#overview","title":"Overview","text":"<p>The QA tools provide automated assessment of dataset quality based on multiple metrics:</p> <ul> <li>Unit quality metrics</li> <li>Waveform characteristics</li> <li>Firing rate statistics</li> <li>Inter-spike interval analysis</li> <li>Cluster separation</li> </ul>"},{"location":"reference/qa-tools/#blech_run_qash","title":"blech_run_QA.sh","text":"<p>Main QA script that runs comprehensive quality checks.</p>"},{"location":"reference/qa-tools/#usage","title":"Usage","text":"<pre><code>bash blech_run_QA.sh\n</code></pre>"},{"location":"reference/qa-tools/#checks-performed","title":"Checks Performed","text":"<ol> <li>Waveform Quality</li> <li>Signal-to-noise ratio</li> <li>Waveform consistency</li> <li> <p>Peak-to-trough ratio</p> </li> <li> <p>Firing Statistics</p> </li> <li>Mean firing rate</li> <li>Coefficient of variation</li> <li> <p>Burst analysis</p> </li> <li> <p>Cluster Quality</p> </li> <li>Isolation distance</li> <li>L-ratio</li> <li> <p>Silhouette score</p> </li> <li> <p>ISI Violations</p> </li> <li>Refractory period violations</li> <li>ISI distribution analysis</li> </ol>"},{"location":"reference/qa-tools/#blech_units_characteristicspy","title":"blech_units_characteristics.py","text":"<p>Analyze and compute unit characteristics.</p>"},{"location":"reference/qa-tools/#usage_1","title":"Usage","text":"<pre><code>python blech_units_characteristics.py\n</code></pre>"},{"location":"reference/qa-tools/#computed-metrics","title":"Computed Metrics","text":""},{"location":"reference/qa-tools/#waveform-metrics","title":"Waveform Metrics","text":"<ul> <li>Peak amplitude: Maximum waveform amplitude</li> <li>Trough amplitude: Minimum waveform amplitude</li> <li>Peak-to-trough time: Time between peak and trough</li> <li>Half-width: Waveform width at half-maximum</li> </ul>"},{"location":"reference/qa-tools/#firing-metrics","title":"Firing Metrics","text":"<ul> <li>Mean firing rate: Average spikes per second</li> <li>CV: Coefficient of variation of ISIs</li> <li>Fano factor: Variance-to-mean ratio</li> <li>Burst index: Proportion of spikes in bursts</li> </ul>"},{"location":"reference/qa-tools/#quality-metrics","title":"Quality Metrics","text":"<ul> <li>SNR: Signal-to-noise ratio</li> <li>Isolation distance: Cluster separation metric</li> <li>L-ratio: Cluster quality metric</li> <li>ISI violations: Refractory period violations</li> </ul>"},{"location":"reference/qa-tools/#output","title":"Output","text":"<p>Creates a CSV file with unit characteristics:</p> <pre><code>unit_num,electrode,single_unit,snr,firing_rate,cv,isolation_distance,...\n0,1,True,8.5,12.3,0.45,25.6,...\n1,1,False,4.2,8.7,0.62,12.3,...\n...\n</code></pre>"},{"location":"reference/qa-tools/#blech_data_summarypy","title":"blech_data_summary.py","text":"<p>Generate comprehensive dataset summary.</p>"},{"location":"reference/qa-tools/#usage_2","title":"Usage","text":"<pre><code>python utils/blech_data_summary.py\n</code></pre>"},{"location":"reference/qa-tools/#summary-contents","title":"Summary Contents","text":"<ol> <li>Dataset Overview</li> <li>Number of electrodes</li> <li>Number of units</li> <li>Recording duration</li> <li> <p>Number of trials</p> </li> <li> <p>Unit Statistics</p> </li> <li>Single vs. multi-unit counts</li> <li>Quality distribution</li> <li> <p>Firing rate distribution</p> </li> <li> <p>Trial Information</p> </li> <li>Stimuli presented</li> <li>Trial counts per stimulus</li> <li> <p>Trial timing</p> </li> <li> <p>Quality Metrics</p> </li> <li>Overall dataset quality score</li> <li>Per-electrode quality</li> <li>Recommended units for analysis</li> </ol>"},{"location":"reference/qa-tools/#output_1","title":"Output","text":"<p>Creates a summary report:</p> <pre><code>Dataset Summary\n===============\nData Directory: /path/to/data\nRecording Date: 2024-01-15\n\nUnits:\n  Total: 45\n  Single Units: 32\n  Multi Units: 13\n  Mean Quality: 0.78\n\nTrials:\n  Total: 120\n  Stimuli: 4\n  Trials per Stimulus: 30\n\nQuality Score: 8.5/10\n</code></pre>"},{"location":"reference/qa-tools/#grade_datasetpy","title":"grade_dataset.py","text":"<p>Grade dataset quality based on comprehensive metrics.</p>"},{"location":"reference/qa-tools/#usage_3","title":"Usage","text":"<pre><code>python utils/grade_dataset.py\n</code></pre>"},{"location":"reference/qa-tools/#grading-criteria","title":"Grading Criteria","text":""},{"location":"reference/qa-tools/#a-grade-9-10","title":"A Grade (9-10)","text":"<ul> <li>High unit count (&gt;30 single units)</li> <li>Excellent isolation (&gt;20 isolation distance)</li> <li>Low ISI violations (&lt;1%)</li> <li>High SNR (&gt;8)</li> </ul>"},{"location":"reference/qa-tools/#b-grade-7-8","title":"B Grade (7-8)","text":"<ul> <li>Good unit count (20-30 single units)</li> <li>Good isolation (15-20 isolation distance)</li> <li>Moderate ISI violations (1-2%)</li> <li>Good SNR (6-8)</li> </ul>"},{"location":"reference/qa-tools/#c-grade-5-6","title":"C Grade (5-6)","text":"<ul> <li>Moderate unit count (10-20 single units)</li> <li>Fair isolation (10-15 isolation distance)</li> <li>Some ISI violations (2-5%)</li> <li>Fair SNR (4-6)</li> </ul>"},{"location":"reference/qa-tools/#d-grade-5","title":"D Grade (&lt;5)","text":"<ul> <li>Low unit count (&lt;10 single units)</li> <li>Poor isolation (&lt;10 isolation distance)</li> <li>High ISI violations (&gt;5%)</li> <li>Low SNR (&lt;4)</li> </ul>"},{"location":"reference/qa-tools/#output_2","title":"Output","text":"<p>Creates a grading report:</p> <pre><code>Dataset Grade: B+\n===============\n\nOverall Score: 7.8/10\n\nComponent Scores:\n  Unit Count: 8/10 (28 single units)\n  Isolation Quality: 7/10 (avg 17.5)\n  ISI Violations: 9/10 (0.8% violations)\n  SNR: 7/10 (avg 6.8)\n\nRecommendations:\n  - Good quality dataset suitable for analysis\n  - Consider excluding 3 low-quality units\n  - Electrode 5 shows lower quality, review manually\n</code></pre>"},{"location":"reference/qa-tools/#quality-metrics-reference","title":"Quality Metrics Reference","text":""},{"location":"reference/qa-tools/#signal-to-noise-ratio-snr","title":"Signal-to-Noise Ratio (SNR)","text":"<pre><code>SNR = peak_amplitude / noise_std\n</code></pre> <ul> <li>Good: SNR &gt; 8</li> <li>Fair: SNR 4-8</li> <li>Poor: SNR &lt; 4</li> </ul>"},{"location":"reference/qa-tools/#isolation-distance","title":"Isolation Distance","text":"<p>Mahalanobis distance-based cluster separation metric.</p> <ul> <li>Good: &gt; 20</li> <li>Fair: 10-20</li> <li>Poor: &lt; 10</li> </ul>"},{"location":"reference/qa-tools/#l-ratio","title":"L-ratio","text":"<p>Cluster quality metric based on Mahalanobis distance.</p> <ul> <li>Good: &lt; 0.1</li> <li>Fair: 0.1-0.3</li> <li>Poor: &gt; 0.3</li> </ul>"},{"location":"reference/qa-tools/#isi-violations","title":"ISI Violations","text":"<p>Percentage of spikes violating refractory period (typically 2ms).</p> <ul> <li>Good: &lt; 1%</li> <li>Fair: 1-2%</li> <li>Poor: &gt; 2%</li> </ul>"},{"location":"reference/qa-tools/#usage-examples","title":"Usage Examples","text":""},{"location":"reference/qa-tools/#basic-qa-workflow","title":"Basic QA Workflow","text":"<pre><code># Run comprehensive QA\nbash blech_run_QA.sh\n\n# Analyze unit characteristics\npython blech_units_characteristics.py\n\n# Generate summary\npython utils/blech_data_summary.py\n\n# Grade dataset\npython utils/grade_dataset.py\n</code></pre>"},{"location":"reference/qa-tools/#programmatic-access","title":"Programmatic Access","text":"<pre><code>import pandas as pd\n\n# Load unit characteristics\nunits = pd.read_csv('unit_characteristics.csv')\n\n# Filter high-quality units\ngood_units = units[\n    (units['single_unit'] == True) &amp;\n    (units['snr'] &gt; 6) &amp;\n    (units['isolation_distance'] &gt; 15) &amp;\n    (units['isi_violations'] &lt; 0.02)\n]\n\nprint(f\"Found {len(good_units)} high-quality units\")\n</code></pre>"},{"location":"reference/qa-tools/#see-also","title":"See Also","text":"<ul> <li>Core Pipeline</li> <li>Utilities</li> <li>Tutorials</li> </ul>"},{"location":"reference/utilities/","title":"Utilities","text":"<p>Utility modules provide supporting functionality for the spike sorting pipeline.</p>"},{"location":"reference/utilities/#blech_utils","title":"blech_utils","text":"<p>Core utility functions used throughout the codebase.</p>"},{"location":"reference/utilities/#key-classes-and-functions","title":"Key Classes and Functions","text":""},{"location":"reference/utilities/#tee","title":"Tee","text":"<p>Redirect stdout to both console and file for logging.</p> <pre><code>from blech_utils import Tee\n\n# Redirect output to file and console\nsys.stdout = Tee('/path/to/logfile.txt')\nprint(\"This goes to both console and file\")\n</code></pre>"},{"location":"reference/utilities/#path_handler","title":"path_handler","text":"<p>Handle file paths and directory operations.</p> <pre><code>from blech_utils import path_handler\n\n# Get data directory\ndata_dir = path_handler.get_data_dir()\n\n# Create output directory\npath_handler.make_dir('/path/to/output')\n</code></pre>"},{"location":"reference/utilities/#imp_metadata","title":"imp_metadata","text":"<p>Import and manage experimental metadata.</p> <pre><code>from blech_utils import imp_metadata\n\n# Load metadata\nmetadata = imp_metadata('/path/to/data')\n\n# Access metadata fields\ntastes = metadata['taste_names']\ndig_ins = metadata['dig_in_channels']\n</code></pre>"},{"location":"reference/utilities/#clustering-utilities","title":"Clustering Utilities","text":"<p>Located in <code>utils/clustering/</code>, these modules provide clustering algorithms and tools.</p>"},{"location":"reference/utilities/#key-functions","title":"Key Functions","text":"<ul> <li>Spike clustering algorithms</li> <li>Feature extraction</li> <li>Cluster validation</li> <li>Merge/split operations</li> </ul>"},{"location":"reference/utilities/#data-management","title":"Data Management","text":""},{"location":"reference/utilities/#ephys_data-module","title":"ephys_data Module","text":"<p>Comprehensive data handling for electrophysiology recordings.</p> <p>See Ephys Data for detailed documentation.</p>"},{"location":"reference/utilities/#quality-assurance","title":"Quality Assurance","text":""},{"location":"reference/utilities/#qa_utils-module","title":"qa_utils Module","text":"<p>Tools for dataset quality assessment and validation.</p> <p>See QA Tools for detailed documentation.</p>"},{"location":"reference/utilities/#helper-scripts","title":"Helper Scripts","text":""},{"location":"reference/utilities/#infer_rnn_ratespy","title":"infer_rnn_rates.py","text":"<p>Infer firing rates from spike trains using RNN.</p> <pre><code>python utils/infer_rnn_rates.py &lt;data_dir&gt; [options]\n</code></pre> <p>Options:</p> <ul> <li><code>--train_steps</code>: Number of training steps</li> <li><code>--hidden_size</code>: RNN hidden layer size</li> <li><code>--bin_size</code>: Spike binning size</li> <li><code>--retrain</code>: Force model retraining</li> </ul>"},{"location":"reference/utilities/#blech_data_summarypy","title":"blech_data_summary.py","text":"<p>Generate comprehensive dataset summary.</p> <pre><code>python utils/blech_data_summary.py\n</code></pre>"},{"location":"reference/utilities/#grade_datasetpy","title":"grade_dataset.py","text":"<p>Grade dataset quality based on metrics.</p> <pre><code>python utils/grade_dataset.py\n</code></pre>"},{"location":"reference/utilities/#configuration-management","title":"Configuration Management","text":""},{"location":"reference/utilities/#parameter-files","title":"Parameter Files","text":"<p>Utilities for loading and managing parameter files:</p> <ul> <li>JSON parameter loading</li> <li>Parameter validation</li> <li>Default value handling</li> </ul>"},{"location":"reference/utilities/#example","title":"Example","text":"<pre><code>import json\n\n# Load parameters\nwith open('params/sorting_params.json', 'r') as f:\n    params = json.load(f)\n\n# Access parameters\nmax_clusters = params['max_clusters']\nmin_cluster_size = params['min_cluster_size']\n</code></pre>"},{"location":"reference/utilities/#file-io","title":"File I/O","text":""},{"location":"reference/utilities/#hdf5-operations","title":"HDF5 Operations","text":"<p>Functions for reading and writing HDF5 files:</p> <pre><code>import tables\n\n# Open HDF5 file\nwith tables.open_file('data.h5', 'r') as hf5:\n    # Read spike times\n    spike_times = hf5.root.spike_times[:]\n\n    # Read unit information\n    units = hf5.root.units[:]\n</code></pre>"},{"location":"reference/utilities/#binary-data","title":"Binary Data","text":"<p>Functions for reading Intan binary data:</p> <ul> <li>Amplifier data (<code>.dat</code> files)</li> <li>Digital input data (DIN files)</li> <li>Auxiliary input data</li> </ul>"},{"location":"reference/utilities/#see-also","title":"See Also","text":"<ul> <li>Core Pipeline</li> <li>Ephys Data</li> <li>QA Tools</li> </ul>"}]}