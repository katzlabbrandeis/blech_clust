[
  {
    "objectID": "getting-started.html",
    "href": "getting-started.html",
    "title": "Getting Started",
    "section": "",
    "text": "Before installing blech_clust, ensure you have:\n\nConda/Miniconda: Required for environment management\nGit: For cloning repositories\nSystem packages: GNU parallel (optional, for parallel processing)"
  },
  {
    "objectID": "getting-started.html#prerequisites",
    "href": "getting-started.html#prerequisites",
    "title": "Getting Started",
    "section": "",
    "text": "Before installing blech_clust, ensure you have:\n\nConda/Miniconda: Required for environment management\nGit: For cloning repositories\nSystem packages: GNU parallel (optional, for parallel processing)"
  },
  {
    "objectID": "getting-started.html#quick-start-recommended",
    "href": "getting-started.html#quick-start-recommended",
    "title": "Getting Started",
    "section": "Quick Start (Recommended)",
    "text": "Quick Start (Recommended)\n\nClone the repository:\ngit clone https://github.com/katzlabbrandeis/blech_clust.git\ncd blech_clust\nInstall everything:\nmake all\nThis installs the base environment, EMG analysis tools, neuRecommend classifier, BlechRNN, and all optional dependencies.\nActivate the environment:\nconda activate blech_clust"
  },
  {
    "objectID": "getting-started.html#custom-installation",
    "href": "getting-started.html#custom-installation",
    "title": "Getting Started",
    "section": "Custom Installation",
    "text": "Custom Installation\nFor more control over what gets installed:\n# Core spike sorting functionality only\nmake core\n\n# Or install components individually:\nmake base      # Base environment and core dependencies (required)\nmake emg       # EMG analysis requirements (BSA/STFT, QDA)\nmake neurec    # neuRecommend waveform classifier\nmake blechrnn  # BlechRNN for firing rate estimation\nmake prefect   # Prefect workflow management (for testing)\nmake dev       # Development dependencies\nmake optional  # Optional analysis tools"
  },
  {
    "objectID": "getting-started.html#parameter-setup",
    "href": "getting-started.html#parameter-setup",
    "title": "Getting Started",
    "section": "Parameter Setup",
    "text": "Parameter Setup\nAfter installation, set up parameter templates:\n# Copy parameter templates (if none exist)\nmake params\n\n# Edit the parameter files according to your experimental setup\nSee the Getting Started wiki for detailed parameter configuration."
  },
  {
    "objectID": "getting-started.html#gpu-support-optional",
    "href": "getting-started.html#gpu-support-optional",
    "title": "Getting Started",
    "section": "GPU Support (Optional)",
    "text": "GPU Support (Optional)\nIf you plan to use GPU acceleration with BlechRNN:\n\nInstall CUDA toolkit separately (see PyTorch GPU installation guide)\nReinstall PyTorch with GPU support:\nconda activate blech_clust\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
  },
  {
    "objectID": "getting-started.html#troubleshooting",
    "href": "getting-started.html#troubleshooting",
    "title": "Getting Started",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nClean installation: If you encounter issues, remove the environment and start fresh:\nmake clean\nmake all\nPartial installation: If a component fails, you can retry individual components:\nmake base    # Retry base installation\nmake emg     # Retry EMG components\nEnvironment activation: Always activate the environment before running scripts:\nconda activate blech_clust"
  },
  {
    "objectID": "getting-started.html#first-steps",
    "href": "getting-started.html#first-steps",
    "title": "Getting Started",
    "section": "First Steps",
    "text": "First Steps\nOnce installed, you can start processing your data:\n\nPrepare your data: Ensure you have Intan RHD2132 recordings\nSet up experiment info: Run python blech_exp_info.py to annotate channels\nConfigure parameters: Edit the parameter files in the params/ directory\nRun the pipeline: Use the convenience scripts or run individual modules\n\nFor a detailed walkthrough, see the Tutorials page."
  },
  {
    "objectID": "getting-started.html#test-dataset",
    "href": "getting-started.html#test-dataset",
    "title": "Getting Started",
    "section": "Test Dataset",
    "text": "Test Dataset\nA test dataset is available to verify your installation:\nTest Dataset on Google Drive"
  },
  {
    "objectID": "getting-started.html#next-steps",
    "href": "getting-started.html#next-steps",
    "title": "Getting Started",
    "section": "Next Steps",
    "text": "Next Steps\n\nExplore the API Reference to understand available functions\nRead the Tutorials for step-by-step guides\nCheck out the Blog for insights and updates"
  },
  {
    "objectID": "reference/api-index.html",
    "href": "reference/api-index.html",
    "title": "Auto-Generated API Documentation",
    "section": "",
    "text": "This section contains automatically generated API documentation extracted from Python docstrings.",
    "crumbs": [
      "Auto-Generated API",
      "Auto-Generated API Documentation"
    ]
  },
  {
    "objectID": "reference/api-index.html#about-auto-generated-docs",
    "href": "reference/api-index.html#about-auto-generated-docs",
    "title": "Auto-Generated API Documentation",
    "section": "About Auto-Generated Docs",
    "text": "About Auto-Generated Docs\nThe documentation in this section is automatically generated using quartodoc from the source code docstrings. This ensures the API reference stays in sync with the actual code.\n\nHow It Works\n\nDocstrings - Functions and classes have docstrings in NumPy format\nquartodoc - Extracts docstrings and generates Markdown\nQuarto - Renders the Markdown into HTML\nGitHub Actions - Automatically rebuilds when code changes\n\n\n\nOrganization\nThe auto-generated documentation is organized by module:\n\nCore Utilities - Essential utility functions and classes from utils.blech_utils\nClustering - Clustering algorithms from utils.clustering\nEphys Data Analysis - Electrophysiology analysis tools from utils.ephys_data\nQuality Assurance - QA tools from utils.qa_utils\n\n\n\nComplementary Documentation\nFor higher-level overviews and tutorials, see the Overview Guides section:\n\nCore Pipeline - Complete pipeline walkthrough\nUtilities - Utility modules overview\nEphys Data - Ephys analysis guide\nQA Tools - Quality assurance guide\nEMG Analysis - EMG processing guide",
    "crumbs": [
      "Auto-Generated API",
      "Auto-Generated API Documentation"
    ]
  },
  {
    "objectID": "reference/api-index.html#navigation",
    "href": "reference/api-index.html#navigation",
    "title": "Auto-Generated API Documentation",
    "section": "Navigation",
    "text": "Navigation\nUse the sidebar to browse the auto-generated API documentation. Each function and class has:\n\nSignature - Function/class signature with parameters\nParameters - Detailed parameter descriptions\nReturns - Return value descriptions\nExamples - Usage examples (when available)\nSee Also - Related functions and classes",
    "crumbs": [
      "Auto-Generated API",
      "Auto-Generated API Documentation"
    ]
  },
  {
    "objectID": "reference/api-index.html#contributing",
    "href": "reference/api-index.html#contributing",
    "title": "Auto-Generated API Documentation",
    "section": "Contributing",
    "text": "Contributing\nTo improve the auto-generated documentation:\n\nUpdate docstrings in the source code (.py files)\nFollow NumPy docstring format - See NumPy Style Guide\nRebuild docs - Run quartodoc build in the docs/ directory\nPreview changes - Run quarto preview to see the results\n\nSee docs/README.md for detailed instructions.",
    "crumbs": [
      "Auto-Generated API",
      "Auto-Generated API Documentation"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "blech_clust",
    "section": "Overview",
    "text": "Overview\nblech_clust is a comprehensive Python and R based toolkit for clustering and sorting electrophysiology data recorded using the Intan RHD2132 chips. Originally written for cortical multi-electrode recordings in Don Katz’s lab at Brandeis University, it’s optimized for high-performance computing clusters but can be easily modified to work in any parallel environment.\nVisit the Katz lab website for more information."
  },
  {
    "objectID": "index.html#key-features",
    "href": "index.html#key-features",
    "title": "blech_clust",
    "section": "Key Features",
    "text": "Key Features\n\nAutomated Spike Sorting: Complete pipeline from raw data to sorted units\nEMG Analysis: Multiple approaches including BSA/STFT and QDA-based gape detection\nQuality Assessment: Built-in tools for dataset quality grading and validation\nParallel Processing: Optimized for HPC environments\nComprehensive Documentation: Detailed API reference and tutorials"
  },
  {
    "objectID": "index.html#quick-links",
    "href": "index.html#quick-links",
    "title": "blech_clust",
    "section": "Quick Links",
    "text": "Quick Links\n\nGetting Started - Installation and setup instructions\nAPI Reference - Complete API documentation\nTutorials - Step-by-step guides\nGitHub Repository\nBlog"
  },
  {
    "objectID": "index.html#pipeline-overview",
    "href": "index.html#pipeline-overview",
    "title": "blech_clust",
    "section": "Pipeline Overview",
    "text": "Pipeline Overview\n\nSpike Sorting Pipeline\n\nblech_exp_info.py - Pre-clustering step to annotate channels and save experimental parameters\nblech_clust.py - Setup directories and define clustering parameters\nblech_common_avg_reference.py - Perform common average referencing\nblech_run_process.sh - Parallel spike extraction and clustering\nblech_post_process.py - Add selected units to HDF5 file\nblech_units_plot.py - Plot waveforms of selected spikes\nblech_make_arrays.py - Generate spike-train arrays\nblech_run_QA.sh - Quality assurance checks\nblech_unit_characteristics.py - Analyze unit characteristics\nblech_data_summary.py - Generate comprehensive dataset summary\ngrade_dataset.py - Grade dataset quality based on metrics\n\n\n\nEMG Analysis Pipelines\nShared Steps: 1. Complete spike sorting through blech_make_arrays.py 2. emg_filter.py - Filter EMG signals\nBSA/STFT Branch: - Bayesian Spectrum Analysis and Short-Time Fourier Transform for frequency analysis\nQDA Branch: - Quadratic Discriminant Analysis for gape detection (based on Li et al.’s methodology)"
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "blech_clust",
    "section": "Installation",
    "text": "Installation\nThe installation process is managed through a Makefile that handles all dependencies:\n# Clone the repository\ngit clone https://github.com/katzlabbrandeis/blech_clust.git\ncd blech_clust\n\n# Install everything\nmake all\n\n# Activate the environment\nconda activate blech_clust\nFor more detailed installation instructions, see the Getting Started guide."
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "blech_clust",
    "section": "Contributing",
    "text": "Contributing\nWe welcome contributions! Please read our CONTRIBUTING.md file for guidelines.\n\nContributing to Documentation\nHelp us improve the documentation:\n\nReport issues: Found an error or unclear explanation? Open an issue\nSuggest improvements: Have ideas for better organization or content? We’d love to hear them\nSubmit changes: See docs/README.md for instructions on building and updating documentation\n\nThe documentation is built with Quarto and automatically deployed via GitHub Actions."
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "blech_clust",
    "section": "Citation",
    "text": "Citation\nIf you use this code in your research, please cite:\n@software{blech_clust_katz,\n  author       = {Mahmood, Abuzar and\n                  Mukherjee, Narendra and\n                  Stone, Bradly and\n                  Raymond, Martin and\n                  Germaine, Hannah and\n                  Lin, Jian-You and\n                  Mazzio, Christina and\n                  Katz, Donald},\n  title        = {katzlabbrandeis/blech\\_clust: v1.1.0},\n  month        = apr,\n  year         = 2025,\n  publisher    = {Zenodo},\n  version      = {1.1.0},\n  doi          = {10.5281/zenodo.15175273},\n  url          = {https://doi.org/10.5281/zenodo.15175273}\n}"
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "blech_clust",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis work used ACCESS-allocated resources at Brandeis University through allocation BIO230103 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by U.S. National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296.\nThe project titled “Computational Processing and Modeling of Neural Ensembles in Identifying the Nonlinear Dynamics of Taste Perception” was led by PI Abuzar Mahmood. The computational allocation was active from 2023-06-26 to 2024-06-25."
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "This tutorial walks you through the complete spike sorting pipeline from raw data to analyzed units.\n\n\nOpen a terminal and run:\ncd /path/to/blech_clust  # Make the blech_clust repository your working directory\nconda activate blech_clust  # Activate blech_clust environment\nDIR=/path/to/raw/data/files  # Save the path of the target Intan data to be sorted\npython blech_exp_info.py $DIR  # Generate metadata and electrode layout\n\n\nOnce you’ve started running the script, it will ask you to “fill in car groups”. Go to the Intan data folder, where you’ll find a file named [...]_electrode_layout.csv.\n\nOpen this file in a spreadsheet editor\nFill in the CAR_group column\nGive all electrodes implanted in the same bundle the same identifier\nUse different identifiers for different bundles\n\nExample: All electrodes from a bundle in right GC are called GC1, and all electrodes from a bundle in left GC are called GC2\n\nReturn to the terminal and type y then press enter\n\n\n\n\nThe script will search your data folder for DIN files and print something like:\n(0, 'board-DIN-09.dat'),\n(1, 'board-DIN-11.dat'),\n(2, 'board-DIN-12.dat'),\n(3, 'board-DIN-13.dat')\nThese are the files for the Intan digital inputs that correspond to stimulus presentations and/or laser activations.\nPrompt: Taste dig_ins used (IN ORDER, anything separated) :: \"x\" to exit ::\n\nSelect the DINs to include in later analysis steps\nExample: To include DINs 11 and 13 but not 09 or 12, type 1,3 and press enter\nNote: If you have a DIN for laser activations, do not include it here; it will be requested later\n\n\n\n\nPrompt: Tastes names used (IN ORDER, anything separated) :: \"x\" to exit ::\n\nProvide taste names for each selected DIN\nExample: If DIN-11 was DI H2O and DIN-13 was 300mM sucrose, enter Water,Sucrose\nLeave off the molarity (provided in the next step)\n\n\n\n\nPrompt: Corresponding concs used (in M, IN ORDER, COMMA separated) :: \"x\" to exit ::\n\nProvide numeric inputs for concentrations in Molarity\nExample: For DI H2O and 300mM sucrose, enter 0,0.3\n\n\n\n\nPrompt: Enter palatability rankings used (anything separated), higher number = more palatable :: \"x\" to exit ::\n\nProvide numeric rankings (&gt; 0 and &lt;= number of stimuli)\nCan be non-integer and accept duplicates\nValid examples: 4,3,2,1 or 0.4,0.3,0.2,0.1 or 3,2,2,1\nInvalid examples: 2,1,1,0 (contains 0) or 5,4,3,2 (exceeds number of stimuli)\nExample for water/sucrose: 1,2\n\n\n\n\nPrompt: Laser dig_in index, &lt;BLANK&gt; for none::: \"x\" to exit ::\n\nIf you have a laser DIN, enter its index (e.g., 0 for DIN-09)\nIf no laser, just press enter\n\n\n\n\nPrompt: ::: Please enter any notes about the experiment.\n\nEnter any pertinent comments or press enter to finish\n\n\n\n\n\nBefore running the clustering pipeline, set up parameter files:\n\nCopy blech_clust/params/_templates/sorting_params_template.json to blech_clust/params/sorting_params_template.json\nUpdate the parameters as needed for your experiment\nAlso copy and adapt:\n\nwaveform_classifier_params.json\nemg_params.json\n\n\n\n\n\n\n\nbash blech_clust_pre.sh $DIR   # Perform steps up to spike extraction and UMAP\npython blech_post_process.py   # Add sorted units to HDF5 (CLI or .CSV as input)\nbash blech_clust_post.sh       # Perform steps up to PSTH generation\n\n\n\nbash blech_autosort.sh &lt;data_directory&gt; [--force]\n\n&lt;data_directory&gt;: Path to the directory containing the raw data files\n--force: Optional flag to force re-processing even if previous results exist\n\nThe blech_autosort.sh script: - Checks for required parameter files - Verifies that specific settings are enabled - Executes the pre-processing, clustering, and post-processing steps in sequence\n\n\n\n\nAfter processing, assess the quality of your dataset:\npython blech_unit_characteristics.py  # Analyze unit characteristics\npython utils/blech_data_summary.py    # Generate comprehensive dataset summary\npython utils/grade_dataset.py         # Grade dataset quality based on metrics"
  },
  {
    "objectID": "tutorials.html#workflow-walkthrough",
    "href": "tutorials.html#workflow-walkthrough",
    "title": "Tutorials",
    "section": "",
    "text": "This tutorial walks you through the complete spike sorting pipeline from raw data to analyzed units.\n\n\nOpen a terminal and run:\ncd /path/to/blech_clust  # Make the blech_clust repository your working directory\nconda activate blech_clust  # Activate blech_clust environment\nDIR=/path/to/raw/data/files  # Save the path of the target Intan data to be sorted\npython blech_exp_info.py $DIR  # Generate metadata and electrode layout\n\n\nOnce you’ve started running the script, it will ask you to “fill in car groups”. Go to the Intan data folder, where you’ll find a file named [...]_electrode_layout.csv.\n\nOpen this file in a spreadsheet editor\nFill in the CAR_group column\nGive all electrodes implanted in the same bundle the same identifier\nUse different identifiers for different bundles\n\nExample: All electrodes from a bundle in right GC are called GC1, and all electrodes from a bundle in left GC are called GC2\n\nReturn to the terminal and type y then press enter\n\n\n\n\nThe script will search your data folder for DIN files and print something like:\n(0, 'board-DIN-09.dat'),\n(1, 'board-DIN-11.dat'),\n(2, 'board-DIN-12.dat'),\n(3, 'board-DIN-13.dat')\nThese are the files for the Intan digital inputs that correspond to stimulus presentations and/or laser activations.\nPrompt: Taste dig_ins used (IN ORDER, anything separated) :: \"x\" to exit ::\n\nSelect the DINs to include in later analysis steps\nExample: To include DINs 11 and 13 but not 09 or 12, type 1,3 and press enter\nNote: If you have a DIN for laser activations, do not include it here; it will be requested later\n\n\n\n\nPrompt: Tastes names used (IN ORDER, anything separated) :: \"x\" to exit ::\n\nProvide taste names for each selected DIN\nExample: If DIN-11 was DI H2O and DIN-13 was 300mM sucrose, enter Water,Sucrose\nLeave off the molarity (provided in the next step)\n\n\n\n\nPrompt: Corresponding concs used (in M, IN ORDER, COMMA separated) :: \"x\" to exit ::\n\nProvide numeric inputs for concentrations in Molarity\nExample: For DI H2O and 300mM sucrose, enter 0,0.3\n\n\n\n\nPrompt: Enter palatability rankings used (anything separated), higher number = more palatable :: \"x\" to exit ::\n\nProvide numeric rankings (&gt; 0 and &lt;= number of stimuli)\nCan be non-integer and accept duplicates\nValid examples: 4,3,2,1 or 0.4,0.3,0.2,0.1 or 3,2,2,1\nInvalid examples: 2,1,1,0 (contains 0) or 5,4,3,2 (exceeds number of stimuli)\nExample for water/sucrose: 1,2\n\n\n\n\nPrompt: Laser dig_in index, &lt;BLANK&gt; for none::: \"x\" to exit ::\n\nIf you have a laser DIN, enter its index (e.g., 0 for DIN-09)\nIf no laser, just press enter\n\n\n\n\nPrompt: ::: Please enter any notes about the experiment.\n\nEnter any pertinent comments or press enter to finish\n\n\n\n\n\nBefore running the clustering pipeline, set up parameter files:\n\nCopy blech_clust/params/_templates/sorting_params_template.json to blech_clust/params/sorting_params_template.json\nUpdate the parameters as needed for your experiment\nAlso copy and adapt:\n\nwaveform_classifier_params.json\nemg_params.json\n\n\n\n\n\n\n\nbash blech_clust_pre.sh $DIR   # Perform steps up to spike extraction and UMAP\npython blech_post_process.py   # Add sorted units to HDF5 (CLI or .CSV as input)\nbash blech_clust_post.sh       # Perform steps up to PSTH generation\n\n\n\nbash blech_autosort.sh &lt;data_directory&gt; [--force]\n\n&lt;data_directory&gt;: Path to the directory containing the raw data files\n--force: Optional flag to force re-processing even if previous results exist\n\nThe blech_autosort.sh script: - Checks for required parameter files - Verifies that specific settings are enabled - Executes the pre-processing, clustering, and post-processing steps in sequence\n\n\n\n\nAfter processing, assess the quality of your dataset:\npython blech_unit_characteristics.py  # Analyze unit characteristics\npython utils/blech_data_summary.py    # Generate comprehensive dataset summary\npython utils/grade_dataset.py         # Grade dataset quality based on metrics"
  },
  {
    "objectID": "tutorials.html#emg-analysis-tutorial",
    "href": "tutorials.html#emg-analysis-tutorial",
    "title": "Tutorials",
    "section": "EMG Analysis Tutorial",
    "text": "EMG Analysis Tutorial\n\nShared Setup\n\nComplete spike sorting through blech_make_arrays.py\nFilter EMG signals:\npython emg_filter.py\n\n\n\nBSA/STFT Branch\nFor Bayesian Spectrum Analysis and Short-Time Fourier Transform:\npython emg_freq_setup.py              # Configure parameters for frequency analysis\nbash blech_emg_jetstream_parallel.sh  # Parallel processing of EMG signals\npython emg_freq_post_process.py       # Aggregate and process results\npython emg_freq_plot.py               # Generate visualizations\n\n\nQDA Branch\nFor Quadratic Discriminant Analysis (gape detection):\npython emg_freq_setup.py  # Setup parameters for gape detection\npython get_gapes_Li.py    # Detect gapes using QDA classifier"
  },
  {
    "objectID": "tutorials.html#testing-your-installation",
    "href": "tutorials.html#testing-your-installation",
    "title": "Tutorials",
    "section": "Testing Your Installation",
    "text": "Testing Your Installation\n\nLocal Testing with Prefect\n\nStart the Prefect server in a separate terminal:\nprefect server start\nIn another terminal, run the tests:\ncd &lt;path_to_blech_clust&gt;\nmake prefect  # Install/update Prefect\nRun specific test suites:\n# Run all tests\npython pipeline_testing/prefect_pipeline.py --all\n\n# Run only spike sorting tests\npython pipeline_testing/prefect_pipeline.py -s\n\n# Run only EMG analysis tests\npython pipeline_testing/prefect_pipeline.py -e\n\n# Run spike sorting followed by EMG analysis\npython pipeline_testing/prefect_pipeline.py --spike-emg\n\nMonitor test progress at http://localhost:4200"
  },
  {
    "objectID": "tutorials.html#advanced-topics",
    "href": "tutorials.html#advanced-topics",
    "title": "Tutorials",
    "section": "Advanced Topics",
    "text": "Advanced Topics\n\nRNN-based Firing Rate Inference\nUse the infer_rnn_rates.py utility to infer firing rates from spike trains:\npython utils/infer_rnn_rates.py &lt;data_dir&gt; [options]\nOptions: - --override_config: Override config file and use provided arguments - --train_steps TRAIN_STEPS: Number of training steps (default: 15000) - --hidden_size HIDDEN_SIZE: Hidden size of RNN (default: 8) - --bin_size BIN_SIZE: Bin size for binning spikes (default: 25) - --train_test_split TRAIN_TEST_SPLIT: Fraction of data for training (default: 0.75) - --no_pca: Do not use PCA for preprocessing - --retrain: Force retraining of model - --time_lims TIME_LIMS TIME_LIMS: Time limits for inferred firing rates (default: [1500, 4500])"
  },
  {
    "objectID": "tutorials.html#additional-resources",
    "href": "tutorials.html#additional-resources",
    "title": "Tutorials",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nModule Documentation: Detailed documentation for the ephys_data module\nWiki: Additional guides and information\nBlog: Insights and updates from the development team"
  }
]